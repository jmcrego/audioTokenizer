Loading pytorch-gpu/py3/2.6.0
  Loading requirement: llvm/15.0.6 gcc/11.4.1 cuda/12.4.1 nccl/2.21.5-1-cuda
    cudnn/9.2.0.82-cuda openmpi/4.1.5-cuda intel-mkl/2020.4 magma/2.8.0-cuda
    sox/14.4.2 hdf5/1.12.0-mpi-cuda libjpeg-turbo/2.1.3 ffmpeg/6.1.1
    openjdk/11.0.2
2025-12-18 12:29:22,780 [INFO] train: CUDA available: True
2025-12-18 12:29:22,782 [INFO] train: Device count: 1
2025-12-18 12:29:22,782 [INFO] train: device: cuda, dtype: torch.float32
2025-12-18 12:29:23,111 [INFO] Embedder: Loaded /lustre/fsmisc/dataset/HuggingFace_Models/utter-project/mHuBERT-147, embedding_dim=768, sample_rate=16000 downsample_ratio=320
2025-12-18 12:29:25,736 [INFO] Backbone: Loaded Tokenizer from /lustre/fsmisc/dataset/HuggingFace_Models/utter-project/EuroLLM-1.7B-Instruct
2025-12-18 12:30:07,284 [INFO] Backbone: Loaded LLM model from /lustre/fsmisc/dataset/HuggingFace_Models/utter-project/EuroLLM-1.7B-Instruct
2025-12-18 12:30:07,620 [INFO] Backbone: Initialized LoRa adapters LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/lustre/fsmisc/dataset/HuggingFace_Models/utter-project/EuroLLM-1.7B-Instruct', revision=None, inference_mode=False, r=16, target_modules={'o_proj', 'k_proj', 'q_proj', 'v_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
2025-12-18 12:30:07,620 [INFO] Projector: Initializing Projector {'path': None, 'stack_size': 8, 'rank_dim': 256}, audio_embedding_dim=768, llm_embedding_dim=2048
2025-12-18 12:30:07,637 [INFO] Projector: Initialized Projector with random weights
2025-12-18 12:30:10,056 [INFO] Dataset: sample=0 prompt=
Transcribe then translate into en.
[ASR]  target=No gens. [STT] None.<|im_end|>
2025-12-18 12:33:57,436 [INFO] Dataset: sample=100000 prompt=
Transcribe.
[ASR]  target=The Kahl Educational Center is located in downtown Davenport.<|im_end|>
2025-12-18 12:37:30,859 [INFO] Dataset: sample=200000 prompt=
Translate into en.
[STT]  target=And now we proceed to the regular agenda of the Conference on Disarmament.<|im_end|>
2025-12-18 12:37:52,035 [INFO] Dataset: Read dataset ./data/covost2/all.3tasks.train.tsv with 209496 samples (defaultdict(<class 'int'>, {'asr+stt': 69832, 'asr': 69832, 'stt': 69832}))
2025-12-18 12:37:52,039 [INFO] Dataset: sample=0 prompt=
Transcribe then translate into en.
[ASR]  target=Supervisa l'emissió de les resolucions de concessió de l'habitació. [STT] Supervises issuance of room concession decisions.<|im_end|>
2025-12-18 12:37:57,313 [INFO] Dataset: Read dataset ./data/covost2/all.3tasks.test.tsv with 2100 samples (defaultdict(<class 'int'>, {'asr+stt': 700, 'asr': 700, 'stt': 700}))
2025-12-18 12:37:57,315 [INFO] Trainer: Initializing {'config': {'audio': {'path': '/lustre/fsmisc/dataset/HuggingFace_Models/utter-project/mHuBERT-147', 'l2_norm': False}, 'projector': {'path': None, 'stack_size': 8, 'rank_dim': 256}, 'llm': {'path': '/lustre/fsmisc/dataset/HuggingFace_Models/utter-project/EuroLLM-1.7B-Instruct'}, 'lora': {'path': None, 'r': 16, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 'lora_dropout': 0.05, 'bias': 'none', 'task_type': 'CAUSAL_LM'}, 'asr_token': '[ASR]', 'stt_token': '[STT]'}, 'batch_size': 8, 'lr_proj': 0.0005, 'lr_lora': 0.0001, 'max_steps': 100000, 'max_epochs': 0, 'save_best_n': 3, 'eval_every': 1000, 'log_every': 100, 'accum_steps': 4, 'output_dir': 'sft5', 'seed': 42}
2025-12-18 12:37:57,332 [INFO] Trainer: Trainable params in model: 7608576
2025-12-18 12:37:57,428 [INFO] Trainer: Initialized Sampler and DataLoader for train with batch_size=8 with 209496 samples
2025-12-18 12:37:57,431 [INFO] Trainer: Initialized Sampler and DataLoader for eval with batch_size=8 with 209496 samples
2025-12-18 12:37:57,435 [INFO] Trainer: Initialized AdamW optimizer with lr_proj=0.0005 lr_lora=0.0001
2025-12-18 12:37:57,435 [INFO] Trainer: Initialized Linear scheduler with warmup for 100000 steps, with 1000 warmup_steps
2025-12-18 12:37:57,435 [INFO] Trainer: Start training
2025-12-18 12:38:48,422 [INFO] Trainer: Train [Step    100/100000, Epoch 0.004/0] loss=7.5561 | lr_proj=1.250000e-05, lr_lora=2.500000e-06 | elapsed=00h:00m:50s
2025-12-18 12:39:35,766 [INFO] Trainer: Train [Step    200/100000, Epoch 0.008/0] loss=3.8029 | lr_proj=2.500000e-05, lr_lora=5.000000e-06 | elapsed=00h:01m:38s
2025-12-18 12:40:22,509 [INFO] Trainer: Train [Step    300/100000, Epoch 0.011/0] loss=4.7281 | lr_proj=3.750000e-05, lr_lora=7.500000e-06 | elapsed=00h:02m:25s
2025-12-18 12:41:11,446 [INFO] Trainer: Train [Step    400/100000, Epoch 0.015/0] loss=4.6234 | lr_proj=5.000000e-05, lr_lora=1.000000e-05 | elapsed=00h:03m:14s
2025-12-18 12:41:59,290 [INFO] Trainer: Train [Step    500/100000, Epoch 0.019/0] loss=3.1310 | lr_proj=6.250000e-05, lr_lora=1.250000e-05 | elapsed=00h:04m:01s
2025-12-18 12:42:48,108 [INFO] Trainer: Train [Step    600/100000, Epoch 0.023/0] loss=2.7194 | lr_proj=7.500000e-05, lr_lora=1.500000e-05 | elapsed=00h:04m:50s
2025-12-18 12:43:35,594 [INFO] Trainer: Train [Step    700/100000, Epoch 0.027/0] loss=3.0212 | lr_proj=8.750000e-05, lr_lora=1.750000e-05 | elapsed=00h:05m:38s
2025-12-18 12:44:25,312 [INFO] Trainer: Train [Step    800/100000, Epoch 0.031/0] loss=2.2662 | lr_proj=1.000000e-04, lr_lora=2.000000e-05 | elapsed=00h:06m:27s
2025-12-18 12:45:13,835 [INFO] Trainer: Train [Step    900/100000, Epoch 0.034/0] loss=2.1261 | lr_proj=1.125000e-04, lr_lora=2.250000e-05 | elapsed=00h:07m:16s
2025-12-18 12:46:01,924 [INFO] Trainer: Train [Step   1000/100000, Epoch 0.038/0] loss=2.6189 | lr_proj=1.250000e-04, lr_lora=2.500000e-05 | elapsed=00h:08m:04s
2025-12-18 12:47:32,307 [INFO] Trainer: Eval  [Step   1000/100000, Epoch 0.038/0] loss=2.7099 | lr_proj=1.250000e-04, lr_lora=2.500000e-05 | elapsed=00h:09m:34s
2025-12-18 12:47:32,340 [INFO] Trainer: Saved Projector to sft5/checkpoint_step1000.proj.pt
2025-12-18 12:47:32,449 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step1000.lora
2025-12-18 12:47:32,596 [INFO] Trainer: Saved config to sft5/checkpoint_step1000.config.json
2025-12-18 12:48:20,667 [INFO] Trainer: Train [Step   1100/100000, Epoch 0.042/0] loss=3.3782 | lr_proj=1.375000e-04, lr_lora=2.750000e-05 | elapsed=00h:10m:23s
2025-12-18 12:49:09,035 [INFO] Trainer: Train [Step   1200/100000, Epoch 0.046/0] loss=2.6503 | lr_proj=1.500000e-04, lr_lora=3.000000e-05 | elapsed=00h:11m:11s
2025-12-18 12:49:57,061 [INFO] Trainer: Train [Step   1300/100000, Epoch 0.050/0] loss=1.8358 | lr_proj=1.625000e-04, lr_lora=3.250000e-05 | elapsed=00h:11m:59s
2025-12-18 12:50:44,177 [INFO] Trainer: Train [Step   1400/100000, Epoch 0.053/0] loss=2.9563 | lr_proj=1.750000e-04, lr_lora=3.500000e-05 | elapsed=00h:12m:46s
2025-12-18 12:51:32,328 [INFO] Trainer: Train [Step   1500/100000, Epoch 0.057/0] loss=2.0592 | lr_proj=1.875000e-04, lr_lora=3.750000e-05 | elapsed=00h:13m:34s
2025-12-18 12:52:21,855 [INFO] Trainer: Train [Step   1600/100000, Epoch 0.061/0] loss=2.7621 | lr_proj=2.000000e-04, lr_lora=4.000000e-05 | elapsed=00h:14m:24s
2025-12-18 12:53:07,444 [INFO] Trainer: Train [Step   1700/100000, Epoch 0.065/0] loss=2.4433 | lr_proj=2.125000e-04, lr_lora=4.250000e-05 | elapsed=00h:15m:10s
2025-12-18 12:53:57,170 [INFO] Trainer: Train [Step   1800/100000, Epoch 0.069/0] loss=3.2706 | lr_proj=2.250000e-04, lr_lora=4.500000e-05 | elapsed=00h:15m:59s
2025-12-18 12:54:45,790 [INFO] Trainer: Train [Step   1900/100000, Epoch 0.073/0] loss=2.8290 | lr_proj=2.375000e-04, lr_lora=4.750000e-05 | elapsed=00h:16m:48s
2025-12-18 12:55:33,338 [INFO] Trainer: Train [Step   2000/100000, Epoch 0.076/0] loss=2.8620 | lr_proj=2.500000e-04, lr_lora=5.000000e-05 | elapsed=00h:17m:35s
2025-12-18 12:57:03,457 [INFO] Trainer: Eval  [Step   2000/100000, Epoch 0.076/0] loss=2.5798 | lr_proj=2.500000e-04, lr_lora=5.000000e-05 | elapsed=00h:19m:06s
2025-12-18 12:57:03,485 [INFO] Trainer: Saved Projector to sft5/checkpoint_step2000.proj.pt
2025-12-18 12:57:03,564 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step2000.lora
2025-12-18 12:57:03,711 [INFO] Trainer: Saved config to sft5/checkpoint_step2000.config.json
2025-12-18 12:57:52,137 [INFO] Trainer: Train [Step   2100/100000, Epoch 0.080/0] loss=2.3502 | lr_proj=2.625000e-04, lr_lora=5.250000e-05 | elapsed=00h:19m:54s
2025-12-18 12:58:38,009 [INFO] Trainer: Train [Step   2200/100000, Epoch 0.084/0] loss=2.0872 | lr_proj=2.750000e-04, lr_lora=5.500000e-05 | elapsed=00h:20m:40s
2025-12-18 12:59:27,272 [INFO] Trainer: Train [Step   2300/100000, Epoch 0.088/0] loss=1.7906 | lr_proj=2.875000e-04, lr_lora=5.750000e-05 | elapsed=00h:21m:29s
2025-12-18 13:00:15,040 [INFO] Trainer: Train [Step   2400/100000, Epoch 0.092/0] loss=2.9796 | lr_proj=3.000000e-04, lr_lora=6.000000e-05 | elapsed=00h:22m:17s
2025-12-18 13:01:02,262 [INFO] Trainer: Train [Step   2500/100000, Epoch 0.095/0] loss=2.7771 | lr_proj=3.125000e-04, lr_lora=6.250000e-05 | elapsed=00h:23m:04s
2025-12-18 13:01:49,268 [INFO] Trainer: Train [Step   2600/100000, Epoch 0.099/0] loss=2.7293 | lr_proj=3.250000e-04, lr_lora=6.500000e-05 | elapsed=00h:23m:51s
2025-12-18 13:02:36,898 [INFO] Trainer: Train [Step   2700/100000, Epoch 0.103/0] loss=2.0515 | lr_proj=3.375000e-04, lr_lora=6.750000e-05 | elapsed=00h:24m:39s
2025-12-18 13:03:24,506 [INFO] Trainer: Train [Step   2800/100000, Epoch 0.107/0] loss=2.1320 | lr_proj=3.500000e-04, lr_lora=7.000000e-05 | elapsed=00h:25m:27s
2025-12-18 13:04:15,054 [INFO] Trainer: Train [Step   2900/100000, Epoch 0.111/0] loss=2.3089 | lr_proj=3.625000e-04, lr_lora=7.250000e-05 | elapsed=00h:26m:17s
2025-12-18 13:05:02,754 [INFO] Trainer: Train [Step   3000/100000, Epoch 0.115/0] loss=2.9653 | lr_proj=3.750000e-04, lr_lora=7.500000e-05 | elapsed=00h:27m:05s
2025-12-18 13:06:32,830 [INFO] Trainer: Eval  [Step   3000/100000, Epoch 0.115/0] loss=2.5744 | lr_proj=3.750000e-04, lr_lora=7.500000e-05 | elapsed=00h:28m:35s
2025-12-18 13:06:32,856 [INFO] Trainer: Saved Projector to sft5/checkpoint_step3000.proj.pt
2025-12-18 13:06:32,944 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step3000.lora
2025-12-18 13:06:33,091 [INFO] Trainer: Saved config to sft5/checkpoint_step3000.config.json
2025-12-18 13:07:20,987 [INFO] Trainer: Train [Step   3100/100000, Epoch 0.118/0] loss=2.6096 | lr_proj=3.875000e-04, lr_lora=7.750000e-05 | elapsed=00h:29m:23s
2025-12-18 13:08:11,006 [INFO] Trainer: Train [Step   3200/100000, Epoch 0.122/0] loss=2.9224 | lr_proj=4.000000e-04, lr_lora=8.000000e-05 | elapsed=00h:30m:13s
2025-12-18 13:08:59,946 [INFO] Trainer: Train [Step   3300/100000, Epoch 0.126/0] loss=2.1741 | lr_proj=4.125000e-04, lr_lora=8.250000e-05 | elapsed=00h:31m:02s
2025-12-18 13:09:47,868 [INFO] Trainer: Train [Step   3400/100000, Epoch 0.130/0] loss=2.2657 | lr_proj=4.250000e-04, lr_lora=8.500000e-05 | elapsed=00h:31m:50s
2025-12-18 13:10:37,238 [INFO] Trainer: Train [Step   3500/100000, Epoch 0.134/0] loss=1.9555 | lr_proj=4.375000e-04, lr_lora=8.750000e-05 | elapsed=00h:32m:39s
2025-12-18 13:11:25,702 [INFO] Trainer: Train [Step   3600/100000, Epoch 0.137/0] loss=3.3507 | lr_proj=4.500000e-04, lr_lora=9.000000e-05 | elapsed=00h:33m:28s
2025-12-18 13:12:13,655 [INFO] Trainer: Train [Step   3700/100000, Epoch 0.141/0] loss=2.3137 | lr_proj=4.625000e-04, lr_lora=9.250000e-05 | elapsed=00h:34m:16s
2025-12-18 13:13:03,245 [INFO] Trainer: Train [Step   3800/100000, Epoch 0.145/0] loss=1.8596 | lr_proj=4.750000e-04, lr_lora=9.500000e-05 | elapsed=00h:35m:05s
2025-12-18 13:13:50,138 [INFO] Trainer: Train [Step   3900/100000, Epoch 0.149/0] loss=2.2243 | lr_proj=4.875000e-04, lr_lora=9.750000e-05 | elapsed=00h:35m:52s
2025-12-18 13:14:37,652 [INFO] Trainer: Train [Step   4000/100000, Epoch 0.153/0] loss=1.9404 | lr_proj=5.000000e-04, lr_lora=1.000000e-04 | elapsed=00h:36m:40s
2025-12-18 13:16:07,659 [INFO] Trainer: Eval  [Step   4000/100000, Epoch 0.153/0] loss=2.5359 | lr_proj=5.000000e-04, lr_lora=1.000000e-04 | elapsed=00h:38m:10s
2025-12-18 13:16:07,687 [INFO] Trainer: Saved Projector to sft5/checkpoint_step4000.proj.pt
2025-12-18 13:16:07,768 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step4000.lora
2025-12-18 13:16:07,917 [INFO] Trainer: Saved config to sft5/checkpoint_step4000.config.json
2025-12-18 13:16:07,921 [INFO] Trainer: Removing sft5/checkpoint_step1000.config.json.*
2025-12-18 13:16:07,922 [INFO] Trainer: Removing sft5/checkpoint_step1000.proj.pt.*
2025-12-18 13:16:07,923 [INFO] Trainer: Removing sft5/checkpoint_step1000.lora.*
2025-12-18 13:16:07,928 [INFO] Trainer: Removing sft5/checkpoint_step1000.optim.pt.*
2025-12-18 13:16:56,999 [INFO] Trainer: Train [Step   4100/100000, Epoch 0.157/0] loss=1.8191 | lr_proj=4.998737e-04, lr_lora=9.997475e-05 | elapsed=00h:38m:59s
2025-12-18 13:17:45,084 [INFO] Trainer: Train [Step   4200/100000, Epoch 0.160/0] loss=3.0815 | lr_proj=4.997475e-04, lr_lora=9.994949e-05 | elapsed=00h:39m:47s
2025-12-18 13:18:31,492 [INFO] Trainer: Train [Step   4300/100000, Epoch 0.164/0] loss=2.0547 | lr_proj=4.996212e-04, lr_lora=9.992424e-05 | elapsed=00h:40m:34s
2025-12-18 13:19:17,846 [INFO] Trainer: Train [Step   4400/100000, Epoch 0.168/0] loss=2.5749 | lr_proj=4.994949e-04, lr_lora=9.989899e-05 | elapsed=00h:41m:20s
2025-12-18 13:20:06,874 [INFO] Trainer: Train [Step   4500/100000, Epoch 0.172/0] loss=2.5224 | lr_proj=4.993687e-04, lr_lora=9.987374e-05 | elapsed=00h:42m:09s
2025-12-18 13:20:54,350 [INFO] Trainer: Train [Step   4600/100000, Epoch 0.176/0] loss=2.1756 | lr_proj=4.992424e-04, lr_lora=9.984848e-05 | elapsed=00h:42m:56s
2025-12-18 13:21:41,528 [INFO] Trainer: Train [Step   4700/100000, Epoch 0.179/0] loss=3.0616 | lr_proj=4.991162e-04, lr_lora=9.982323e-05 | elapsed=00h:43m:44s
2025-12-18 13:22:29,378 [INFO] Trainer: Train [Step   4800/100000, Epoch 0.183/0] loss=2.1944 | lr_proj=4.989899e-04, lr_lora=9.979798e-05 | elapsed=00h:44m:31s
2025-12-18 13:23:14,643 [INFO] Trainer: Train [Step   4900/100000, Epoch 0.187/0] loss=2.8484 | lr_proj=4.988636e-04, lr_lora=9.977273e-05 | elapsed=00h:45m:17s
2025-12-18 13:24:04,422 [INFO] Trainer: Train [Step   5000/100000, Epoch 0.191/0] loss=2.2659 | lr_proj=4.987374e-04, lr_lora=9.974747e-05 | elapsed=00h:46m:06s
2025-12-18 13:25:34,499 [INFO] Trainer: Eval  [Step   5000/100000, Epoch 0.191/0] loss=2.5017 | lr_proj=4.987374e-04, lr_lora=9.974747e-05 | elapsed=00h:47m:37s
2025-12-18 13:25:34,554 [INFO] Trainer: Saved Projector to sft5/checkpoint_step5000.proj.pt
2025-12-18 13:25:34,631 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step5000.lora
2025-12-18 13:25:34,773 [INFO] Trainer: Saved config to sft5/checkpoint_step5000.config.json
2025-12-18 13:25:34,776 [INFO] Trainer: Removing sft5/checkpoint_step2000.config.json.*
2025-12-18 13:25:34,778 [INFO] Trainer: Removing sft5/checkpoint_step2000.optim.pt.*
2025-12-18 13:25:34,779 [INFO] Trainer: Removing sft5/checkpoint_step2000.proj.pt.*
2025-12-18 13:25:34,780 [INFO] Trainer: Removing sft5/checkpoint_step2000.lora.*
2025-12-18 13:26:24,883 [INFO] Trainer: Train [Step   5100/100000, Epoch 0.195/0] loss=1.8355 | lr_proj=4.986111e-04, lr_lora=9.972222e-05 | elapsed=00h:48m:27s
2025-12-18 13:27:12,786 [INFO] Trainer: Train [Step   5200/100000, Epoch 0.199/0] loss=2.9938 | lr_proj=4.984848e-04, lr_lora=9.969697e-05 | elapsed=00h:49m:15s
2025-12-18 13:28:02,328 [INFO] Trainer: Train [Step   5300/100000, Epoch 0.202/0] loss=1.8184 | lr_proj=4.983586e-04, lr_lora=9.967172e-05 | elapsed=00h:50m:04s
2025-12-18 13:28:50,825 [INFO] Trainer: Train [Step   5400/100000, Epoch 0.206/0] loss=2.0404 | lr_proj=4.982323e-04, lr_lora=9.964646e-05 | elapsed=00h:50m:53s
2025-12-18 13:29:40,834 [INFO] Trainer: Train [Step   5500/100000, Epoch 0.210/0] loss=2.9337 | lr_proj=4.981061e-04, lr_lora=9.962121e-05 | elapsed=00h:51m:43s
2025-12-18 13:30:28,168 [INFO] Trainer: Train [Step   5600/100000, Epoch 0.214/0] loss=1.9124 | lr_proj=4.979798e-04, lr_lora=9.959596e-05 | elapsed=00h:52m:30s
2025-12-18 13:31:15,362 [INFO] Trainer: Train [Step   5700/100000, Epoch 0.218/0] loss=2.1054 | lr_proj=4.978535e-04, lr_lora=9.957071e-05 | elapsed=00h:53m:17s
2025-12-18 13:32:04,858 [INFO] Trainer: Train [Step   5800/100000, Epoch 0.221/0] loss=2.2848 | lr_proj=4.977273e-04, lr_lora=9.954545e-05 | elapsed=00h:54m:07s
2025-12-18 13:32:54,038 [INFO] Trainer: Train [Step   5900/100000, Epoch 0.225/0] loss=2.7216 | lr_proj=4.976010e-04, lr_lora=9.952020e-05 | elapsed=00h:54m:56s
2025-12-18 13:33:43,723 [INFO] Trainer: Train [Step   6000/100000, Epoch 0.229/0] loss=2.3064 | lr_proj=4.974747e-04, lr_lora=9.949495e-05 | elapsed=00h:55m:46s
2025-12-18 13:35:14,670 [INFO] Trainer: Eval  [Step   6000/100000, Epoch 0.229/0] loss=2.4741 | lr_proj=4.974747e-04, lr_lora=9.949495e-05 | elapsed=00h:57m:17s
2025-12-18 13:35:14,746 [INFO] Trainer: Saved Projector to sft5/checkpoint_step6000.proj.pt
2025-12-18 13:35:14,870 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step6000.lora
2025-12-18 13:35:15,016 [INFO] Trainer: Saved config to sft5/checkpoint_step6000.config.json
2025-12-18 13:35:15,018 [INFO] Trainer: Removing sft5/checkpoint_step3000.lora.*
2025-12-18 13:35:15,024 [INFO] Trainer: Removing sft5/checkpoint_step3000.proj.pt.*
2025-12-18 13:35:15,026 [INFO] Trainer: Removing sft5/checkpoint_step3000.optim.pt.*
2025-12-18 13:35:15,027 [INFO] Trainer: Removing sft5/checkpoint_step3000.config.json.*
2025-12-18 13:36:04,869 [INFO] Trainer: Train [Step   6100/100000, Epoch 0.233/0] loss=2.5388 | lr_proj=4.973485e-04, lr_lora=9.946970e-05 | elapsed=00h:58m:07s
2025-12-18 13:36:53,303 [INFO] Trainer: Train [Step   6200/100000, Epoch 0.237/0] loss=3.2359 | lr_proj=4.972222e-04, lr_lora=9.944444e-05 | elapsed=00h:58m:55s
2025-12-18 13:37:44,134 [INFO] Trainer: Train [Step   6300/100000, Epoch 0.241/0] loss=2.7001 | lr_proj=4.970960e-04, lr_lora=9.941919e-05 | elapsed=00h:59m:46s
2025-12-18 13:38:33,973 [INFO] Trainer: Train [Step   6400/100000, Epoch 0.244/0] loss=2.5086 | lr_proj=4.969697e-04, lr_lora=9.939394e-05 | elapsed=01h:00m:36s
2025-12-18 13:39:25,112 [INFO] Trainer: Train [Step   6500/100000, Epoch 0.248/0] loss=2.0871 | lr_proj=4.968434e-04, lr_lora=9.936869e-05 | elapsed=01h:01m:27s
2025-12-18 13:40:16,314 [INFO] Trainer: Train [Step   6600/100000, Epoch 0.252/0] loss=2.6791 | lr_proj=4.967172e-04, lr_lora=9.934343e-05 | elapsed=01h:02m:18s
2025-12-18 13:41:04,880 [INFO] Trainer: Train [Step   6700/100000, Epoch 0.256/0] loss=2.6283 | lr_proj=4.965909e-04, lr_lora=9.931818e-05 | elapsed=01h:03m:07s
2025-12-18 13:41:53,063 [INFO] Trainer: Train [Step   6800/100000, Epoch 0.260/0] loss=3.5194 | lr_proj=4.964646e-04, lr_lora=9.929293e-05 | elapsed=01h:03m:55s
2025-12-18 13:42:43,311 [INFO] Trainer: Train [Step   6900/100000, Epoch 0.263/0] loss=2.3388 | lr_proj=4.963384e-04, lr_lora=9.926768e-05 | elapsed=01h:04m:45s
2025-12-18 13:43:34,066 [INFO] Trainer: Train [Step   7000/100000, Epoch 0.267/0] loss=1.9750 | lr_proj=4.962121e-04, lr_lora=9.924242e-05 | elapsed=01h:05m:36s
2025-12-18 13:45:04,101 [INFO] Trainer: Eval  [Step   7000/100000, Epoch 0.267/0] loss=2.4672 | lr_proj=4.962121e-04, lr_lora=9.924242e-05 | elapsed=01h:07m:06s
2025-12-18 13:45:04,128 [INFO] Trainer: Saved Projector to sft5/checkpoint_step7000.proj.pt
2025-12-18 13:45:04,203 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step7000.lora
2025-12-18 13:45:04,359 [INFO] Trainer: Saved config to sft5/checkpoint_step7000.config.json
2025-12-18 13:45:04,363 [INFO] Trainer: Removing sft5/checkpoint_step4000.proj.pt.*
2025-12-18 13:45:04,366 [INFO] Trainer: Removing sft5/checkpoint_step4000.lora.*
2025-12-18 13:45:04,374 [INFO] Trainer: Removing sft5/checkpoint_step4000.optim.pt.*
2025-12-18 13:45:04,385 [INFO] Trainer: Removing sft5/checkpoint_step4000.config.json.*
2025-12-18 13:45:53,543 [INFO] Trainer: Train [Step   7100/100000, Epoch 0.271/0] loss=2.8396 | lr_proj=4.960859e-04, lr_lora=9.921717e-05 | elapsed=01h:07m:56s
2025-12-18 13:46:42,813 [INFO] Trainer: Train [Step   7200/100000, Epoch 0.275/0] loss=2.7617 | lr_proj=4.959596e-04, lr_lora=9.919192e-05 | elapsed=01h:08m:45s
2025-12-18 13:47:31,447 [INFO] Trainer: Train [Step   7300/100000, Epoch 0.279/0] loss=1.7323 | lr_proj=4.958333e-04, lr_lora=9.916667e-05 | elapsed=01h:09m:34s
2025-12-18 13:48:21,692 [INFO] Trainer: Train [Step   7400/100000, Epoch 0.283/0] loss=1.9140 | lr_proj=4.957071e-04, lr_lora=9.914141e-05 | elapsed=01h:10m:24s
2025-12-18 13:49:12,297 [INFO] Trainer: Train [Step   7500/100000, Epoch 0.286/0] loss=2.8019 | lr_proj=4.955808e-04, lr_lora=9.911616e-05 | elapsed=01h:11m:14s
2025-12-18 13:50:01,843 [INFO] Trainer: Train [Step   7600/100000, Epoch 0.290/0] loss=1.6581 | lr_proj=4.954545e-04, lr_lora=9.909091e-05 | elapsed=01h:12m:04s
2025-12-18 13:50:49,248 [INFO] Trainer: Train [Step   7700/100000, Epoch 0.294/0] loss=2.6515 | lr_proj=4.953283e-04, lr_lora=9.906566e-05 | elapsed=01h:12m:51s
2025-12-18 13:51:36,273 [INFO] Trainer: Train [Step   7800/100000, Epoch 0.298/0] loss=2.3333 | lr_proj=4.952020e-04, lr_lora=9.904040e-05 | elapsed=01h:13m:38s
2025-12-18 13:52:25,114 [INFO] Trainer: Train [Step   7900/100000, Epoch 0.302/0] loss=3.0770 | lr_proj=4.950758e-04, lr_lora=9.901515e-05 | elapsed=01h:14m:27s
2025-12-18 13:53:15,315 [INFO] Trainer: Train [Step   8000/100000, Epoch 0.305/0] loss=2.5531 | lr_proj=4.949495e-04, lr_lora=9.898990e-05 | elapsed=01h:15m:17s
2025-12-18 13:54:45,473 [INFO] Trainer: Eval  [Step   8000/100000, Epoch 0.305/0] loss=2.4628 | lr_proj=4.949495e-04, lr_lora=9.898990e-05 | elapsed=01h:16m:48s
2025-12-18 13:54:45,553 [INFO] Trainer: Saved Projector to sft5/checkpoint_step8000.proj.pt
2025-12-18 13:54:45,680 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step8000.lora
2025-12-18 13:54:45,844 [INFO] Trainer: Saved config to sft5/checkpoint_step8000.config.json
2025-12-18 13:54:45,846 [INFO] Trainer: Removing sft5/checkpoint_step5000.optim.pt.*
2025-12-18 13:54:45,860 [INFO] Trainer: Removing sft5/checkpoint_step5000.config.json.*
2025-12-18 13:54:45,861 [INFO] Trainer: Removing sft5/checkpoint_step5000.lora.*
2025-12-18 13:54:45,869 [INFO] Trainer: Removing sft5/checkpoint_step5000.proj.pt.*
2025-12-18 13:56:24,150 [INFO] Trainer: Train [Step   8100/100000, Epoch 0.309/0] loss=2.9372 | lr_proj=4.948232e-04, lr_lora=9.896465e-05 | elapsed=01h:18m:26s
2025-12-18 13:58:45,701 [INFO] Trainer: Train [Step   8200/100000, Epoch 0.313/0] loss=2.3785 | lr_proj=4.946970e-04, lr_lora=9.893939e-05 | elapsed=01h:20m:48s
2025-12-18 14:01:04,038 [INFO] Trainer: Train [Step   8300/100000, Epoch 0.317/0] loss=2.8033 | lr_proj=4.945707e-04, lr_lora=9.891414e-05 | elapsed=01h:23m:06s
2025-12-18 14:03:18,928 [INFO] Trainer: Train [Step   8400/100000, Epoch 0.321/0] loss=2.7064 | lr_proj=4.944444e-04, lr_lora=9.888889e-05 | elapsed=01h:25m:21s
2025-12-18 14:05:12,176 [INFO] Trainer: Train [Step   8500/100000, Epoch 0.325/0] loss=2.1649 | lr_proj=4.943182e-04, lr_lora=9.886364e-05 | elapsed=01h:27m:14s
2025-12-18 14:07:27,850 [INFO] Trainer: Train [Step   8600/100000, Epoch 0.328/0] loss=2.0234 | lr_proj=4.941919e-04, lr_lora=9.883838e-05 | elapsed=01h:29m:30s
2025-12-18 14:09:18,569 [INFO] Trainer: Train [Step   8700/100000, Epoch 0.332/0] loss=2.6594 | lr_proj=4.940657e-04, lr_lora=9.881313e-05 | elapsed=01h:31m:21s
2025-12-18 14:11:23,784 [INFO] Trainer: Train [Step   8800/100000, Epoch 0.336/0] loss=3.2524 | lr_proj=4.939394e-04, lr_lora=9.878788e-05 | elapsed=01h:33m:26s
2025-12-18 14:13:31,065 [INFO] Trainer: Train [Step   8900/100000, Epoch 0.340/0] loss=2.4266 | lr_proj=4.938131e-04, lr_lora=9.876263e-05 | elapsed=01h:35m:33s
2025-12-18 14:15:38,901 [INFO] Trainer: Train [Step   9000/100000, Epoch 0.344/0] loss=2.3407 | lr_proj=4.936869e-04, lr_lora=9.873737e-05 | elapsed=01h:37m:41s
2025-12-18 14:17:09,663 [INFO] Trainer: Eval  [Step   9000/100000, Epoch 0.344/0] loss=2.4624 | lr_proj=4.936869e-04, lr_lora=9.873737e-05 | elapsed=01h:39m:12s
2025-12-18 14:17:09,723 [INFO] Trainer: Saved Projector to sft5/checkpoint_step9000.proj.pt
2025-12-18 14:17:09,873 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step9000.lora
2025-12-18 14:17:10,057 [INFO] Trainer: Saved config to sft5/checkpoint_step9000.config.json
2025-12-18 14:17:10,060 [INFO] Trainer: Removing sft5/checkpoint_step6000.lora.*
2025-12-18 14:17:10,072 [INFO] Trainer: Removing sft5/checkpoint_step6000.config.json.*
2025-12-18 14:17:10,073 [INFO] Trainer: Removing sft5/checkpoint_step6000.proj.pt.*
2025-12-18 14:17:10,075 [INFO] Trainer: Removing sft5/checkpoint_step6000.optim.pt.*
2025-12-18 14:19:44,924 [INFO] Trainer: Train [Step   9100/100000, Epoch 0.348/0] loss=1.8608 | lr_proj=4.935606e-04, lr_lora=9.871212e-05 | elapsed=01h:41m:47s
2025-12-18 14:22:11,797 [INFO] Trainer: Train [Step   9200/100000, Epoch 0.351/0] loss=2.7868 | lr_proj=4.934343e-04, lr_lora=9.868687e-05 | elapsed=01h:44m:14s
2025-12-18 14:24:26,440 [INFO] Trainer: Train [Step   9300/100000, Epoch 0.355/0] loss=1.7856 | lr_proj=4.933081e-04, lr_lora=9.866162e-05 | elapsed=01h:46m:29s
2025-12-18 14:26:26,599 [INFO] Trainer: Train [Step   9400/100000, Epoch 0.359/0] loss=2.6560 | lr_proj=4.931818e-04, lr_lora=9.863636e-05 | elapsed=01h:48m:29s
2025-12-18 14:28:02,180 [INFO] Trainer: Train [Step   9500/100000, Epoch 0.363/0] loss=2.3242 | lr_proj=4.930556e-04, lr_lora=9.861111e-05 | elapsed=01h:50m:04s
2025-12-18 14:29:37,121 [INFO] Trainer: Train [Step   9600/100000, Epoch 0.367/0] loss=2.3671 | lr_proj=4.929293e-04, lr_lora=9.858586e-05 | elapsed=01h:51m:39s
2025-12-18 14:31:18,688 [INFO] Trainer: Train [Step   9700/100000, Epoch 0.370/0] loss=3.3535 | lr_proj=4.928030e-04, lr_lora=9.856061e-05 | elapsed=01h:53m:21s
2025-12-18 14:33:09,031 [INFO] Trainer: Train [Step   9800/100000, Epoch 0.374/0] loss=3.6177 | lr_proj=4.926768e-04, lr_lora=9.853535e-05 | elapsed=01h:55m:11s
2025-12-18 14:34:59,046 [INFO] Trainer: Train [Step   9900/100000, Epoch 0.378/0] loss=1.5707 | lr_proj=4.925505e-04, lr_lora=9.851010e-05 | elapsed=01h:57m:01s
2025-12-18 14:36:24,003 [INFO] Trainer: Train [Step  10000/100000, Epoch 0.382/0] loss=3.0387 | lr_proj=4.924242e-04, lr_lora=9.848485e-05 | elapsed=01h:58m:26s
2025-12-18 14:37:54,809 [INFO] Trainer: Eval  [Step  10000/100000, Epoch 0.382/0] loss=2.4389 | lr_proj=4.924242e-04, lr_lora=9.848485e-05 | elapsed=01h:59m:57s
2025-12-18 14:37:54,875 [INFO] Trainer: Saved Projector to sft5/checkpoint_step10000.proj.pt
2025-12-18 14:37:55,025 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step10000.lora
2025-12-18 14:37:55,189 [INFO] Trainer: Saved config to sft5/checkpoint_step10000.config.json
2025-12-18 14:37:55,192 [INFO] Trainer: Removing sft5/checkpoint_step7000.optim.pt.*
2025-12-18 14:37:55,211 [INFO] Trainer: Removing sft5/checkpoint_step7000.config.json.*
2025-12-18 14:37:55,229 [INFO] Trainer: Removing sft5/checkpoint_step7000.proj.pt.*
2025-12-18 14:37:55,231 [INFO] Trainer: Removing sft5/checkpoint_step7000.lora.*
2025-12-18 14:39:12,040 [INFO] Trainer: Train [Step  10100/100000, Epoch 0.386/0] loss=2.5352 | lr_proj=4.922980e-04, lr_lora=9.845960e-05 | elapsed=02h:01m:14s
2025-12-18 14:40:29,797 [INFO] Trainer: Train [Step  10200/100000, Epoch 0.390/0] loss=2.4627 | lr_proj=4.921717e-04, lr_lora=9.843434e-05 | elapsed=02h:02m:32s
2025-12-18 14:41:45,836 [INFO] Trainer: Train [Step  10300/100000, Epoch 0.393/0] loss=2.8860 | lr_proj=4.920455e-04, lr_lora=9.840909e-05 | elapsed=02h:03m:48s
2025-12-18 14:42:58,290 [INFO] Trainer: Train [Step  10400/100000, Epoch 0.397/0] loss=2.4115 | lr_proj=4.919192e-04, lr_lora=9.838384e-05 | elapsed=02h:05m:00s
2025-12-18 14:44:10,449 [INFO] Trainer: Train [Step  10500/100000, Epoch 0.401/0] loss=2.4571 | lr_proj=4.917929e-04, lr_lora=9.835859e-05 | elapsed=02h:06m:13s
2025-12-18 14:45:25,764 [INFO] Trainer: Train [Step  10600/100000, Epoch 0.405/0] loss=2.3298 | lr_proj=4.916667e-04, lr_lora=9.833333e-05 | elapsed=02h:07m:28s
2025-12-18 14:46:45,037 [INFO] Trainer: Train [Step  10700/100000, Epoch 0.409/0] loss=3.1336 | lr_proj=4.915404e-04, lr_lora=9.830808e-05 | elapsed=02h:08m:47s
2025-12-18 14:47:53,856 [INFO] Trainer: Train [Step  10800/100000, Epoch 0.412/0] loss=2.4314 | lr_proj=4.914141e-04, lr_lora=9.828283e-05 | elapsed=02h:09m:56s
2025-12-18 14:48:54,147 [INFO] Trainer: Train [Step  10900/100000, Epoch 0.416/0] loss=2.5995 | lr_proj=4.912879e-04, lr_lora=9.825758e-05 | elapsed=02h:10m:56s
2025-12-18 14:50:20,521 [INFO] Trainer: Train [Step  11000/100000, Epoch 0.420/0] loss=3.0478 | lr_proj=4.911616e-04, lr_lora=9.823232e-05 | elapsed=02h:12m:23s
2025-12-18 14:51:51,049 [INFO] Trainer: Eval  [Step  11000/100000, Epoch 0.420/0] loss=2.4227 | lr_proj=4.911616e-04, lr_lora=9.823232e-05 | elapsed=02h:13m:53s
2025-12-18 14:51:51,104 [INFO] Trainer: Saved Projector to sft5/checkpoint_step11000.proj.pt
2025-12-18 14:51:51,230 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step11000.lora
2025-12-18 14:51:51,388 [INFO] Trainer: Saved config to sft5/checkpoint_step11000.config.json
2025-12-18 14:51:51,391 [INFO] Trainer: Removing sft5/checkpoint_step8000.lora.*
2025-12-18 14:51:51,398 [INFO] Trainer: Removing sft5/checkpoint_step8000.config.json.*
2025-12-18 14:51:51,400 [INFO] Trainer: Removing sft5/checkpoint_step8000.proj.pt.*
2025-12-18 14:51:51,403 [INFO] Trainer: Removing sft5/checkpoint_step8000.optim.pt.*
2025-12-18 14:53:19,572 [INFO] Trainer: Train [Step  11100/100000, Epoch 0.424/0] loss=2.5801 | lr_proj=4.910354e-04, lr_lora=9.820707e-05 | elapsed=02h:15m:22s
2025-12-18 14:55:22,167 [INFO] Trainer: Train [Step  11200/100000, Epoch 0.428/0] loss=2.0795 | lr_proj=4.909091e-04, lr_lora=9.818182e-05 | elapsed=02h:17m:24s
2025-12-18 14:57:32,125 [INFO] Trainer: Train [Step  11300/100000, Epoch 0.432/0] loss=2.7642 | lr_proj=4.907828e-04, lr_lora=9.815657e-05 | elapsed=02h:19m:34s
2025-12-18 14:59:43,079 [INFO] Trainer: Train [Step  11400/100000, Epoch 0.435/0] loss=2.8386 | lr_proj=4.906566e-04, lr_lora=9.813131e-05 | elapsed=02h:21m:45s
2025-12-18 15:02:07,787 [INFO] Trainer: Train [Step  11500/100000, Epoch 0.439/0] loss=3.0128 | lr_proj=4.905303e-04, lr_lora=9.810606e-05 | elapsed=02h:24m:10s
2025-12-18 15:04:36,500 [INFO] Trainer: Train [Step  11600/100000, Epoch 0.443/0] loss=2.4939 | lr_proj=4.904040e-04, lr_lora=9.808081e-05 | elapsed=02h:26m:39s
2025-12-18 15:07:05,935 [INFO] Trainer: Train [Step  11700/100000, Epoch 0.447/0] loss=2.9402 | lr_proj=4.902778e-04, lr_lora=9.805556e-05 | elapsed=02h:29m:08s
2025-12-18 15:09:21,176 [INFO] Trainer: Train [Step  11800/100000, Epoch 0.451/0] loss=2.5656 | lr_proj=4.901515e-04, lr_lora=9.803030e-05 | elapsed=02h:31m:23s
2025-12-18 15:12:09,172 [INFO] Trainer: Train [Step  11900/100000, Epoch 0.454/0] loss=2.5357 | lr_proj=4.900253e-04, lr_lora=9.800505e-05 | elapsed=02h:34m:11s
2025-12-18 15:14:35,330 [INFO] Trainer: Train [Step  12000/100000, Epoch 0.458/0] loss=2.1351 | lr_proj=4.898990e-04, lr_lora=9.797980e-05 | elapsed=02h:36m:37s
2025-12-18 15:16:06,530 [INFO] Trainer: Eval  [Step  12000/100000, Epoch 0.458/0] loss=2.4044 | lr_proj=4.898990e-04, lr_lora=9.797980e-05 | elapsed=02h:38m:09s
2025-12-18 15:16:06,592 [INFO] Trainer: Saved Projector to sft5/checkpoint_step12000.proj.pt
2025-12-18 15:16:06,720 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step12000.lora
2025-12-18 15:16:06,888 [INFO] Trainer: Saved config to sft5/checkpoint_step12000.config.json
2025-12-18 15:16:06,891 [INFO] Trainer: Removing sft5/checkpoint_step9000.optim.pt.*
2025-12-18 15:16:06,905 [INFO] Trainer: Removing sft5/checkpoint_step9000.lora.*
2025-12-18 15:16:06,919 [INFO] Trainer: Removing sft5/checkpoint_step9000.proj.pt.*
2025-12-18 15:16:06,923 [INFO] Trainer: Removing sft5/checkpoint_step9000.config.json.*
2025-12-18 15:18:10,556 [INFO] Trainer: Train [Step  12100/100000, Epoch 0.462/0] loss=2.0202 | lr_proj=4.897727e-04, lr_lora=9.795455e-05 | elapsed=02h:40m:13s
2025-12-18 15:20:02,492 [INFO] Trainer: Train [Step  12200/100000, Epoch 0.466/0] loss=1.5832 | lr_proj=4.896465e-04, lr_lora=9.792929e-05 | elapsed=02h:42m:05s
2025-12-18 15:21:51,524 [INFO] Trainer: Train [Step  12300/100000, Epoch 0.470/0] loss=2.2057 | lr_proj=4.895202e-04, lr_lora=9.790404e-05 | elapsed=02h:43m:54s
2025-12-18 15:23:52,051 [INFO] Trainer: Train [Step  12400/100000, Epoch 0.474/0] loss=2.2033 | lr_proj=4.893939e-04, lr_lora=9.787879e-05 | elapsed=02h:45m:54s
2025-12-18 15:25:31,287 [INFO] Trainer: Train [Step  12500/100000, Epoch 0.477/0] loss=2.5839 | lr_proj=4.892677e-04, lr_lora=9.785354e-05 | elapsed=02h:47m:33s
2025-12-18 15:27:13,007 [INFO] Trainer: Train [Step  12600/100000, Epoch 0.481/0] loss=2.0007 | lr_proj=4.891414e-04, lr_lora=9.782828e-05 | elapsed=02h:49m:15s
2025-12-18 15:28:49,906 [INFO] Trainer: Train [Step  12700/100000, Epoch 0.485/0] loss=1.9901 | lr_proj=4.890152e-04, lr_lora=9.780303e-05 | elapsed=02h:50m:52s
2025-12-18 15:30:10,908 [INFO] Trainer: Train [Step  12800/100000, Epoch 0.489/0] loss=1.5847 | lr_proj=4.888889e-04, lr_lora=9.777778e-05 | elapsed=02h:52m:13s
2025-12-18 15:31:23,477 [INFO] Trainer: Train [Step  12900/100000, Epoch 0.493/0] loss=2.5270 | lr_proj=4.887626e-04, lr_lora=9.775253e-05 | elapsed=02h:53m:26s
2025-12-18 15:32:43,424 [INFO] Trainer: Train [Step  13000/100000, Epoch 0.496/0] loss=1.8934 | lr_proj=4.886364e-04, lr_lora=9.772727e-05 | elapsed=02h:54m:45s
2025-12-18 15:34:13,509 [INFO] Trainer: Eval  [Step  13000/100000, Epoch 0.496/0] loss=2.3960 | lr_proj=4.886364e-04, lr_lora=9.772727e-05 | elapsed=02h:56m:16s
2025-12-18 15:34:13,556 [INFO] Trainer: Saved Projector to sft5/checkpoint_step13000.proj.pt
2025-12-18 15:34:13,670 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step13000.lora
2025-12-18 15:34:13,850 [INFO] Trainer: Saved config to sft5/checkpoint_step13000.config.json
2025-12-18 15:34:13,854 [INFO] Trainer: Removing sft5/checkpoint_step10000.optim.pt.*
2025-12-18 15:34:13,884 [INFO] Trainer: Removing sft5/checkpoint_step10000.proj.pt.*
2025-12-18 15:34:13,887 [INFO] Trainer: Removing sft5/checkpoint_step10000.lora.*
2025-12-18 15:34:13,894 [INFO] Trainer: Removing sft5/checkpoint_step10000.config.json.*
2025-12-18 15:35:31,568 [INFO] Trainer: Train [Step  13100/100000, Epoch 0.500/0] loss=3.0702 | lr_proj=4.885101e-04, lr_lora=9.770202e-05 | elapsed=02h:57m:34s
2025-12-18 15:36:47,644 [INFO] Trainer: Train [Step  13200/100000, Epoch 0.504/0] loss=1.6360 | lr_proj=4.883838e-04, lr_lora=9.767677e-05 | elapsed=02h:58m:50s
2025-12-18 15:38:11,624 [INFO] Trainer: Train [Step  13300/100000, Epoch 0.508/0] loss=2.6764 | lr_proj=4.882576e-04, lr_lora=9.765152e-05 | elapsed=03h:00m:14s
2025-12-18 15:39:48,248 [INFO] Trainer: Train [Step  13400/100000, Epoch 0.512/0] loss=2.6629 | lr_proj=4.881313e-04, lr_lora=9.762626e-05 | elapsed=03h:01m:50s
2025-12-18 15:41:33,346 [INFO] Trainer: Train [Step  13500/100000, Epoch 0.516/0] loss=2.8910 | lr_proj=4.880051e-04, lr_lora=9.760101e-05 | elapsed=03h:03m:35s
2025-12-18 15:43:17,885 [INFO] Trainer: Train [Step  13600/100000, Epoch 0.519/0] loss=2.5990 | lr_proj=4.878788e-04, lr_lora=9.757576e-05 | elapsed=03h:05m:20s
2025-12-18 15:44:59,044 [INFO] Trainer: Train [Step  13700/100000, Epoch 0.523/0] loss=1.9037 | lr_proj=4.877525e-04, lr_lora=9.755051e-05 | elapsed=03h:07m:01s
2025-12-18 15:46:46,633 [INFO] Trainer: Train [Step  13800/100000, Epoch 0.527/0] loss=2.5547 | lr_proj=4.876263e-04, lr_lora=9.752525e-05 | elapsed=03h:08m:49s
2025-12-18 15:48:48,161 [INFO] Trainer: Train [Step  13900/100000, Epoch 0.531/0] loss=3.1288 | lr_proj=4.875000e-04, lr_lora=9.750000e-05 | elapsed=03h:10m:50s
2025-12-18 15:50:42,200 [INFO] Trainer: Train [Step  14000/100000, Epoch 0.535/0] loss=1.7767 | lr_proj=4.873737e-04, lr_lora=9.747475e-05 | elapsed=03h:12m:44s
2025-12-18 15:52:12,556 [INFO] Trainer: Eval  [Step  14000/100000, Epoch 0.535/0] loss=2.3715 | lr_proj=4.873737e-04, lr_lora=9.747475e-05 | elapsed=03h:14m:15s
2025-12-18 15:52:12,620 [INFO] Trainer: Saved Projector to sft5/checkpoint_step14000.proj.pt
2025-12-18 15:52:12,807 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step14000.lora
2025-12-18 15:52:12,974 [INFO] Trainer: Saved config to sft5/checkpoint_step14000.config.json
2025-12-18 15:52:12,977 [INFO] Trainer: Removing sft5/checkpoint_step11000.optim.pt.*
2025-12-18 15:52:12,991 [INFO] Trainer: Removing sft5/checkpoint_step11000.proj.pt.*
2025-12-18 15:52:13,003 [INFO] Trainer: Removing sft5/checkpoint_step11000.config.json.*
2025-12-18 15:52:13,005 [INFO] Trainer: Removing sft5/checkpoint_step11000.lora.*
2025-12-18 15:54:28,596 [INFO] Trainer: Train [Step  14100/100000, Epoch 0.538/0] loss=2.0099 | lr_proj=4.872475e-04, lr_lora=9.744949e-05 | elapsed=03h:16m:31s
2025-12-18 15:56:32,505 [INFO] Trainer: Train [Step  14200/100000, Epoch 0.542/0] loss=1.6270 | lr_proj=4.871212e-04, lr_lora=9.742424e-05 | elapsed=03h:18m:35s
2025-12-18 15:58:41,967 [INFO] Trainer: Train [Step  14300/100000, Epoch 0.546/0] loss=3.0750 | lr_proj=4.869949e-04, lr_lora=9.739899e-05 | elapsed=03h:20m:44s
2025-12-18 16:01:32,852 [INFO] Trainer: Train [Step  14400/100000, Epoch 0.550/0] loss=1.8430 | lr_proj=4.868687e-04, lr_lora=9.737374e-05 | elapsed=03h:23m:35s
2025-12-18 16:04:14,211 [INFO] Trainer: Train [Step  14500/100000, Epoch 0.554/0] loss=2.4324 | lr_proj=4.867424e-04, lr_lora=9.734848e-05 | elapsed=03h:26m:16s
2025-12-18 16:06:42,503 [INFO] Trainer: Train [Step  14600/100000, Epoch 0.558/0] loss=3.1678 | lr_proj=4.866162e-04, lr_lora=9.732323e-05 | elapsed=03h:28m:45s
2025-12-18 16:10:00,938 [INFO] Trainer: Train [Step  14700/100000, Epoch 0.561/0] loss=2.1554 | lr_proj=4.864899e-04, lr_lora=9.729798e-05 | elapsed=03h:32m:03s
2025-12-18 16:12:41,626 [INFO] Trainer: Train [Step  14800/100000, Epoch 0.565/0] loss=1.9163 | lr_proj=4.863636e-04, lr_lora=9.727273e-05 | elapsed=03h:34m:44s
2025-12-18 16:15:08,858 [INFO] Trainer: Train [Step  14900/100000, Epoch 0.569/0] loss=2.4506 | lr_proj=4.862374e-04, lr_lora=9.724747e-05 | elapsed=03h:37m:11s
2025-12-18 16:17:35,882 [INFO] Trainer: Train [Step  15000/100000, Epoch 0.573/0] loss=2.3099 | lr_proj=4.861111e-04, lr_lora=9.722222e-05 | elapsed=03h:39m:38s
2025-12-18 16:19:05,904 [INFO] Trainer: Eval  [Step  15000/100000, Epoch 0.573/0] loss=2.3376 | lr_proj=4.861111e-04, lr_lora=9.722222e-05 | elapsed=03h:41m:08s
2025-12-18 16:19:05,954 [INFO] Trainer: Saved Projector to sft5/checkpoint_step15000.proj.pt
2025-12-18 16:19:06,084 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step15000.lora
2025-12-18 16:19:06,235 [INFO] Trainer: Saved config to sft5/checkpoint_step15000.config.json
2025-12-18 16:19:06,239 [INFO] Trainer: Removing sft5/checkpoint_step12000.config.json.*
2025-12-18 16:19:06,240 [INFO] Trainer: Removing sft5/checkpoint_step12000.lora.*
2025-12-18 16:19:06,249 [INFO] Trainer: Removing sft5/checkpoint_step12000.proj.pt.*
2025-12-18 16:19:06,276 [INFO] Trainer: Removing sft5/checkpoint_step12000.optim.pt.*
2025-12-18 16:21:21,538 [INFO] Trainer: Train [Step  15100/100000, Epoch 0.577/0] loss=2.5166 | lr_proj=4.859848e-04, lr_lora=9.719697e-05 | elapsed=03h:43m:24s
2025-12-18 16:23:37,680 [INFO] Trainer: Train [Step  15200/100000, Epoch 0.580/0] loss=2.8044 | lr_proj=4.858586e-04, lr_lora=9.717172e-05 | elapsed=03h:45m:40s
2025-12-18 16:25:52,459 [INFO] Trainer: Train [Step  15300/100000, Epoch 0.584/0] loss=2.4309 | lr_proj=4.857323e-04, lr_lora=9.714646e-05 | elapsed=03h:47m:55s
2025-12-18 16:27:59,137 [INFO] Trainer: Train [Step  15400/100000, Epoch 0.588/0] loss=3.4705 | lr_proj=4.856061e-04, lr_lora=9.712121e-05 | elapsed=03h:50m:01s
2025-12-18 16:30:08,222 [INFO] Trainer: Train [Step  15500/100000, Epoch 0.592/0] loss=2.1747 | lr_proj=4.854798e-04, lr_lora=9.709596e-05 | elapsed=03h:52m:10s
2025-12-18 16:31:47,287 [INFO] Trainer: Train [Step  15600/100000, Epoch 0.596/0] loss=2.4044 | lr_proj=4.853535e-04, lr_lora=9.707071e-05 | elapsed=03h:53m:49s
2025-12-18 16:33:59,670 [INFO] Trainer: Train [Step  15700/100000, Epoch 0.600/0] loss=2.2977 | lr_proj=4.852273e-04, lr_lora=9.704545e-05 | elapsed=03h:56m:02s
2025-12-18 16:35:42,179 [INFO] Trainer: Train [Step  15800/100000, Epoch 0.603/0] loss=3.1120 | lr_proj=4.851010e-04, lr_lora=9.702020e-05 | elapsed=03h:57m:44s
2025-12-18 16:37:36,698 [INFO] Trainer: Train [Step  15900/100000, Epoch 0.607/0] loss=1.7788 | lr_proj=4.849747e-04, lr_lora=9.699495e-05 | elapsed=03h:59m:39s
2025-12-18 16:39:13,403 [INFO] Trainer: Train [Step  16000/100000, Epoch 0.611/0] loss=2.8661 | lr_proj=4.848485e-04, lr_lora=9.696970e-05 | elapsed=04h:01m:15s
2025-12-18 16:40:44,150 [INFO] Trainer: Eval  [Step  16000/100000, Epoch 0.611/0] loss=2.3137 | lr_proj=4.848485e-04, lr_lora=9.696970e-05 | elapsed=04h:02m:46s
2025-12-18 16:40:44,186 [INFO] Trainer: Saved Projector to sft5/checkpoint_step16000.proj.pt
2025-12-18 16:40:44,287 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step16000.lora
2025-12-18 16:40:44,458 [INFO] Trainer: Saved config to sft5/checkpoint_step16000.config.json
2025-12-18 16:40:44,461 [INFO] Trainer: Removing sft5/checkpoint_step13000.lora.*
2025-12-18 16:40:44,468 [INFO] Trainer: Removing sft5/checkpoint_step13000.config.json.*
2025-12-18 16:40:44,470 [INFO] Trainer: Removing sft5/checkpoint_step13000.optim.pt.*
2025-12-18 16:40:44,471 [INFO] Trainer: Removing sft5/checkpoint_step13000.proj.pt.*
2025-12-18 16:42:31,444 [INFO] Trainer: Train [Step  16100/100000, Epoch 0.615/0] loss=1.8593 | lr_proj=4.847222e-04, lr_lora=9.694444e-05 | elapsed=04h:04m:34s
2025-12-18 16:44:19,599 [INFO] Trainer: Train [Step  16200/100000, Epoch 0.619/0] loss=2.2831 | lr_proj=4.845960e-04, lr_lora=9.691919e-05 | elapsed=04h:06m:22s
2025-12-18 16:46:05,055 [INFO] Trainer: Train [Step  16300/100000, Epoch 0.622/0] loss=1.9804 | lr_proj=4.844697e-04, lr_lora=9.689394e-05 | elapsed=04h:08m:07s
2025-12-18 16:47:42,030 [INFO] Trainer: Train [Step  16400/100000, Epoch 0.626/0] loss=2.2207 | lr_proj=4.843434e-04, lr_lora=9.686869e-05 | elapsed=04h:09m:44s
2025-12-18 16:49:45,842 [INFO] Trainer: Train [Step  16500/100000, Epoch 0.630/0] loss=2.6736 | lr_proj=4.842172e-04, lr_lora=9.684343e-05 | elapsed=04h:11m:48s
2025-12-18 16:51:32,111 [INFO] Trainer: Train [Step  16600/100000, Epoch 0.634/0] loss=2.9345 | lr_proj=4.840909e-04, lr_lora=9.681818e-05 | elapsed=04h:13m:34s
2025-12-18 16:53:03,037 [INFO] Trainer: Train [Step  16700/100000, Epoch 0.638/0] loss=2.1317 | lr_proj=4.839646e-04, lr_lora=9.679293e-05 | elapsed=04h:15m:05s
2025-12-18 16:54:55,394 [INFO] Trainer: Train [Step  16800/100000, Epoch 0.642/0] loss=2.9704 | lr_proj=4.838384e-04, lr_lora=9.676768e-05 | elapsed=04h:16m:57s
2025-12-18 16:56:21,992 [INFO] Trainer: Train [Step  16900/100000, Epoch 0.645/0] loss=2.3977 | lr_proj=4.837121e-04, lr_lora=9.674242e-05 | elapsed=04h:18m:24s
2025-12-18 16:57:55,389 [INFO] Trainer: Train [Step  17000/100000, Epoch 0.649/0] loss=2.1103 | lr_proj=4.835859e-04, lr_lora=9.671717e-05 | elapsed=04h:19m:57s
2025-12-18 16:59:25,172 [INFO] Trainer: Eval  [Step  17000/100000, Epoch 0.649/0] loss=2.2350 | lr_proj=4.835859e-04, lr_lora=9.671717e-05 | elapsed=04h:21m:27s
2025-12-18 16:59:25,228 [INFO] Trainer: Saved Projector to sft5/checkpoint_step17000.proj.pt
2025-12-18 16:59:25,342 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step17000.lora
2025-12-18 16:59:25,511 [INFO] Trainer: Saved config to sft5/checkpoint_step17000.config.json
2025-12-18 16:59:25,514 [INFO] Trainer: Removing sft5/checkpoint_step14000.config.json.*
2025-12-18 16:59:25,516 [INFO] Trainer: Removing sft5/checkpoint_step14000.optim.pt.*
2025-12-18 16:59:25,518 [INFO] Trainer: Removing sft5/checkpoint_step14000.proj.pt.*
2025-12-18 16:59:25,541 [INFO] Trainer: Removing sft5/checkpoint_step14000.lora.*
2025-12-18 17:00:55,370 [INFO] Trainer: Train [Step  17100/100000, Epoch 0.653/0] loss=2.5215 | lr_proj=4.834596e-04, lr_lora=9.669192e-05 | elapsed=04h:22m:57s
2025-12-18 17:02:47,243 [INFO] Trainer: Train [Step  17200/100000, Epoch 0.657/0] loss=1.8200 | lr_proj=4.833333e-04, lr_lora=9.666667e-05 | elapsed=04h:24m:49s
2025-12-18 17:04:32,294 [INFO] Trainer: Train [Step  17300/100000, Epoch 0.661/0] loss=1.9488 | lr_proj=4.832071e-04, lr_lora=9.664141e-05 | elapsed=04h:26m:34s
2025-12-18 17:06:29,597 [INFO] Trainer: Train [Step  17400/100000, Epoch 0.664/0] loss=2.7899 | lr_proj=4.830808e-04, lr_lora=9.661616e-05 | elapsed=04h:28m:32s
2025-12-18 17:08:17,877 [INFO] Trainer: Train [Step  17500/100000, Epoch 0.668/0] loss=2.0586 | lr_proj=4.829545e-04, lr_lora=9.659091e-05 | elapsed=04h:30m:20s
2025-12-18 17:10:02,192 [INFO] Trainer: Train [Step  17600/100000, Epoch 0.672/0] loss=2.1282 | lr_proj=4.828283e-04, lr_lora=9.656566e-05 | elapsed=04h:32m:04s
2025-12-18 17:11:45,074 [INFO] Trainer: Train [Step  17700/100000, Epoch 0.676/0] loss=2.1910 | lr_proj=4.827020e-04, lr_lora=9.654040e-05 | elapsed=04h:33m:47s
2025-12-18 17:13:38,481 [INFO] Trainer: Train [Step  17800/100000, Epoch 0.680/0] loss=1.7681 | lr_proj=4.825758e-04, lr_lora=9.651515e-05 | elapsed=04h:35m:41s
2025-12-18 17:15:39,540 [INFO] Trainer: Train [Step  17900/100000, Epoch 0.684/0] loss=1.8441 | lr_proj=4.824495e-04, lr_lora=9.648990e-05 | elapsed=04h:37m:42s
2025-12-18 17:17:56,598 [INFO] Trainer: Train [Step  18000/100000, Epoch 0.687/0] loss=1.3094 | lr_proj=4.823232e-04, lr_lora=9.646465e-05 | elapsed=04h:39m:59s
2025-12-18 17:19:26,935 [INFO] Trainer: Eval  [Step  18000/100000, Epoch 0.687/0] loss=2.1004 | lr_proj=4.823232e-04, lr_lora=9.646465e-05 | elapsed=04h:41m:29s
2025-12-18 17:19:26,997 [INFO] Trainer: Saved Projector to sft5/checkpoint_step18000.proj.pt
2025-12-18 17:19:27,151 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step18000.lora
2025-12-18 17:19:27,337 [INFO] Trainer: Saved config to sft5/checkpoint_step18000.config.json
2025-12-18 17:19:27,339 [INFO] Trainer: Removing sft5/checkpoint_step15000.lora.*
2025-12-18 17:19:27,345 [INFO] Trainer: Removing sft5/checkpoint_step15000.config.json.*
2025-12-18 17:19:27,413 [INFO] Trainer: Removing sft5/checkpoint_step15000.proj.pt.*
2025-12-18 17:19:27,416 [INFO] Trainer: Removing sft5/checkpoint_step15000.optim.pt.*
2025-12-18 17:21:43,561 [INFO] Trainer: Train [Step  18100/100000, Epoch 0.691/0] loss=2.2407 | lr_proj=4.821970e-04, lr_lora=9.643939e-05 | elapsed=04h:43m:46s
2025-12-18 17:25:10,543 [INFO] Trainer: Train [Step  18200/100000, Epoch 0.695/0] loss=1.8949 | lr_proj=4.820707e-04, lr_lora=9.641414e-05 | elapsed=04h:47m:13s
2025-12-18 17:27:52,047 [INFO] Trainer: Train [Step  18300/100000, Epoch 0.699/0] loss=2.8075 | lr_proj=4.819444e-04, lr_lora=9.638889e-05 | elapsed=04h:49m:54s
2025-12-18 17:30:57,021 [INFO] Trainer: Train [Step  18400/100000, Epoch 0.703/0] loss=1.4745 | lr_proj=4.818182e-04, lr_lora=9.636364e-05 | elapsed=04h:52m:59s
2025-12-18 17:33:27,926 [INFO] Trainer: Train [Step  18500/100000, Epoch 0.706/0] loss=2.4890 | lr_proj=4.816919e-04, lr_lora=9.633838e-05 | elapsed=04h:55m:30s
2025-12-18 17:35:38,615 [INFO] Trainer: Train [Step  18600/100000, Epoch 0.710/0] loss=1.5885 | lr_proj=4.815657e-04, lr_lora=9.631313e-05 | elapsed=04h:57m:41s
2025-12-18 17:37:58,675 [INFO] Trainer: Train [Step  18700/100000, Epoch 0.714/0] loss=2.9193 | lr_proj=4.814394e-04, lr_lora=9.628788e-05 | elapsed=05h:00m:01s
2025-12-18 17:39:59,198 [INFO] Trainer: Train [Step  18800/100000, Epoch 0.718/0] loss=1.6241 | lr_proj=4.813131e-04, lr_lora=9.626263e-05 | elapsed=05h:02m:01s
2025-12-18 17:42:00,582 [INFO] Trainer: Train [Step  18900/100000, Epoch 0.722/0] loss=1.7184 | lr_proj=4.811869e-04, lr_lora=9.623737e-05 | elapsed=05h:04m:03s
2025-12-18 17:43:56,074 [INFO] Trainer: Train [Step  19000/100000, Epoch 0.726/0] loss=2.2903 | lr_proj=4.810606e-04, lr_lora=9.621212e-05 | elapsed=05h:05m:58s
2025-12-18 17:45:26,877 [INFO] Trainer: Eval  [Step  19000/100000, Epoch 0.726/0] loss=2.0456 | lr_proj=4.810606e-04, lr_lora=9.621212e-05 | elapsed=05h:07m:29s
2025-12-18 17:45:26,905 [INFO] Trainer: Saved Projector to sft5/checkpoint_step19000.proj.pt
2025-12-18 17:45:27,032 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step19000.lora
2025-12-18 17:45:27,203 [INFO] Trainer: Saved config to sft5/checkpoint_step19000.config.json
2025-12-18 17:45:27,216 [INFO] Trainer: Removing sft5/checkpoint_step16000.lora.*
2025-12-18 17:45:27,228 [INFO] Trainer: Removing sft5/checkpoint_step16000.proj.pt.*
2025-12-18 17:45:27,231 [INFO] Trainer: Removing sft5/checkpoint_step16000.optim.pt.*
2025-12-18 17:45:27,247 [INFO] Trainer: Removing sft5/checkpoint_step16000.config.json.*
2025-12-18 17:47:41,308 [INFO] Trainer: Train [Step  19100/100000, Epoch 0.729/0] loss=2.5733 | lr_proj=4.809343e-04, lr_lora=9.618687e-05 | elapsed=05h:09m:43s
2025-12-18 17:50:00,842 [INFO] Trainer: Train [Step  19200/100000, Epoch 0.733/0] loss=1.4075 | lr_proj=4.808081e-04, lr_lora=9.616162e-05 | elapsed=05h:12m:03s
2025-12-18 17:52:01,451 [INFO] Trainer: Train [Step  19300/100000, Epoch 0.737/0] loss=2.4980 | lr_proj=4.806818e-04, lr_lora=9.613636e-05 | elapsed=05h:14m:04s
2025-12-18 17:53:53,800 [INFO] Trainer: Train [Step  19400/100000, Epoch 0.741/0] loss=2.5027 | lr_proj=4.805556e-04, lr_lora=9.611111e-05 | elapsed=05h:15m:56s
2025-12-18 17:56:04,469 [INFO] Trainer: Train [Step  19500/100000, Epoch 0.745/0] loss=2.3456 | lr_proj=4.804293e-04, lr_lora=9.608586e-05 | elapsed=05h:18m:07s
2025-12-18 17:57:28,748 [INFO] Trainer: Train [Step  19600/100000, Epoch 0.748/0] loss=2.4154 | lr_proj=4.803030e-04, lr_lora=9.606061e-05 | elapsed=05h:19m:31s
2025-12-18 17:59:31,812 [INFO] Trainer: Train [Step  19700/100000, Epoch 0.752/0] loss=2.9622 | lr_proj=4.801768e-04, lr_lora=9.603535e-05 | elapsed=05h:21m:34s
2025-12-18 18:01:08,885 [INFO] Trainer: Train [Step  19800/100000, Epoch 0.756/0] loss=2.2265 | lr_proj=4.800505e-04, lr_lora=9.601010e-05 | elapsed=05h:23m:11s
2025-12-18 18:02:57,187 [INFO] Trainer: Train [Step  19900/100000, Epoch 0.760/0] loss=2.0989 | lr_proj=4.799242e-04, lr_lora=9.598485e-05 | elapsed=05h:24m:59s
2025-12-18 18:04:51,679 [INFO] Trainer: Train [Step  20000/100000, Epoch 0.764/0] loss=1.6171 | lr_proj=4.797980e-04, lr_lora=9.595960e-05 | elapsed=05h:26m:54s
2025-12-18 18:06:22,876 [INFO] Trainer: Eval  [Step  20000/100000, Epoch 0.764/0] loss=2.0223 | lr_proj=4.797980e-04, lr_lora=9.595960e-05 | elapsed=05h:28m:25s
2025-12-18 18:06:22,974 [INFO] Trainer: Saved Projector to sft5/checkpoint_step20000.proj.pt
2025-12-18 18:06:23,204 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step20000.lora
2025-12-18 18:06:23,401 [INFO] Trainer: Saved config to sft5/checkpoint_step20000.config.json
2025-12-18 18:06:23,404 [INFO] Trainer: Removing sft5/checkpoint_step17000.proj.pt.*
2025-12-18 18:06:23,420 [INFO] Trainer: Removing sft5/checkpoint_step17000.lora.*
2025-12-18 18:06:23,424 [INFO] Trainer: Removing sft5/checkpoint_step17000.optim.pt.*
2025-12-18 18:06:23,444 [INFO] Trainer: Removing sft5/checkpoint_step17000.config.json.*
2025-12-18 18:08:02,448 [INFO] Trainer: Train [Step  20100/100000, Epoch 0.768/0] loss=2.5410 | lr_proj=4.796717e-04, lr_lora=9.593434e-05 | elapsed=05h:30m:05s
2025-12-18 18:09:50,855 [INFO] Trainer: Train [Step  20200/100000, Epoch 0.771/0] loss=2.3313 | lr_proj=4.795455e-04, lr_lora=9.590909e-05 | elapsed=05h:31m:53s
2025-12-18 18:11:40,145 [INFO] Trainer: Train [Step  20300/100000, Epoch 0.775/0] loss=1.5223 | lr_proj=4.794192e-04, lr_lora=9.588384e-05 | elapsed=05h:33m:42s
2025-12-18 18:13:13,921 [INFO] Trainer: Train [Step  20400/100000, Epoch 0.779/0] loss=1.9779 | lr_proj=4.792929e-04, lr_lora=9.585859e-05 | elapsed=05h:35m:16s
2025-12-18 18:14:54,569 [INFO] Trainer: Train [Step  20500/100000, Epoch 0.783/0] loss=2.3791 | lr_proj=4.791667e-04, lr_lora=9.583333e-05 | elapsed=05h:36m:57s
2025-12-18 18:16:36,434 [INFO] Trainer: Train [Step  20600/100000, Epoch 0.787/0] loss=1.2715 | lr_proj=4.790404e-04, lr_lora=9.580808e-05 | elapsed=05h:38m:38s
2025-12-18 18:18:23,303 [INFO] Trainer: Train [Step  20700/100000, Epoch 0.790/0] loss=2.1550 | lr_proj=4.789141e-04, lr_lora=9.578283e-05 | elapsed=05h:40m:25s
2025-12-18 18:20:25,938 [INFO] Trainer: Train [Step  20800/100000, Epoch 0.794/0] loss=3.1014 | lr_proj=4.787879e-04, lr_lora=9.575758e-05 | elapsed=05h:42m:28s
2025-12-18 18:21:57,241 [INFO] Trainer: Train [Step  20900/100000, Epoch 0.798/0] loss=1.4678 | lr_proj=4.786616e-04, lr_lora=9.573232e-05 | elapsed=05h:43m:59s
2025-12-18 18:23:36,770 [INFO] Trainer: Train [Step  21000/100000, Epoch 0.802/0] loss=1.6065 | lr_proj=4.785354e-04, lr_lora=9.570707e-05 | elapsed=05h:45m:39s
2025-12-18 18:25:07,263 [INFO] Trainer: Eval  [Step  21000/100000, Epoch 0.802/0] loss=1.9847 | lr_proj=4.785354e-04, lr_lora=9.570707e-05 | elapsed=05h:47m:09s
2025-12-18 18:25:07,328 [INFO] Trainer: Saved Projector to sft5/checkpoint_step21000.proj.pt
2025-12-18 18:25:07,490 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step21000.lora
2025-12-18 18:25:07,715 [INFO] Trainer: Saved config to sft5/checkpoint_step21000.config.json
2025-12-18 18:25:07,718 [INFO] Trainer: Removing sft5/checkpoint_step18000.optim.pt.*
2025-12-18 18:25:07,760 [INFO] Trainer: Removing sft5/checkpoint_step18000.config.json.*
2025-12-18 18:25:07,801 [INFO] Trainer: Removing sft5/checkpoint_step18000.proj.pt.*
2025-12-18 18:25:07,817 [INFO] Trainer: Removing sft5/checkpoint_step18000.lora.*
2025-12-18 18:26:57,479 [INFO] Trainer: Train [Step  21100/100000, Epoch 0.806/0] loss=1.6435 | lr_proj=4.784091e-04, lr_lora=9.568182e-05 | elapsed=05h:49m:00s
2025-12-18 18:28:18,087 [INFO] Trainer: Train [Step  21200/100000, Epoch 0.810/0] loss=2.0804 | lr_proj=4.782828e-04, lr_lora=9.565657e-05 | elapsed=05h:50m:20s
2025-12-18 18:29:55,782 [INFO] Trainer: Train [Step  21300/100000, Epoch 0.813/0] loss=1.6770 | lr_proj=4.781566e-04, lr_lora=9.563131e-05 | elapsed=05h:51m:58s
2025-12-18 18:32:25,753 [INFO] Trainer: Train [Step  21400/100000, Epoch 0.817/0] loss=1.2749 | lr_proj=4.780303e-04, lr_lora=9.560606e-05 | elapsed=05h:54m:28s
2025-12-18 18:34:08,172 [INFO] Trainer: Train [Step  21500/100000, Epoch 0.821/0] loss=2.4398 | lr_proj=4.779040e-04, lr_lora=9.558081e-05 | elapsed=05h:56m:10s
2025-12-18 18:35:48,580 [INFO] Trainer: Train [Step  21600/100000, Epoch 0.825/0] loss=1.4988 | lr_proj=4.777778e-04, lr_lora=9.555556e-05 | elapsed=05h:57m:51s
2025-12-18 18:37:42,375 [INFO] Trainer: Train [Step  21700/100000, Epoch 0.829/0] loss=1.7929 | lr_proj=4.776515e-04, lr_lora=9.553030e-05 | elapsed=05h:59m:44s
2025-12-18 18:39:46,856 [INFO] Trainer: Train [Step  21800/100000, Epoch 0.832/0] loss=2.2526 | lr_proj=4.775253e-04, lr_lora=9.550505e-05 | elapsed=06h:01m:49s
2025-12-18 18:42:12,020 [INFO] Trainer: Train [Step  21900/100000, Epoch 0.836/0] loss=2.0549 | lr_proj=4.773990e-04, lr_lora=9.547980e-05 | elapsed=06h:04m:14s
2025-12-18 18:44:26,991 [INFO] Trainer: Train [Step  22000/100000, Epoch 0.840/0] loss=1.8733 | lr_proj=4.772727e-04, lr_lora=9.545455e-05 | elapsed=06h:06m:29s
2025-12-18 18:45:58,428 [INFO] Trainer: Eval  [Step  22000/100000, Epoch 0.840/0] loss=1.9438 | lr_proj=4.772727e-04, lr_lora=9.545455e-05 | elapsed=06h:08m:00s
2025-12-18 18:45:58,482 [INFO] Trainer: Saved Projector to sft5/checkpoint_step22000.proj.pt
2025-12-18 18:45:58,685 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step22000.lora
2025-12-18 18:45:58,901 [INFO] Trainer: Saved config to sft5/checkpoint_step22000.config.json
2025-12-18 18:45:58,904 [INFO] Trainer: Removing sft5/checkpoint_step19000.proj.pt.*
2025-12-18 18:45:58,905 [INFO] Trainer: Removing sft5/checkpoint_step19000.config.json.*
2025-12-18 18:45:58,944 [INFO] Trainer: Removing sft5/checkpoint_step19000.lora.*
2025-12-18 18:45:58,950 [INFO] Trainer: Removing sft5/checkpoint_step19000.optim.pt.*
2025-12-18 18:48:17,381 [INFO] Trainer: Train [Step  22100/100000, Epoch 0.844/0] loss=2.1865 | lr_proj=4.771465e-04, lr_lora=9.542929e-05 | elapsed=06h:10m:19s
2025-12-18 18:51:32,019 [INFO] Trainer: Train [Step  22200/100000, Epoch 0.848/0] loss=1.5760 | lr_proj=4.770202e-04, lr_lora=9.540404e-05 | elapsed=06h:13m:34s
2025-12-18 18:54:07,053 [INFO] Trainer: Train [Step  22300/100000, Epoch 0.852/0] loss=1.2475 | lr_proj=4.768939e-04, lr_lora=9.537879e-05 | elapsed=06h:16m:09s
2025-12-18 18:56:36,880 [INFO] Trainer: Train [Step  22400/100000, Epoch 0.855/0] loss=1.9817 | lr_proj=4.767677e-04, lr_lora=9.535354e-05 | elapsed=06h:18m:39s
2025-12-18 18:59:18,328 [INFO] Trainer: Train [Step  22500/100000, Epoch 0.859/0] loss=1.7551 | lr_proj=4.766414e-04, lr_lora=9.532828e-05 | elapsed=06h:21m:20s
2025-12-18 19:01:36,486 [INFO] Trainer: Train [Step  22600/100000, Epoch 0.863/0] loss=1.8131 | lr_proj=4.765152e-04, lr_lora=9.530303e-05 | elapsed=06h:23m:39s
2025-12-18 19:03:59,875 [INFO] Trainer: Train [Step  22700/100000, Epoch 0.867/0] loss=1.2149 | lr_proj=4.763889e-04, lr_lora=9.527778e-05 | elapsed=06h:26m:02s
2025-12-18 19:06:29,355 [INFO] Trainer: Train [Step  22800/100000, Epoch 0.871/0] loss=1.8088 | lr_proj=4.762626e-04, lr_lora=9.525253e-05 | elapsed=06h:28m:31s
2025-12-18 19:08:31,313 [INFO] Trainer: Train [Step  22900/100000, Epoch 0.874/0] loss=1.6265 | lr_proj=4.761364e-04, lr_lora=9.522727e-05 | elapsed=06h:30m:33s
2025-12-18 19:11:11,027 [INFO] Trainer: Train [Step  23000/100000, Epoch 0.878/0] loss=1.7763 | lr_proj=4.760101e-04, lr_lora=9.520202e-05 | elapsed=06h:33m:13s
2025-12-18 19:12:41,772 [INFO] Trainer: Eval  [Step  23000/100000, Epoch 0.878/0] loss=1.8864 | lr_proj=4.760101e-04, lr_lora=9.520202e-05 | elapsed=06h:34m:44s
2025-12-18 19:12:41,826 [INFO] Trainer: Saved Projector to sft5/checkpoint_step23000.proj.pt
2025-12-18 19:12:41,983 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step23000.lora
2025-12-18 19:12:42,206 [INFO] Trainer: Saved config to sft5/checkpoint_step23000.config.json
2025-12-18 19:12:42,209 [INFO] Trainer: Removing sft5/checkpoint_step20000.optim.pt.*
2025-12-18 19:12:42,227 [INFO] Trainer: Removing sft5/checkpoint_step20000.lora.*
2025-12-18 19:12:42,233 [INFO] Trainer: Removing sft5/checkpoint_step20000.proj.pt.*
2025-12-18 19:12:42,258 [INFO] Trainer: Removing sft5/checkpoint_step20000.config.json.*
2025-12-18 19:15:14,224 [INFO] Trainer: Train [Step  23100/100000, Epoch 0.882/0] loss=1.8045 | lr_proj=4.758838e-04, lr_lora=9.517677e-05 | elapsed=06h:37m:16s
2025-12-18 19:17:38,067 [INFO] Trainer: Train [Step  23200/100000, Epoch 0.886/0] loss=1.2569 | lr_proj=4.757576e-04, lr_lora=9.515152e-05 | elapsed=06h:39m:40s
2025-12-18 19:19:27,867 [INFO] Trainer: Train [Step  23300/100000, Epoch 0.890/0] loss=1.8915 | lr_proj=4.756313e-04, lr_lora=9.512626e-05 | elapsed=06h:41m:30s
2025-12-18 19:21:27,418 [INFO] Trainer: Train [Step  23400/100000, Epoch 0.894/0] loss=1.2117 | lr_proj=4.755051e-04, lr_lora=9.510101e-05 | elapsed=06h:43m:29s
2025-12-18 19:23:31,872 [INFO] Trainer: Train [Step  23500/100000, Epoch 0.897/0] loss=1.4793 | lr_proj=4.753788e-04, lr_lora=9.507576e-05 | elapsed=06h:45m:34s
2025-12-18 19:25:08,366 [INFO] Trainer: Train [Step  23600/100000, Epoch 0.901/0] loss=2.3968 | lr_proj=4.752525e-04, lr_lora=9.505051e-05 | elapsed=06h:47m:10s
2025-12-18 19:26:46,739 [INFO] Trainer: Train [Step  23700/100000, Epoch 0.905/0] loss=1.7378 | lr_proj=4.751263e-04, lr_lora=9.502525e-05 | elapsed=06h:48m:49s
2025-12-18 19:28:19,344 [INFO] Trainer: Train [Step  23800/100000, Epoch 0.909/0] loss=2.1125 | lr_proj=4.750000e-04, lr_lora=9.500000e-05 | elapsed=06h:50m:21s
2025-12-18 19:30:10,089 [INFO] Trainer: Train [Step  23900/100000, Epoch 0.913/0] loss=1.2309 | lr_proj=4.748737e-04, lr_lora=9.497475e-05 | elapsed=06h:52m:12s
2025-12-18 19:31:55,357 [INFO] Trainer: Train [Step  24000/100000, Epoch 0.916/0] loss=2.2323 | lr_proj=4.747475e-04, lr_lora=9.494949e-05 | elapsed=06h:53m:57s
2025-12-18 19:33:25,722 [INFO] Trainer: Eval  [Step  24000/100000, Epoch 0.916/0] loss=1.8615 | lr_proj=4.747475e-04, lr_lora=9.494949e-05 | elapsed=06h:55m:28s
2025-12-18 19:33:25,866 [INFO] Trainer: Saved Projector to sft5/checkpoint_step24000.proj.pt
2025-12-18 19:33:26,031 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step24000.lora
2025-12-18 19:33:26,241 [INFO] Trainer: Saved config to sft5/checkpoint_step24000.config.json
2025-12-18 19:33:26,243 [INFO] Trainer: Removing sft5/checkpoint_step21000.proj.pt.*
2025-12-18 19:33:26,269 [INFO] Trainer: Removing sft5/checkpoint_step21000.lora.*
2025-12-18 19:33:26,274 [INFO] Trainer: Removing sft5/checkpoint_step21000.optim.pt.*
2025-12-18 19:33:26,294 [INFO] Trainer: Removing sft5/checkpoint_step21000.config.json.*
2025-12-18 19:35:00,999 [INFO] Trainer: Train [Step  24100/100000, Epoch 0.920/0] loss=1.3000 | lr_proj=4.746212e-04, lr_lora=9.492424e-05 | elapsed=06h:57m:03s
2025-12-18 19:36:51,123 [INFO] Trainer: Train [Step  24200/100000, Epoch 0.924/0] loss=1.1087 | lr_proj=4.744949e-04, lr_lora=9.489899e-05 | elapsed=06h:58m:53s
2025-12-18 19:38:19,775 [INFO] Trainer: Train [Step  24300/100000, Epoch 0.928/0] loss=1.7508 | lr_proj=4.743687e-04, lr_lora=9.487374e-05 | elapsed=07h:00m:22s
2025-12-18 19:39:45,707 [INFO] Trainer: Train [Step  24400/100000, Epoch 0.932/0] loss=2.3568 | lr_proj=4.742424e-04, lr_lora=9.484848e-05 | elapsed=07h:01m:48s
2025-12-18 19:41:39,659 [INFO] Trainer: Train [Step  24500/100000, Epoch 0.936/0] loss=1.2225 | lr_proj=4.741162e-04, lr_lora=9.482323e-05 | elapsed=07h:03m:42s
2025-12-18 19:43:26,253 [INFO] Trainer: Train [Step  24600/100000, Epoch 0.939/0] loss=2.0218 | lr_proj=4.739899e-04, lr_lora=9.479798e-05 | elapsed=07h:05m:28s
2025-12-18 19:45:03,309 [INFO] Trainer: Train [Step  24700/100000, Epoch 0.943/0] loss=1.3594 | lr_proj=4.738636e-04, lr_lora=9.477273e-05 | elapsed=07h:07m:05s
2025-12-18 19:46:43,077 [INFO] Trainer: Train [Step  24800/100000, Epoch 0.947/0] loss=1.5215 | lr_proj=4.737374e-04, lr_lora=9.474747e-05 | elapsed=07h:08m:45s
2025-12-18 19:48:38,788 [INFO] Trainer: Train [Step  24900/100000, Epoch 0.951/0] loss=1.5779 | lr_proj=4.736111e-04, lr_lora=9.472222e-05 | elapsed=07h:10m:41s
2025-12-18 19:50:11,811 [INFO] Trainer: Train [Step  25000/100000, Epoch 0.955/0] loss=1.6083 | lr_proj=4.734848e-04, lr_lora=9.469697e-05 | elapsed=07h:12m:14s
2025-12-18 19:51:41,749 [INFO] Trainer: Eval  [Step  25000/100000, Epoch 0.955/0] loss=1.8609 | lr_proj=4.734848e-04, lr_lora=9.469697e-05 | elapsed=07h:13m:44s
2025-12-18 19:51:41,875 [INFO] Trainer: Saved Projector to sft5/checkpoint_step25000.proj.pt
2025-12-18 19:51:42,049 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step25000.lora
2025-12-18 19:51:42,221 [INFO] Trainer: Saved config to sft5/checkpoint_step25000.config.json
2025-12-18 19:51:42,223 [INFO] Trainer: Removing sft5/checkpoint_step22000.config.json.*
2025-12-18 19:51:42,243 [INFO] Trainer: Removing sft5/checkpoint_step22000.lora.*
2025-12-18 19:51:42,248 [INFO] Trainer: Removing sft5/checkpoint_step22000.optim.pt.*
2025-12-18 19:51:42,266 [INFO] Trainer: Removing sft5/checkpoint_step22000.proj.pt.*
2025-12-18 19:53:04,392 [INFO] Trainer: Train [Step  25100/100000, Epoch 0.958/0] loss=2.3901 | lr_proj=4.733586e-04, lr_lora=9.467172e-05 | elapsed=07h:15m:06s
2025-12-18 19:54:36,082 [INFO] Trainer: Train [Step  25200/100000, Epoch 0.962/0] loss=2.0687 | lr_proj=4.732323e-04, lr_lora=9.464646e-05 | elapsed=07h:16m:38s
2025-12-18 19:56:24,779 [INFO] Trainer: Train [Step  25300/100000, Epoch 0.966/0] loss=2.7704 | lr_proj=4.731061e-04, lr_lora=9.462121e-05 | elapsed=07h:18m:27s
2025-12-18 19:58:15,235 [INFO] Trainer: Train [Step  25400/100000, Epoch 0.970/0] loss=2.0350 | lr_proj=4.729798e-04, lr_lora=9.459596e-05 | elapsed=07h:20m:17s
2025-12-18 20:00:25,343 [INFO] Trainer: Train [Step  25500/100000, Epoch 0.974/0] loss=2.0121 | lr_proj=4.728535e-04, lr_lora=9.457071e-05 | elapsed=07h:22m:27s
2025-12-18 20:02:08,336 [INFO] Trainer: Train [Step  25600/100000, Epoch 0.978/0] loss=1.9202 | lr_proj=4.727273e-04, lr_lora=9.454545e-05 | elapsed=07h:24m:10s
2025-12-18 20:03:49,611 [INFO] Trainer: Train [Step  25700/100000, Epoch 0.981/0] loss=1.9514 | lr_proj=4.726010e-04, lr_lora=9.452020e-05 | elapsed=07h:25m:52s
2025-12-18 20:05:29,613 [INFO] Trainer: Train [Step  25800/100000, Epoch 0.985/0] loss=1.8886 | lr_proj=4.724747e-04, lr_lora=9.449495e-05 | elapsed=07h:27m:32s
2025-12-18 20:07:27,721 [INFO] Trainer: Train [Step  25900/100000, Epoch 0.989/0] loss=2.2207 | lr_proj=4.723485e-04, lr_lora=9.446970e-05 | elapsed=07h:29m:30s
2025-12-18 20:09:48,355 [INFO] Trainer: Train [Step  26000/100000, Epoch 0.993/0] loss=1.9810 | lr_proj=4.722222e-04, lr_lora=9.444444e-05 | elapsed=07h:31m:50s
2025-12-18 20:11:18,717 [INFO] Trainer: Eval  [Step  26000/100000, Epoch 0.993/0] loss=1.8308 | lr_proj=4.722222e-04, lr_lora=9.444444e-05 | elapsed=07h:33m:21s
2025-12-18 20:11:18,798 [INFO] Trainer: Saved Projector to sft5/checkpoint_step26000.proj.pt
2025-12-18 20:11:18,993 [INFO] Trainer: Saved LoRa adapters to sft5/checkpoint_step26000.lora
2025-12-18 20:11:19,229 [INFO] Trainer: Saved config to sft5/checkpoint_step26000.config.json
2025-12-18 20:11:19,232 [INFO] Trainer: Removing sft5/checkpoint_step23000.optim.pt.*
2025-12-18 20:11:19,248 [INFO] Trainer: Removing sft5/checkpoint_step23000.lora.*
2025-12-18 20:11:19,258 [INFO] Trainer: Removing sft5/checkpoint_step23000.config.json.*
2025-12-18 20:11:19,266 [INFO] Trainer: Removing sft5/checkpoint_step23000.proj.pt.*
2025-12-18 20:13:27,429 [INFO] Trainer: Train [Step  26100/100000, Epoch 0.997/0] loss=1.6272 | lr_proj=4.720960e-04, lr_lora=9.441919e-05 | elapsed=07h:35m:29s
2025-12-18 20:15:44,841 [INFO] Trainer: Train [Step  26200/100000, Epoch 1.000/0] loss=1.8727 | lr_proj=4.719697e-04, lr_lora=9.439394e-05 | elapsed=07h:37m:47s
2025-12-18 20:18:23,337 [INFO] Trainer: Train [Step  26300/100000, Epoch 1.004/0] loss=1.3063 | lr_proj=4.718434e-04, lr_lora=9.436869e-05 | elapsed=07h:40m:25s
2025-12-18 20:21:04,723 [INFO] Trainer: Train [Step  26400/100000, Epoch 1.008/0] loss=1.5551 | lr_proj=4.717172e-04, lr_lora=9.434343e-05 | elapsed=07h:43m:07s
2025-12-18 20:23:48,949 [INFO] Trainer: Train [Step  26500/100000, Epoch 1.012/0] loss=2.8775 | lr_proj=4.715909e-04, lr_lora=9.431818e-05 | elapsed=07h:45m:51s
2025-12-18 20:26:18,838 [INFO] Trainer: Train [Step  26600/100000, Epoch 1.016/0] loss=1.0058 | lr_proj=4.714646e-04, lr_lora=9.429293e-05 | elapsed=07h:48m:21s
slurmstepd: error: *** JOB 770366 ON r6i2n7 CANCELLED AT 2025-12-18T20:28:36 DUE TO TIME LIMIT ***
Loading pytorch-gpu/py3/2.6.0
  Loading requirement: llvm/15.0.6 gcc/11.4.1 cuda/12.4.1 nccl/2.21.5-1-cuda
    cudnn/9.2.0.82-cuda openmpi/4.1.5-cuda intel-mkl/2020.4 magma/2.8.0-cuda
    sox/14.4.2 hdf5/1.12.0-mpi-cuda libjpeg-turbo/2.1.3 ffmpeg/6.1.1
    openjdk/11.0.2
2025-12-19 08:10:08,759 [INFO] train: CUDA available: True
2025-12-19 08:10:08,760 [INFO] train: Device count: 1
2025-12-19 08:10:08,760 [INFO] train: device: cuda, dtype: torch.float32
2025-12-19 08:10:08,905 [INFO] Embedder: Loaded /lustre/fsmisc/dataset/HuggingFace_Models/utter-project/mHuBERT-147, embedding_dim=768, sample_rate=16000 downsample_ratio=320
2025-12-19 08:10:11,173 [INFO] Backbone: Loaded Tokenizer from /lustre/fsmisc/dataset/HuggingFace_Models/utter-project/EuroLLM-1.7B-Instruct
2025-12-19 08:10:12,795 [INFO] Backbone: Loaded LLM model from /lustre/fsmisc/dataset/HuggingFace_Models/utter-project/EuroLLM-1.7B-Instruct
2025-12-19 08:10:13,709 [INFO] Backbone: Loaded LoRa adapters from sft5/checkpoint_step26000.lora
2025-12-19 08:10:13,710 [INFO] Projector: Initializing Projector {'path': 'sft5/checkpoint_step26000.proj.pt', 'stack_size': 8, 'rank_dim': 256}, audio_embedding_dim=768, llm_embedding_dim=2048
2025-12-19 08:10:13,845 [INFO] Projector: Loaded Projector from sft5/checkpoint_step26000.proj.pt
2025-12-19 08:10:15,615 [INFO] Dataset: sample=0 prompt=
Transcribe then translate into en.
[ASR]  target=No gens. [STT] None.<|im_end|>
2025-12-19 08:10:16,830 [INFO] Dataset: sample=1000 prompt=
Transcribe.
[ASR]  target=Si la Candelera plora, l'hivern fora; i si riu, ni hivern ni estiu.<|im_end|>
2025-12-19 08:10:17,968 [INFO] Dataset: sample=2000 prompt=
Translate into en.
[STT]  target=This applies both to Catholicism and to other beliefs.<|im_end|>
2025-12-19 08:10:19,124 [INFO] Dataset: sample=3000 prompt=
Transcribe then translate into en.
[ASR]  target=Això és una nació, un plebiscit quotidià entorn d'això. [STT] This is a nation, a daily plebiscite around this same thing.<|im_end|>
2025-12-19 08:10:20,284 [INFO] Dataset: sample=4000 prompt=
Transcribe.
[ASR]  target=Qualsevol altra que se li atribueixi.<|im_end|>
2025-12-19 08:10:21,419 [INFO] Dataset: sample=5000 prompt=
Translate into en.
[STT]  target=Seen from the coast, its name describes its appearance.<|im_end|>
2025-12-19 08:10:22,467 [INFO] Dataset: sample=6000 prompt=
Transcribe then translate into en.
[ASR]  target=Tenen cinc estadis nimfals en el seu desenvolupament. [STT] They have five nymph stages in their development.<|im_end|>
2025-12-19 08:10:23,514 [INFO] Dataset: sample=7000 prompt=
Transcribe.
[ASR]  target=Feblesa natural de la justícia en les confederacions.<|im_end|>
2025-12-19 08:10:24,697 [INFO] Dataset: sample=8000 prompt=
Translate into en.
[STT]  target=Some works or a number of styles of music are often identified with one of these types of texture.<|im_end|>
2025-12-19 08:10:25,700 [INFO] Dataset: sample=9000 prompt=
Transcribe then translate into en.
[ASR]  target=La política de les administracions públiques fins fa molt poc ha estat equivocada. [STT] Recently the public administration policy has been very wrong.<|im_end|>
2025-12-19 08:10:26,906 [INFO] Dataset: sample=10000 prompt=
Transcribe.
[ASR]  target=Deixem per un moment de banda el problema afegit de votar per Internet.<|im_end|>
2025-12-19 08:10:27,988 [INFO] Dataset: sample=11000 prompt=
Translate into en.
[STT]  target=The barber came home and took the opportunity to clean the necks.<|im_end|>
2025-12-19 08:10:29,079 [INFO] Dataset: sample=12000 prompt=
Transcribe then translate into en.
[ASR]  target=Pluja de març, dóna faves i furta blat. [STT] The rain of March is good for vegetables but harmful for wheat.<|im_end|>
2025-12-19 08:10:30,164 [INFO] Dataset: sample=13000 prompt=
Transcribe.
[ASR]  target=Venim de Dosaigües.<|im_end|>
2025-12-19 08:10:31,282 [INFO] Dataset: sample=14000 prompt=
Translate into en.
[STT]  target=The French line has different fortifications.<|im_end|>
2025-12-19 08:10:32,474 [INFO] Dataset: sample=15000 prompt=
Transcribe then translate into en.
[ASR]  target=Té, per tant, un valor de canvi. [STT] It therefore has exchange value.<|im_end|>
2025-12-19 08:10:33,607 [INFO] Dataset: sample=16000 prompt=
Transcribe.
[ASR]  target=L'escassetat de les eleccions exposa l'Estat a grans crisis.<|im_end|>
2025-12-19 08:10:34,637 [INFO] Dataset: sample=17000 prompt=
Translate into en.
[STT]  target=He was interned in a concentration camp when the Spanish Civil War had ended.<|im_end|>
2025-12-19 08:10:35,794 [INFO] Dataset: sample=18000 prompt=
Transcribe then translate into en.
[ASR]  target=No obstant això, van ser l'última victòria de la revolta. [STT] However, this was the last victory in the rebellion.<|im_end|>
2025-12-19 08:10:36,857 [INFO] Dataset: sample=19000 prompt=
Transcribe.
[ASR]  target=Era clergue que picava alt.<|im_end|>
2025-12-19 08:10:38,028 [INFO] Dataset: sample=20000 prompt=
Translate into en.
[STT]  target=This signaled the beginning of the end of the First World War.<|im_end|>
2025-12-19 08:10:39,269 [INFO] Dataset: sample=21000 prompt=
Transcribe then translate into en.
[ASR]  target=Tots sembla que són obra d'una persona amb grans coneixements literaris. [STT] Apparently they are the work of a person with an extensive knowledge on literature.<|im_end|>
2025-12-19 08:10:40,434 [INFO] Dataset: sample=22000 prompt=
Transcribe.
[ASR]  target=Els uns són rars i els altres impotents.<|im_end|>
2025-12-19 08:10:41,549 [INFO] Dataset: sample=23000 prompt=
Translate into en.
[STT]  target=Only one proposal per bidder for each may be submitted.<|im_end|>
2025-12-19 08:10:42,710 [INFO] Dataset: sample=24000 prompt=
Transcribe then translate into en.
[ASR]  target=Constava de propietaris agrícoles terratinents que podien viure completament de les seves rendes. [STT] It consisted of landowning agricultural proprietors who could live completely on their incomes.<|im_end|>
2025-12-19 08:10:43,925 [INFO] Dataset: sample=25000 prompt=
Transcribe.
[ASR]  target=Cal dir que els ordinals són molt confusos per aquest personatge.<|im_end|>
2025-12-19 08:10:44,986 [INFO] Dataset: sample=26000 prompt=
Translate into en.
[STT]  target=Every day they validate the information introduced in the Register by different users.<|im_end|>
2025-12-19 08:10:46,078 [INFO] Dataset: sample=27000 prompt=
Transcribe then translate into en.
[ASR]  target=Un règim repressiu revolucionari es va imposar. [STT] A repressive revolutionary regime was imposed.<|im_end|>
2025-12-19 08:10:47,271 [INFO] Dataset: sample=28000 prompt=
Transcribe.
[ASR]  target=A què es refereix?<|im_end|>
2025-12-19 08:10:48,444 [INFO] Dataset: sample=29000 prompt=
Translate into en.
[STT]  target=He was professor of Mathematics and Astronomy.<|im_end|>
2025-12-19 08:10:49,580 [INFO] Dataset: sample=30000 prompt=
Transcribe then translate into en.
[ASR]  target=Aber die eigentliche Party steigt woanders. [STT] But the actual party is somewhere else.<|im_end|>
2025-12-19 08:10:50,717 [INFO] Dataset: sample=31000 prompt=
Transcribe.
[ASR]  target=Dabei werden meistens virale, aber gelegentlich auch bakterielle oder künstliche Gene übertragen.<|im_end|>
2025-12-19 08:10:51,767 [INFO] Dataset: sample=32000 prompt=
Translate into en.
[STT]  target=Many are suffering from post-traumatic symptoms and are being looked after by the university’s psychological service.<|im_end|>
2025-12-19 08:10:53,069 [INFO] Dataset: sample=33000 prompt=
Transcribe then translate into en.
[ASR]  target=Jedoch kam es zu keinerlei Feindberührung, und die Flotte kehrte nach Truk zurück. [STT] But no enemy contact whatsoever occurred and the fleet returned to Truk.<|im_end|>
2025-12-19 08:10:54,121 [INFO] Dataset: sample=34000 prompt=
Transcribe.
[ASR]  target=Die Systematik der Schlangen ist noch Gegenstand aktueller Untersuchungen.<|im_end|>
2025-12-19 08:10:55,301 [INFO] Dataset: sample=35000 prompt=
Translate into en.
[STT]  target=However, the Federal Constitution confers extensive powers on the Confederation.<|im_end|>
2025-12-19 08:10:56,812 [INFO] Dataset: sample=36000 prompt=
Transcribe then translate into en.
[ASR]  target=Ich dachte, das wäre ein freies Land. [STT] I thought this was a free land.<|im_end|>
2025-12-19 08:10:57,915 [INFO] Dataset: sample=37000 prompt=
Transcribe.
[ASR]  target=Sie können auch nicht hüpfen und springen.<|im_end|>
2025-12-19 08:10:59,098 [INFO] Dataset: sample=38000 prompt=
Translate into en.
[STT]  target=Additionally, there were special features for Para Para and Two Mix.<|im_end|>
2025-12-19 08:11:00,368 [INFO] Dataset: sample=39000 prompt=
Transcribe then translate into en.
[ASR]  target=Die Röhren saßen auf einem organischen Substrat und vermehrten sich durch Knospung. [STT] The tubes rested on an organic substrate and multiplied by budding.<|im_end|>
2025-12-19 08:11:01,543 [INFO] Dataset: sample=40000 prompt=
Transcribe.
[ASR]  target=Der Verbandsgemeinde gehörten die Stadt Bad Ems und acht eigenständige Ortsgemeinden an.<|im_end|>
2025-12-19 08:11:02,590 [INFO] Dataset: sample=41000 prompt=
Translate into en.
[STT]  target=He despairs and breaks from his former life.<|im_end|>
2025-12-19 08:11:03,729 [INFO] Dataset: sample=42000 prompt=
Transcribe then translate into en.
[ASR]  target=Das Feld Müllensiefen wurde abgetrennt, um ein neues Bergwerk zu errichten. [STT] The Müllensiefen field was separated to build a new mine.<|im_end|>
2025-12-19 08:11:04,863 [INFO] Dataset: sample=43000 prompt=
Transcribe.
[ASR]  target=Anschließend nahm sie einen Lehrauftrag am University College London wahr.<|im_end|>
2025-12-19 08:11:06,024 [INFO] Dataset: sample=44000 prompt=
Translate into en.
[STT]  target=No systematic order can be found.<|im_end|>
2025-12-19 08:11:07,180 [INFO] Dataset: sample=45000 prompt=
Transcribe then translate into en.
[ASR]  target=Ihr letzter Wunsch war es, noch einmal einen Frühling zu erleben. [STT] It was her final wish, to live to see another spring season.<|im_end|>
2025-12-19 08:11:08,332 [INFO] Dataset: sample=46000 prompt=
Transcribe.
[ASR]  target=Die Ortschaft liegt genau zwischen zwei Hügeln.<|im_end|>
2025-12-19 08:11:09,427 [INFO] Dataset: sample=47000 prompt=
Translate into en.
[STT]  target=What’s the diagnosis?<|im_end|>
2025-12-19 08:11:10,485 [INFO] Dataset: sample=48000 prompt=
Transcribe then translate into en.
[ASR]  target=Die Sparschweine stehen im Wohnzimmer. [STT] The piggy banks are in the living room.<|im_end|>
2025-12-19 08:11:11,527 [INFO] Dataset: sample=49000 prompt=
Transcribe.
[ASR]  target=Sie liegt am Fluss Suir und hat incl.<|im_end|>
2025-12-19 08:11:12,719 [INFO] Dataset: sample=50000 prompt=
Translate into en.
[STT]  target=An engine that fitted the according vehicle was developed.<|im_end|>
2025-12-19 08:11:13,781 [INFO] Dataset: sample=51000 prompt=
Transcribe then translate into en.
[ASR]  target=Die Bahn verkehrt von Mitte Mai bis Mitte September; ihre Benutzung ist kostenlos. [STT] The train runs from mid-May through mid-September; it is free to ride.<|im_end|>
2025-12-19 08:11:15,011 [INFO] Dataset: sample=52000 prompt=
Transcribe.
[ASR]  target=Dort wird der Fluss auch von zwei kleinen Brücken überspannt.<|im_end|>
2025-12-19 08:11:16,165 [INFO] Dataset: sample=53000 prompt=
Translate into en.
[STT]  target=The waterfall is a popular tourist destination in the region.<|im_end|>
2025-12-19 08:11:17,374 [INFO] Dataset: sample=54000 prompt=
Transcribe then translate into en.
[ASR]  target=Die Brennerei befindet sich heute in Besitz des japanisch-amerikanischen Beam Suntory-Konzerns. [STT] Today, the distillery is in possession of the Japanese-American corporation Beam Suntory.<|im_end|>
2025-12-19 08:11:18,443 [INFO] Dataset: sample=55000 prompt=
Transcribe.
[ASR]  target=Seien Sie nicht so knauserig!<|im_end|>
2025-12-19 08:11:19,761 [INFO] Dataset: sample=56000 prompt=
Translate into en.
[STT]  target=On the map, they look like a single city.<|im_end|>
2025-12-19 08:11:21,042 [INFO] Dataset: sample=57000 prompt=
Transcribe then translate into en.
[ASR]  target=Sechs Männer wurden daraufhin an Bord festgesetzt und medizinisch versorgt. [STT] Six men were then boarded and medically attended.<|im_end|>
2025-12-19 08:11:22,078 [INFO] Dataset: sample=58000 prompt=
Transcribe.
[ASR]  target=Bitte haben Sie Verständnis, dass der Termin nicht verschoben werden kann.<|im_end|>
2025-12-19 08:11:23,219 [INFO] Dataset: sample=59000 prompt=
Translate into en.
[STT]  target=The roller has to be calibrated prior to assessing the bearing capacity of the subsoil.<|im_end|>
2025-12-19 08:11:24,256 [INFO] Dataset: sample=60000 prompt=
Transcribe then translate into ca.
[ASR]  target=A group of people are gathered in a public area. [STT] Un grup de persones es reuneixen en una zona pública.<|im_end|>
2025-12-19 08:11:25,355 [INFO] Dataset: sample=61000 prompt=
Transcribe.
[ASR]  target=During filming, rumours sparked of a relationship between Judd and Lohan.<|im_end|>
2025-12-19 08:11:26,437 [INFO] Dataset: sample=62000 prompt=
Translate into ca.
[STT]  target=Intento fer alguna cosa nova amb cada llançament de Snog.<|im_end|>
2025-12-19 08:11:27,527 [INFO] Dataset: sample=63000 prompt=
Transcribe then translate into ca.
[ASR]  target=Speakers in Brazil are found in the Roraima Indigenous Terra Raposa. [STT] Al Brasil, els oradors es troben a la Terra Indígena Raposa de Roraima.<|im_end|>
2025-12-19 08:11:28,645 [INFO] Dataset: sample=64000 prompt=
Transcribe.
[ASR]  target=The capital is Kaga Bandoro.<|im_end|>
2025-12-19 08:11:29,706 [INFO] Dataset: sample=65000 prompt=
Translate into ca.
[STT]  target=La filatura d’anells és un dels mètodes de filatura més comuns del món.<|im_end|>
2025-12-19 08:11:30,855 [INFO] Dataset: sample=66000 prompt=
Transcribe then translate into ca.
[ASR]  target=Plants are usually re-potted according to the size of their root system. [STT] Les plantes se solen trasplantar segons la mida de les arrels.<|im_end|>
2025-12-19 08:11:31,932 [INFO] Dataset: sample=67000 prompt=
Transcribe.
[ASR]  target=The island is in the hurricane belt and prone to severe weather.<|im_end|>
2025-12-19 08:11:33,020 [INFO] Dataset: sample=68000 prompt=
Translate into ca.
[STT]  target=Manteniu l’interès en la vostra carrera, per humil que sigui.<|im_end|>
2025-12-19 08:11:34,075 [INFO] Dataset: sample=69000 prompt=
Transcribe then translate into ca.
[ASR]  target=When tending their young, Western grebe parents use different types of vocalization to communicate. [STT] Quan tenen cura del seus fills, els cabussons occidentals fan servir diferents tipus de vocalització per a comunicar-se.<|im_end|>
2025-12-19 08:11:35,185 [INFO] Dataset: sample=70000 prompt=
Transcribe.
[ASR]  target=He resides in Waunakee, Wisconsin.<|im_end|>
2025-12-19 08:11:36,291 [INFO] Dataset: sample=71000 prompt=
Translate into ca.
[STT]  target=Després d’això, en lloc d’unir-se a les reserves, va ser escollit com a candidat a oficial.<|im_end|>
2025-12-19 08:11:37,352 [INFO] Dataset: sample=72000 prompt=
Transcribe then translate into ca.
[ASR]  target=After much debate, "The Virginian-Pilot" was taken off of the selling block. [STT] Després de debatre molt, “The Virginian-Pilot” es va treure del bloc de vendes.<|im_end|>
2025-12-19 08:11:38,464 [INFO] Dataset: sample=73000 prompt=
Transcribe.
[ASR]  target=The idler arm bolts to the vehicle's frame or subframe.<|im_end|>
2025-12-19 08:11:39,593 [INFO] Dataset: sample=74000 prompt=
Translate into ca.
[STT]  target=Els discos s’actualitzaven cada poques setmanes.<|im_end|>
2025-12-19 08:11:40,662 [INFO] Dataset: sample=75000 prompt=
Transcribe then translate into ca.
[ASR]  target=Later it was on the southwest corner. [STT] Més tard va ser a la cantonada sud-oest.<|im_end|>
2025-12-19 08:11:41,787 [INFO] Dataset: sample=76000 prompt=
Transcribe.
[ASR]  target=It also increased safety for construction workers.<|im_end|>
2025-12-19 08:11:42,866 [INFO] Dataset: sample=77000 prompt=
Translate into ca.
[STT]  target=No obstant, en alguns casos, s’han trobat marques estranyes o empremtes prop del lloc.<|im_end|>
2025-12-19 08:11:43,927 [INFO] Dataset: sample=78000 prompt=
Transcribe then translate into ca.
[ASR]  target=Devil's club reproduces by forming clonal colonies through a layering process. [STT] L’«Oplopanax horridus» es reprodueix establint colònies clonals mitjançant un procés d’estratificació.<|im_end|>
2025-12-19 08:11:44,996 [INFO] Dataset: sample=79000 prompt=
Transcribe.
[ASR]  target=This caused massive Sinhalese migration to the south and west of the island.<|im_end|>
2025-12-19 08:11:46,114 [INFO] Dataset: sample=80000 prompt=
Translate into ca.
[STT]  target=No hauríem d’haver anat per aquest camí.<|im_end|>
2025-12-19 08:11:47,232 [INFO] Dataset: sample=81000 prompt=
Transcribe then translate into ca.
[ASR]  target=Fireworks and music are also a major part of the activities for the weekend. [STT] Els focs artificials i la música també són una part important de les activitats del cap de setmana.<|im_end|>
2025-12-19 08:11:48,726 [INFO] Dataset: sample=82000 prompt=
Transcribe.
[ASR]  target=The sentences he was using were not his own.<|im_end|>
2025-12-19 08:11:49,849 [INFO] Dataset: sample=83000 prompt=
Translate into ca.
[STT]  target=Van ser els primers astrònoms i els primers a anomenar els planetes.<|im_end|>
2025-12-19 08:11:50,945 [INFO] Dataset: sample=84000 prompt=
Transcribe then translate into ca.
[ASR]  target=My name is painted on the door outside. [STT] El meu nom està pintat fora, a la porta.<|im_end|>
2025-12-19 08:11:52,023 [INFO] Dataset: sample=85000 prompt=
Transcribe.
[ASR]  target=Following her undergraduate degree, Morgan studied at the University of Manchester.<|im_end|>
2025-12-19 08:11:53,113 [INFO] Dataset: sample=86000 prompt=
Translate into ca.
[STT]  target=Aquesta feina va ser un èxit i va portar a gent a convertir-se a Crist.<|im_end|>
2025-12-19 08:11:54,214 [INFO] Dataset: sample=87000 prompt=
Transcribe then translate into ca.
[ASR]  target=Other companies integrated into the development include Seagate and Samsung. [STT] Altres companyies integrades en el desenvolupament inclouen Seagate i Samsung.<|im_end|>
2025-12-19 08:11:55,290 [INFO] Dataset: sample=88000 prompt=
Transcribe.
[ASR]  target=Webster University's athletic mascot is the Gorlok.<|im_end|>
2025-12-19 08:11:56,358 [INFO] Dataset: sample=89000 prompt=
Translate into ca.
[STT]  target=El noi va anar a buscar a l'home anglès.<|im_end|>
2025-12-19 08:11:57,418 [INFO] Dataset: sample=90000 prompt=
Transcribe then translate into de.
[ASR]  target=Jason flew a "Starfire". [STT] Jason flog einen „Starfire“.<|im_end|>
2025-12-19 08:11:58,512 [INFO] Dataset: sample=91000 prompt=
Transcribe.
[ASR]  target=A total of six soundtracks were released by Sony Music Entertainment.<|im_end|>
2025-12-19 08:11:59,559 [INFO] Dataset: sample=92000 prompt=
Translate into de.
[STT]  target=Es wird vorrangig für Eishockey verwendet, und es ist das Heimstadion der Pelicans.<|im_end|>
2025-12-19 08:12:00,615 [INFO] Dataset: sample=93000 prompt=
Transcribe then translate into de.
[ASR]  target=Note: Winners in each election are shown in bold. [STT] „Hinweis: Gewinner jeder Wahl sind“ fett gedruckt.<|im_end|>
2025-12-19 08:12:01,681 [INFO] Dataset: sample=94000 prompt=
Transcribe.
[ASR]  target=large group of people holding hands on the beach<|im_end|>
2025-12-19 08:12:02,758 [INFO] Dataset: sample=95000 prompt=
Translate into de.
[STT]  target=Die Chrysler-Versionen dieses Fahrzeugs verdienen einen Artikel.<|im_end|>
2025-12-19 08:12:03,864 [INFO] Dataset: sample=96000 prompt=
Transcribe then translate into de.
[ASR]  target=The bay is connected by the Saimaa Canal to the lake Saimaa in Finland. [STT] Die Bucht ist durch den Saimaa-Kanal mit dem Saimaa-See in Finnland verbunden.<|im_end|>
2025-12-19 08:12:04,966 [INFO] Dataset: sample=97000 prompt=
Transcribe.
[ASR]  target=To find the ibid.<|im_end|>
2025-12-19 08:12:06,036 [INFO] Dataset: sample=98000 prompt=
Translate into de.
[STT]  target=Seine Hauptsorge galt einer guten und wirkungsvollen Priesterschaft.<|im_end|>
2025-12-19 08:12:07,103 [INFO] Dataset: sample=99000 prompt=
Transcribe then translate into de.
[ASR]  target=You'll need three teaspoonsful for that. [STT] Dafür benötigen Sie drei Teelöffel.<|im_end|>
2025-12-19 08:12:08,181 [INFO] Dataset: sample=100000 prompt=
Transcribe.
[ASR]  target=The Kahl Educational Center is located in downtown Davenport.<|im_end|>
2025-12-19 08:12:09,288 [INFO] Dataset: sample=101000 prompt=
Translate into de.
[STT]  target=Sheridans Schauspielkarriere umfasste Theater-, Fernseh- und Spielfilmproduktionen.<|im_end|>
2025-12-19 08:12:10,355 [INFO] Dataset: sample=102000 prompt=
Transcribe then translate into de.
[ASR]  target=Bussin was born in Toronto and grew up in the Dawes Road area. [STT] Bussin wurde in Toronto geboren und wuchs in der Gegend von Dawes Road auf.<|im_end|>
2025-12-19 08:12:11,463 [INFO] Dataset: sample=103000 prompt=
Transcribe.
[ASR]  target=It's hard to convey ideas in short one hundred and fifty character fields.<|im_end|>
2025-12-19 08:12:12,557 [INFO] Dataset: sample=104000 prompt=
Translate into de.
[STT]  target=Wie sind die Abmessungen Ihres Pakets?<|im_end|>
2025-12-19 08:12:13,619 [INFO] Dataset: sample=105000 prompt=
Transcribe then translate into de.
[ASR]  target=Subsequently he served with the army of occupation in France. [STT] Anschließend diente er bei der Besatzungsarmee in Frankreich.<|im_end|>
2025-12-19 08:12:14,707 [INFO] Dataset: sample=106000 prompt=
Transcribe.
[ASR]  target=Wilson has a long record of service with the Metropolitan Community Church.<|im_end|>
2025-12-19 08:12:15,763 [INFO] Dataset: sample=107000 prompt=
Translate into de.
[STT]  target=Ich möchte einen beliebigen Song von Fonzworth Bentley mit dem Dienst Slacker hören.<|im_end|>
2025-12-19 08:12:16,839 [INFO] Dataset: sample=108000 prompt=
Transcribe then translate into de.
[ASR]  target=The television version of Sodor appears quite larger and has more industry. [STT] Die Fernsehversion von Sodor scheint deutlich größer und hat mehr Fabriken.<|im_end|>
2025-12-19 08:12:17,958 [INFO] Dataset: sample=109000 prompt=
Transcribe.
[ASR]  target=Usually, by default, a thermodynamic state is taken to be one of thermodynamic equilibrium.<|im_end|>
2025-12-19 08:12:19,009 [INFO] Dataset: sample=110000 prompt=
Translate into de.
[STT]  target=Dieser Satz wurde speziell für dich verfasst.<|im_end|>
2025-12-19 08:12:20,151 [INFO] Dataset: sample=111000 prompt=
Transcribe then translate into de.
[ASR]  target=Joe heads the competition until he performs poorly in the long jump. [STT] Joe lag an der Spitze, bis er eine schlechte Leistung im Weitsprung zeigte.<|im_end|>
2025-12-19 08:12:21,217 [INFO] Dataset: sample=112000 prompt=
Transcribe.
[ASR]  target=The governor received a suspicious package and the police had to investigate.<|im_end|>
2025-12-19 08:12:22,330 [INFO] Dataset: sample=113000 prompt=
Translate into de.
[STT]  target=Für einige Zeit begünstigten ihn die Umstände.<|im_end|>
2025-12-19 08:12:23,430 [INFO] Dataset: sample=114000 prompt=
Transcribe then translate into de.
[ASR]  target=Their language is called Chichewa. [STT] Ihre Sprache heißt Chichewa.<|im_end|>
2025-12-19 08:12:24,514 [INFO] Dataset: sample=115000 prompt=
Transcribe.
[ASR]  target=Egyptian vulture populations have declined in most parts of its range.<|im_end|>
2025-12-19 08:12:25,566 [INFO] Dataset: sample=116000 prompt=
Translate into de.
[STT]  target=Die meisten der genutzten Häuser sind seitdem abgerissen worden, aber die Straße gibt es noch.<|im_end|>
2025-12-19 08:12:26,672 [INFO] Dataset: sample=117000 prompt=
Transcribe then translate into de.
[ASR]  target=This species is listed as a migratory bird under the Migratory Bird Treaty Act. [STT] Die Art ist im Migratory Bird Treaty Act als Zugvogel gelistet.<|im_end|>
2025-12-19 08:12:27,732 [INFO] Dataset: sample=118000 prompt=
Transcribe.
[ASR]  target=Have you ever tried racing on a Segway?<|im_end|>
2025-12-19 08:12:28,821 [INFO] Dataset: sample=119000 prompt=
Translate into de.
[STT]  target=Zwei junge Fußballspieler in blau-gelben Trikots kämpfen um den Ball.<|im_end|>
2025-12-19 08:12:29,920 [INFO] Dataset: sample=120000 prompt=
Transcribe then translate into en.
[ASR]  target=La boca es un gran indicador de la salud del individuo. [STT] The mouth is a good indicator for an individual’s general health.<|im_end|>
2025-12-19 08:12:31,006 [INFO] Dataset: sample=121000 prompt=
Transcribe.
[ASR]  target=La avenida comienza en la intersección de la Avenida Sáenz y la calle "Beazley".<|im_end|>
2025-12-19 08:12:32,158 [INFO] Dataset: sample=122000 prompt=
Translate into en.
[STT]  target=The extinction would seem to have selectively killed marine life and life on earth at different times.<|im_end|>
2025-12-19 08:12:33,271 [INFO] Dataset: sample=123000 prompt=
Transcribe then translate into en.
[ASR]  target=A su memoria está dedicado el festival internacional de poesía de Rosario [STT] The international poetry festival of Rosario is dedicated to their memory.<|im_end|>
2025-12-19 08:12:34,338 [INFO] Dataset: sample=124000 prompt=
Transcribe.
[ASR]  target=Es uno de los ruegos más solemnes de la Iglesia católica.<|im_end|>
2025-12-19 08:12:35,436 [INFO] Dataset: sample=125000 prompt=
Translate into en.
[STT]  target=It is aimed to a non-specialized audience and it is about mathematics, space, physics and nature.<|im_end|>
2025-12-19 08:12:36,542 [INFO] Dataset: sample=126000 prompt=
Transcribe then translate into en.
[ASR]  target=Inventó el microscopio electrónico de barrido. [STT] They invented the scanning electron microscope.<|im_end|>
2025-12-19 08:12:37,595 [INFO] Dataset: sample=127000 prompt=
Transcribe.
[ASR]  target=Durante este período, Gondwana comenzaría a dividirse.<|im_end|>
2025-12-19 08:12:38,648 [INFO] Dataset: sample=128000 prompt=
Translate into en.
[STT]  target=Elliott Richard Friedman, in "Who wrote the Bible?<|im_end|>
2025-12-19 08:12:39,735 [INFO] Dataset: sample=129000 prompt=
Transcribe then translate into en.
[ASR]  target=Puede ser fácilmente distinguido de sus vecinos por su forma característica. [STT] It can be easily identified among its neighbors for its typical shape.<|im_end|>
2025-12-19 08:12:40,842 [INFO] Dataset: sample=130000 prompt=
Transcribe.
[ASR]  target=Como cachorro su pelaje es oscuro.<|im_end|>
2025-12-19 08:12:41,928 [INFO] Dataset: sample=131000 prompt=
Translate into en.
[STT]  target=It is Ursula’s best friend.<|im_end|>
2025-12-19 08:12:43,029 [INFO] Dataset: sample=132000 prompt=
Transcribe then translate into en.
[ASR]  target=Racimos que se alargan progresivamente durante la antesis. [STT] Clusters that progressively extend during anthesis.<|im_end|>
2025-12-19 08:12:44,103 [INFO] Dataset: sample=133000 prompt=
Transcribe.
[ASR]  target=Se encuentra en Mauricio.<|im_end|>
2025-12-19 08:12:45,223 [INFO] Dataset: sample=134000 prompt=
Translate into en.
[STT]  target=The service was only provided by small cable companies<|im_end|>
2025-12-19 08:12:46,300 [INFO] Dataset: sample=135000 prompt=
Transcribe then translate into en.
[ASR]  target=Nació en Trujillo. [STT] He was born in Trujillo<|im_end|>
2025-12-19 08:12:47,360 [INFO] Dataset: sample=136000 prompt=
Transcribe.
[ASR]  target=Es de las pocas pertenencias que tenía cuando murió en Hiva Oa.<|im_end|>
2025-12-19 08:12:48,431 [INFO] Dataset: sample=137000 prompt=
Translate into en.
[STT]  target=During his last trip, he would transport wool and leather.<|im_end|>
2025-12-19 08:12:49,485 [INFO] Dataset: sample=138000 prompt=
Transcribe then translate into en.
[ASR]  target=Sombras e intrigas del poder real en la Argentina". [STT] “Shadows and intrigues surrounding the real power in Argentina”.<|im_end|>
2025-12-19 08:12:50,633 [INFO] Dataset: sample=139000 prompt=
Transcribe.
[ASR]  target=Ostras, hace dos meses que me ha caducado el carnet de conducir.<|im_end|>
2025-12-19 08:12:52,176 [INFO] Dataset: sample=140000 prompt=
Translate into en.
[STT]  target=It is the view, as a spectator, that Joaquín Sabina has about Raphael.<|im_end|>
2025-12-19 08:12:53,281 [INFO] Dataset: sample=141000 prompt=
Transcribe then translate into en.
[ASR]  target=En el subcontinente indio en la actualidad, el sari es considerado un ícono cultural. [STT] The sari is considered a cultural icon nowadays, in the Indian subcontinent.<|im_end|>
2025-12-19 08:12:54,354 [INFO] Dataset: sample=142000 prompt=
Transcribe.
[ASR]  target=El hummus queda entonces más fino.<|im_end|>
2025-12-19 08:12:55,428 [INFO] Dataset: sample=143000 prompt=
Translate into en.
[STT]  target=It must be noted that the pasiego concept of neighborhood is different from the traditional.<|im_end|>
2025-12-19 08:12:56,489 [INFO] Dataset: sample=144000 prompt=
Transcribe then translate into en.
[ASR]  target=las monjas, dando alaridos , huyeron del coro. [STT] The nuns yelling, escaped from the choir.<|im_end|>
2025-12-19 08:12:57,548 [INFO] Dataset: sample=145000 prompt=
Transcribe.
[ASR]  target=Sin embargo, esta característica no se muestra en las ilustraciones de los manuscritos.<|im_end|>
2025-12-19 08:12:58,634 [INFO] Dataset: sample=146000 prompt=
Translate into en.
[STT]  target=Among other activities, these includes touristic routes across the city, excursions and social events.<|im_end|>
2025-12-19 08:12:59,706 [INFO] Dataset: sample=147000 prompt=
Transcribe then translate into en.
[ASR]  target=Los primeros robots empleaban mecanismos de realimentación para corregir errores. [STT] The first robots used feedback mechanisms to correct errors.<|im_end|>
2025-12-19 08:13:00,779 [INFO] Dataset: sample=148000 prompt=
Transcribe.
[ASR]  target=Unos tienen la fama y otros cardan la lana.<|im_end|>
2025-12-19 08:13:01,839 [INFO] Dataset: sample=149000 prompt=
Translate into en.
[STT]  target=Years later, she was transferred to the Santa Clara convent.<|im_end|>
2025-12-19 08:13:03,083 [INFO] Dataset: sample=150000 prompt=
Transcribe then translate into en.
[ASR]  target=Ce sont des amendements de simplification. [STT] They are simplification amendments.<|im_end|>
2025-12-19 08:13:04,422 [INFO] Dataset: sample=151000 prompt=
Transcribe.
[ASR]  target=À l'origine, le magazine est tiré à exemplaires.<|im_end|>
2025-12-19 08:13:05,685 [INFO] Dataset: sample=152000 prompt=
Translate into en.
[STT]  target=Thirty-nine, Lucien Leducq street<|im_end|>
2025-12-19 08:13:06,786 [INFO] Dataset: sample=153000 prompt=
Transcribe then translate into en.
[ASR]  target=Les cheveux naturellement bouclés de Tori sont sa signature. [STT] Tori’s naturally curly hair is her trademark.<|im_end|>
2025-12-19 08:13:07,997 [INFO] Dataset: sample=154000 prompt=
Transcribe.
[ASR]  target=Détachée de son support, elle est conservée au musée égyptien du Caire.<|im_end|>
2025-12-19 08:13:09,161 [INFO] Dataset: sample=155000 prompt=
Translate into en.
[STT]  target=The arrivals of Kennedy and Deakin were not followed by immediate success.<|im_end|>
2025-12-19 08:13:10,209 [INFO] Dataset: sample=156000 prompt=
Transcribe then translate into en.
[ASR]  target=Pendant la guerre, la princesse tombe plusieurs fois malade. [STT] During the war, the princess falls ill several times.<|im_end|>
2025-12-19 08:13:11,536 [INFO] Dataset: sample=157000 prompt=
Transcribe.
[ASR]  target=Merci… il m’est tout à fait impossible.<|im_end|>
2025-12-19 08:13:12,737 [INFO] Dataset: sample=158000 prompt=
Translate into en.
[STT]  target=One of the numerous finely illustrated keystones represented François.<|im_end|>
2025-12-19 08:13:14,049 [INFO] Dataset: sample=159000 prompt=
Transcribe then translate into en.
[ASR]  target=Allée des Monts d'Arrée, trente-cinq mille cinq cents Vitré [STT] Monts d'Arrée Driveaway, thirty-five thousand five hundred, Vitré<|im_end|>
2025-12-19 08:13:15,209 [INFO] Dataset: sample=160000 prompt=
Transcribe.
[ASR]  target=Ils publient leurs vidéos sur la plateforme YouTube.<|im_end|>
2025-12-19 08:13:16,465 [INFO] Dataset: sample=161000 prompt=
Translate into en.
[STT]  target=So Brett explains to them his method for fighting cancer.<|im_end|>
2025-12-19 08:13:17,641 [INFO] Dataset: sample=162000 prompt=
Transcribe then translate into en.
[ASR]  target=Et elle se hâta de rentrer à son comptoir. [STT] And she hurried back to her counter.<|im_end|>
2025-12-19 08:13:18,745 [INFO] Dataset: sample=163000 prompt=
Transcribe.
[ASR]  target=Les satellites sont également appelés Agena ferrets et heavy ferrets.<|im_end|>
2025-12-19 08:13:19,892 [INFO] Dataset: sample=164000 prompt=
Translate into en.
[STT]  target=He wasn’t listening at all, he was squeezing her desperately, his heart drowned in an immense sadness.<|im_end|>
2025-12-19 08:13:20,974 [INFO] Dataset: sample=165000 prompt=
Transcribe then translate into en.
[ASR]  target=Les vainqueurs du deuxième tour seront qualifiées pour les demi-finales. [STT] The winners of the second round will qualify for the semi-finals.<|im_end|>
2025-12-19 08:13:22,033 [INFO] Dataset: sample=166000 prompt=
Transcribe.
[ASR]  target=C'est une des langues bisayas.<|im_end|>
2025-12-19 08:13:23,158 [INFO] Dataset: sample=167000 prompt=
Translate into en.
[STT]  target=De Marambat Street at number six.<|im_end|>
2025-12-19 08:13:24,478 [INFO] Dataset: sample=168000 prompt=
Transcribe then translate into en.
[ASR]  target=Quand on parle de matraquage fiscal, c’est la réalité. [STT] When we talk about tax bullying, it is the reality.<|im_end|>
2025-12-19 08:13:25,693 [INFO] Dataset: sample=169000 prompt=
Transcribe.
[ASR]  target=Il existe également un accès ferroviaire.<|im_end|>
2025-12-19 08:13:26,911 [INFO] Dataset: sample=170000 prompt=
Translate into en.
[STT]  target=On the way, love will be there.<|im_end|>
2025-12-19 08:13:28,174 [INFO] Dataset: sample=171000 prompt=
Transcribe then translate into en.
[ASR]  target=Tiens ! vous avez un mari ? [STT] Well, hey! Do you have a husband?<|im_end|>
2025-12-19 08:13:29,358 [INFO] Dataset: sample=172000 prompt=
Transcribe.
[ASR]  target=Tout Rennes est content du résultat du match.<|im_end|>
2025-12-19 08:13:30,494 [INFO] Dataset: sample=173000 prompt=
Translate into en.
[STT]  target=The father didn’t let himself be circumvented.<|im_end|>
2025-12-19 08:13:31,648 [INFO] Dataset: sample=174000 prompt=
Transcribe then translate into en.
[ASR]  target=D'autres textes internationaux ou régionaux l'ont, dans les années suivantes, interdite également. [STT] Oher International and Regional texts has also banned it the following years.<|im_end|>
2025-12-19 08:13:32,808 [INFO] Dataset: sample=175000 prompt=
Transcribe.
[ASR]  target=Elle se rencontre dans la grotte Chenjia dans le xian de Songtao.<|im_end|>
2025-12-19 08:13:34,204 [INFO] Dataset: sample=176000 prompt=
Translate into en.
[STT]  target=Ah, that ! Won’t that big sick woman wake up now?<|im_end|>
2025-12-19 08:13:35,424 [INFO] Dataset: sample=177000 prompt=
Transcribe then translate into en.
[ASR]  target=La bataille fut un revers pour les Boers, bien que leurs pertes furent limitées. [STT] The battle was a problem for the Boers, although their losses were limited.<|im_end|>
2025-12-19 08:13:36,587 [INFO] Dataset: sample=178000 prompt=
Transcribe.
[ASR]  target=C'est le granit de la région qui permet l'étanchéité du lac.<|im_end|>
2025-12-19 08:13:37,684 [INFO] Dataset: sample=179000 prompt=
Translate into en.
[STT]  target=Common in all of France, until central Europe.<|im_end|>
2025-12-19 08:13:38,923 [INFO] Dataset: sample=180000 prompt=
Transcribe then translate into en.
[ASR]  target=Изменения касаются лишь приложения. [STT] Only the Annex is amended.<|im_end|>
2025-12-19 08:13:40,202 [INFO] Dataset: sample=181000 prompt=
Transcribe.
[ASR]  target=Ярмарка зашумела.<|im_end|>
2025-12-19 08:13:41,547 [INFO] Dataset: sample=182000 prompt=
Translate into en.
[STT]  target=Our role was to provide affordable diagnostic methods and to train veterinary staff.<|im_end|>
2025-12-19 08:13:42,799 [INFO] Dataset: sample=183000 prompt=
Transcribe then translate into en.
[ASR]  target=Я попрошу представителей вписать в бюллетени название государства, за которое они хотят голосовать. [STT] I will ask the representatives to write in the ballots the name of the state for which they want to vote.<|im_end|>
2025-12-19 08:13:44,050 [INFO] Dataset: sample=184000 prompt=
Transcribe.
[ASR]  target=Мне особенно приятно поздравить правительство и народ Республики Южный Судан с обретением независимости.<|im_end|>
2025-12-19 08:13:45,239 [INFO] Dataset: sample=185000 prompt=
Translate into en.
[STT]  target=The authors agreed with the logic of this approach.<|im_end|>
2025-12-19 08:13:46,392 [INFO] Dataset: sample=186000 prompt=
Transcribe then translate into en.
[ASR]  target=Общеизвестно, что в настоящее время переговоры зашли в тупик. [STT] It is well known that negotiations are currently at an impasse.<|im_end|>
2025-12-19 08:13:47,604 [INFO] Dataset: sample=187000 prompt=
Transcribe.
[ASR]  target=Необходимо добиться, чтобы нарождающиеся во многих частях планеты экономические системы обрели устойчивость и жизнеспособность.<|im_end|>
2025-12-19 08:13:48,716 [INFO] Dataset: sample=188000 prompt=
Translate into en.
[STT]  target=Spain reiterates its commitment to the disarmament mechanism.<|im_end|>
2025-12-19 08:13:49,876 [INFO] Dataset: sample=189000 prompt=
Transcribe then translate into en.
[ASR]  target=Мы достигли этапа, на котором имеется возможность оживить наш переговорный форум. [STT] We have reached a stage at which there is an opportunity to revive our negotiation forum.<|im_end|>
2025-12-19 08:13:51,177 [INFO] Dataset: sample=190000 prompt=
Transcribe.
[ASR]  target=Шестьдесят шестая сессия Генеральной Ассамблеи проходит в сложный для нашего региона политический момент.<|im_end|>
2025-12-19 08:13:52,396 [INFO] Dataset: sample=191000 prompt=
Translate into en.
[STT]  target=Improving the working methods of the Security Council is another important aspect of Council reform.<|im_end|>
2025-12-19 08:13:53,599 [INFO] Dataset: sample=192000 prompt=
Transcribe then translate into en.
[ASR]  target=Что касается Ирландии, то мы не так давно пробрели поучительный опыт миротворчества в Чаде. [STT] As for Ireland, we had an instructive recent experience of peacekeeping in Chad.<|im_end|>
2025-12-19 08:13:54,778 [INFO] Dataset: sample=193000 prompt=
Transcribe.
[ASR]  target=После того, как машину починили, проблема больше не возникает.<|im_end|>
2025-12-19 08:13:55,924 [INFO] Dataset: sample=194000 prompt=
Translate into en.
[STT]  target=Well, then we had an informal event with our fellow translators.<|im_end|>
2025-12-19 08:13:57,192 [INFO] Dataset: sample=195000 prompt=
Transcribe then translate into en.
[ASR]  target=Поэтому я и смотрю на это как на национальную инициативу. [STT] Therefore, I look at it as a national initiative.<|im_end|>
2025-12-19 08:13:58,499 [INFO] Dataset: sample=196000 prompt=
Transcribe.
[ASR]  target=В своем выступлении я расскажу вам о перспективах Фиджи, а также коснусь региональных перспектив.<|im_end|>
2025-12-19 08:13:59,916 [INFO] Dataset: sample=197000 prompt=
Translate into en.
[STT]  target=Today, space technology has become an integral part of the daily lives of people around the globe.<|im_end|>
2025-12-19 08:14:01,065 [INFO] Dataset: sample=198000 prompt=
Transcribe then translate into en.
[ASR]  target=Австралия готова активно содействовать важной работе этой группы. [STT] Australia stands ready to actively contribute to the important work of this group.<|im_end|>
2025-12-19 08:14:02,314 [INFO] Dataset: sample=199000 prompt=
Transcribe.
[ASR]  target=Я предоставляю слово представителю Сальвадора.<|im_end|>
2025-12-19 08:14:03,493 [INFO] Dataset: sample=200000 prompt=
Translate into en.
[STT]  target=And now we proceed to the regular agenda of the Conference on Disarmament.<|im_end|>
2025-12-19 08:14:04,593 [INFO] Dataset: sample=201000 prompt=
Transcribe then translate into en.
[ASR]  target=Никак не могли замену найти. [STT] They couldn’t find a replacement.<|im_end|>
2025-12-19 08:14:05,787 [INFO] Dataset: sample=202000 prompt=
Transcribe.
[ASR]  target=Как мне думается, тут нет нужды повторять аргументацию, которая весьма проста.<|im_end|>
2025-12-19 08:14:06,872 [INFO] Dataset: sample=203000 prompt=
Translate into en.
[STT]  target=Well, now I would like to go to the list of speakers for today.<|im_end|>
2025-12-19 08:14:08,116 [INFO] Dataset: sample=204000 prompt=
Transcribe then translate into en.
[ASR]  target=Очевидно, что имеется позитивная тенденция к началу переговоров по расщепляющемуся материалу и запасам. [STT] Obviously, there is a positive tendency towards the start of negotiations on fissile material and stockpiles.<|im_end|>
2025-12-19 08:14:09,338 [INFO] Dataset: sample=205000 prompt=
Transcribe.
[ASR]  target=Следующий выступающий — Канада.<|im_end|>
2025-12-19 08:14:10,520 [INFO] Dataset: sample=206000 prompt=
Translate into en.
[STT]  target=The truth is that I want peace.<|im_end|>
2025-12-19 08:14:11,648 [INFO] Dataset: sample=207000 prompt=
Transcribe then translate into en.
[ASR]  target=Соединенные Штаты вновь призывают правительство Сирии уважать права человека и достоинство своего народа. [STT] The United States once again calls on the Syrian Government to respect the human rights and dignity of its people.<|im_end|>
2025-12-19 08:14:12,892 [INFO] Dataset: sample=208000 prompt=
Transcribe.
[ASR]  target=Общества могут раскалываться.<|im_end|>
2025-12-19 08:14:14,032 [INFO] Dataset: sample=209000 prompt=
Translate into en.
[STT]  target=Japan acknowledges the important role of the experienced top leaders in strengthening of the humanitarian aid coordination.<|im_end|>
2025-12-19 08:14:14,652 [INFO] Dataset: Read dataset ./data/covost2/all.3tasks.train.tsv with 209496 samples (defaultdict(<class 'int'>, {'asr+stt': 69832, 'asr': 69832, 'stt': 69832}))
2025-12-19 08:14:14,654 [INFO] Dataset: sample=0 prompt=
Transcribe then translate into en.
[ASR]  target=Supervisa l'emissió de les resolucions de concessió de l'habitació. [STT] Supervises issuance of room concession decisions.<|im_end|>
2025-12-19 08:14:16,469 [INFO] Dataset: sample=1000 prompt=
Transcribe.
[ASR]  target=The Board's focus is on strategic management, not operational issues.<|im_end|>
2025-12-19 08:14:17,620 [INFO] Dataset: sample=2000 prompt=
Translate into en.
[STT]  target=Both donors and developing countries must fulfil their financial commitments.<|im_end|>
2025-12-19 08:14:17,732 [INFO] Dataset: Read dataset ./data/covost2/all.3tasks.test.tsv with 2100 samples (defaultdict(<class 'int'>, {'asr+stt': 700, 'asr': 700, 'stt': 700}))
2025-12-19 08:14:17,733 [INFO] Trainer: Initializing {'config': {'audio': {'path': '/lustre/fsmisc/dataset/HuggingFace_Models/utter-project/mHuBERT-147', 'l2_norm': False}, 'projector': {'path': 'sft5/checkpoint_step26000.proj.pt', 'stack_size': 8, 'rank_dim': 256}, 'llm': {'path': '/lustre/fsmisc/dataset/HuggingFace_Models/utter-project/EuroLLM-1.7B-Instruct'}, 'lora': {'path': 'sft5/checkpoint_step26000.lora', 'r': 16, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 'lora_dropout': 0.05, 'bias': 'none', 'task_type': 'CAUSAL_LM'}, 'asr_token': '[ASR]', 'stt_token': '[STT]'}, 'batch_size': 8, 'lr_proj': 0.0005, 'lr_lora': 0.0001, 'max_steps': 100000, 'max_epochs': 0, 'save_best_n': 3, 'eval_every': 1000, 'log_every': 100, 'accum_steps': 4, 'output_dir': 'sft5b', 'seed': 42}
2025-12-19 08:14:17,744 [INFO] Trainer: Trainable params in model: 7608576
2025-12-19 08:14:17,818 [INFO] Trainer: Initialized Sampler and DataLoader for train with batch_size=8 with 209496 samples
2025-12-19 08:14:17,820 [INFO] Trainer: Initialized Sampler and DataLoader for eval with batch_size=8 with 209496 samples
2025-12-19 08:14:17,823 [INFO] Trainer: Initialized AdamW optimizer with lr_proj=0.0005 lr_lora=0.0001
2025-12-19 08:14:17,823 [INFO] Trainer: Initialized Linear scheduler with warmup for 100000 steps, with 1000 warmup_steps
2025-12-19 08:14:17,824 [INFO] Trainer: Start training
2025-12-19 08:15:06,906 [INFO] Trainer: Train [Step    100/100000, Epoch 0.004/0] loss=1.4779 | lr_proj=1.250000e-05, lr_lora=2.500000e-06 | elapsed=00h:00m:49s
2025-12-19 08:15:53,763 [INFO] Trainer: Train [Step    200/100000, Epoch 0.008/0] loss=1.4498 | lr_proj=2.500000e-05, lr_lora=5.000000e-06 | elapsed=00h:01m:35s
2025-12-19 08:16:40,204 [INFO] Trainer: Train [Step    300/100000, Epoch 0.011/0] loss=1.6281 | lr_proj=3.750000e-05, lr_lora=7.500000e-06 | elapsed=00h:02m:22s
2025-12-19 08:17:28,751 [INFO] Trainer: Train [Step    400/100000, Epoch 0.015/0] loss=1.9580 | lr_proj=5.000000e-05, lr_lora=1.000000e-05 | elapsed=00h:03m:10s
2025-12-19 08:18:16,144 [INFO] Trainer: Train [Step    500/100000, Epoch 0.019/0] loss=1.8377 | lr_proj=6.250000e-05, lr_lora=1.250000e-05 | elapsed=00h:03m:58s
2025-12-19 08:19:04,288 [INFO] Trainer: Train [Step    600/100000, Epoch 0.023/0] loss=1.3274 | lr_proj=7.500000e-05, lr_lora=1.500000e-05 | elapsed=00h:04m:46s
2025-12-19 08:19:51,121 [INFO] Trainer: Train [Step    700/100000, Epoch 0.027/0] loss=2.2272 | lr_proj=8.750000e-05, lr_lora=1.750000e-05 | elapsed=00h:05m:33s
2025-12-19 08:20:40,170 [INFO] Trainer: Train [Step    800/100000, Epoch 0.031/0] loss=1.5852 | lr_proj=1.000000e-04, lr_lora=2.000000e-05 | elapsed=00h:06m:22s
2025-12-19 08:21:28,515 [INFO] Trainer: Train [Step    900/100000, Epoch 0.034/0] loss=1.4774 | lr_proj=1.125000e-04, lr_lora=2.250000e-05 | elapsed=00h:07m:10s
2025-12-19 08:22:15,743 [INFO] Trainer: Train [Step   1000/100000, Epoch 0.038/0] loss=1.6410 | lr_proj=1.250000e-04, lr_lora=2.500000e-05 | elapsed=00h:07m:57s
2025-12-19 08:23:45,248 [INFO] Trainer: Eval  [Step   1000/100000, Epoch 0.038/0] loss=1.7808 | lr_proj=1.250000e-04, lr_lora=2.500000e-05 | elapsed=00h:09m:27s
2025-12-19 08:23:45,379 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step1000.proj.pt
2025-12-19 08:23:45,622 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step1000.lora
2025-12-19 08:23:45,848 [INFO] Trainer: Saved config to sft5b/checkpoint_step1000.config.json
2025-12-19 08:24:33,275 [INFO] Trainer: Train [Step   1100/100000, Epoch 0.042/0] loss=1.6043 | lr_proj=1.375000e-04, lr_lora=2.750000e-05 | elapsed=00h:10m:15s
2025-12-19 08:25:21,075 [INFO] Trainer: Train [Step   1200/100000, Epoch 0.046/0] loss=1.7524 | lr_proj=1.500000e-04, lr_lora=3.000000e-05 | elapsed=00h:11m:03s
2025-12-19 08:26:08,485 [INFO] Trainer: Train [Step   1300/100000, Epoch 0.050/0] loss=1.3445 | lr_proj=1.625000e-04, lr_lora=3.250000e-05 | elapsed=00h:11m:50s
2025-12-19 08:26:54,939 [INFO] Trainer: Train [Step   1400/100000, Epoch 0.053/0] loss=1.5754 | lr_proj=1.750000e-04, lr_lora=3.500000e-05 | elapsed=00h:12m:37s
2025-12-19 08:27:42,388 [INFO] Trainer: Train [Step   1500/100000, Epoch 0.057/0] loss=1.6234 | lr_proj=1.875000e-04, lr_lora=3.750000e-05 | elapsed=00h:13m:24s
2025-12-19 08:28:31,200 [INFO] Trainer: Train [Step   1600/100000, Epoch 0.061/0] loss=1.7907 | lr_proj=2.000000e-04, lr_lora=4.000000e-05 | elapsed=00h:14m:13s
2025-12-19 08:29:16,229 [INFO] Trainer: Train [Step   1700/100000, Epoch 0.065/0] loss=1.5627 | lr_proj=2.125000e-04, lr_lora=4.250000e-05 | elapsed=00h:14m:58s
2025-12-19 08:30:05,541 [INFO] Trainer: Train [Step   1800/100000, Epoch 0.069/0] loss=1.8475 | lr_proj=2.250000e-04, lr_lora=4.500000e-05 | elapsed=00h:15m:47s
2025-12-19 08:30:53,055 [INFO] Trainer: Train [Step   1900/100000, Epoch 0.073/0] loss=1.6892 | lr_proj=2.375000e-04, lr_lora=4.750000e-05 | elapsed=00h:16m:35s
2025-12-19 08:31:39,782 [INFO] Trainer: Train [Step   2000/100000, Epoch 0.076/0] loss=2.0808 | lr_proj=2.500000e-04, lr_lora=5.000000e-05 | elapsed=00h:17m:21s
2025-12-19 08:33:08,674 [INFO] Trainer: Eval  [Step   2000/100000, Epoch 0.076/0] loss=1.7883 | lr_proj=2.500000e-04, lr_lora=5.000000e-05 | elapsed=00h:18m:50s
2025-12-19 08:33:08,777 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step2000.proj.pt
2025-12-19 08:33:08,936 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step2000.lora
2025-12-19 08:33:09,170 [INFO] Trainer: Saved config to sft5b/checkpoint_step2000.config.json
2025-12-19 08:33:56,922 [INFO] Trainer: Train [Step   2100/100000, Epoch 0.080/0] loss=1.8316 | lr_proj=2.625000e-04, lr_lora=5.250000e-05 | elapsed=00h:19m:39s
2025-12-19 08:34:41,940 [INFO] Trainer: Train [Step   2200/100000, Epoch 0.084/0] loss=2.0270 | lr_proj=2.750000e-04, lr_lora=5.500000e-05 | elapsed=00h:20m:24s
2025-12-19 08:35:30,459 [INFO] Trainer: Train [Step   2300/100000, Epoch 0.088/0] loss=1.6031 | lr_proj=2.875000e-04, lr_lora=5.750000e-05 | elapsed=00h:21m:12s
2025-12-19 08:36:17,468 [INFO] Trainer: Train [Step   2400/100000, Epoch 0.092/0] loss=1.9238 | lr_proj=3.000000e-04, lr_lora=6.000000e-05 | elapsed=00h:21m:59s
2025-12-19 08:37:04,129 [INFO] Trainer: Train [Step   2500/100000, Epoch 0.095/0] loss=1.9453 | lr_proj=3.125000e-04, lr_lora=6.250000e-05 | elapsed=00h:22m:46s
2025-12-19 08:37:50,436 [INFO] Trainer: Train [Step   2600/100000, Epoch 0.099/0] loss=1.7215 | lr_proj=3.250000e-04, lr_lora=6.500000e-05 | elapsed=00h:23m:32s
2025-12-19 08:38:37,328 [INFO] Trainer: Train [Step   2700/100000, Epoch 0.103/0] loss=2.0319 | lr_proj=3.375000e-04, lr_lora=6.750000e-05 | elapsed=00h:24m:19s
2025-12-19 08:39:24,392 [INFO] Trainer: Train [Step   2800/100000, Epoch 0.107/0] loss=1.5361 | lr_proj=3.500000e-04, lr_lora=7.000000e-05 | elapsed=00h:25m:06s
2025-12-19 08:40:14,362 [INFO] Trainer: Train [Step   2900/100000, Epoch 0.111/0] loss=1.7638 | lr_proj=3.625000e-04, lr_lora=7.250000e-05 | elapsed=00h:25m:56s
2025-12-19 08:41:01,431 [INFO] Trainer: Train [Step   3000/100000, Epoch 0.115/0] loss=2.2676 | lr_proj=3.750000e-04, lr_lora=7.500000e-05 | elapsed=00h:26m:43s
2025-12-19 08:42:30,906 [INFO] Trainer: Eval  [Step   3000/100000, Epoch 0.115/0] loss=1.7954 | lr_proj=3.750000e-04, lr_lora=7.500000e-05 | elapsed=00h:28m:13s
2025-12-19 08:42:30,994 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step3000.proj.pt
2025-12-19 08:42:31,203 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step3000.lora
2025-12-19 08:42:31,407 [INFO] Trainer: Saved config to sft5b/checkpoint_step3000.config.json
2025-12-19 08:43:18,748 [INFO] Trainer: Train [Step   3100/100000, Epoch 0.118/0] loss=1.7193 | lr_proj=3.875000e-04, lr_lora=7.750000e-05 | elapsed=00h:29m:00s
2025-12-19 08:44:08,108 [INFO] Trainer: Train [Step   3200/100000, Epoch 0.122/0] loss=1.6542 | lr_proj=4.000000e-04, lr_lora=8.000000e-05 | elapsed=00h:29m:50s
2025-12-19 08:44:56,725 [INFO] Trainer: Train [Step   3300/100000, Epoch 0.126/0] loss=1.6342 | lr_proj=4.125000e-04, lr_lora=8.250000e-05 | elapsed=00h:30m:38s
2025-12-19 08:45:43,711 [INFO] Trainer: Train [Step   3400/100000, Epoch 0.130/0] loss=1.4730 | lr_proj=4.250000e-04, lr_lora=8.500000e-05 | elapsed=00h:31m:25s
2025-12-19 08:46:32,121 [INFO] Trainer: Train [Step   3500/100000, Epoch 0.134/0] loss=1.7419 | lr_proj=4.375000e-04, lr_lora=8.750000e-05 | elapsed=00h:32m:14s
2025-12-19 08:47:19,539 [INFO] Trainer: Train [Step   3600/100000, Epoch 0.137/0] loss=1.9365 | lr_proj=4.500000e-04, lr_lora=9.000000e-05 | elapsed=00h:33m:01s
2025-12-19 08:48:06,427 [INFO] Trainer: Train [Step   3700/100000, Epoch 0.141/0] loss=1.6212 | lr_proj=4.625000e-04, lr_lora=9.250000e-05 | elapsed=00h:33m:48s
2025-12-19 08:48:55,136 [INFO] Trainer: Train [Step   3800/100000, Epoch 0.145/0] loss=1.5809 | lr_proj=4.750000e-04, lr_lora=9.500000e-05 | elapsed=00h:34m:37s
2025-12-19 08:49:40,956 [INFO] Trainer: Train [Step   3900/100000, Epoch 0.149/0] loss=1.6757 | lr_proj=4.875000e-04, lr_lora=9.750000e-05 | elapsed=00h:35m:23s
2025-12-19 08:50:27,480 [INFO] Trainer: Train [Step   4000/100000, Epoch 0.153/0] loss=1.5014 | lr_proj=5.000000e-04, lr_lora=1.000000e-04 | elapsed=00h:36m:09s
2025-12-19 08:51:55,646 [INFO] Trainer: Eval  [Step   4000/100000, Epoch 0.153/0] loss=1.7852 | lr_proj=5.000000e-04, lr_lora=1.000000e-04 | elapsed=00h:37m:37s
2025-12-19 08:51:55,735 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step4000.proj.pt
2025-12-19 08:51:55,903 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step4000.lora
2025-12-19 08:51:56,095 [INFO] Trainer: Saved config to sft5b/checkpoint_step4000.config.json
2025-12-19 08:51:56,097 [INFO] Trainer: Removing sft5b/checkpoint_step1000.config.json.*
2025-12-19 08:51:56,098 [INFO] Trainer: Removing sft5b/checkpoint_step1000.proj.pt.*
2025-12-19 08:51:56,100 [INFO] Trainer: Removing sft5b/checkpoint_step1000.lora.*
2025-12-19 08:51:56,110 [INFO] Trainer: Removing sft5b/checkpoint_step1000.optim.pt.*
2025-12-19 08:52:44,371 [INFO] Trainer: Train [Step   4100/100000, Epoch 0.157/0] loss=1.7761 | lr_proj=4.998737e-04, lr_lora=9.997475e-05 | elapsed=00h:38m:26s
2025-12-19 08:53:31,616 [INFO] Trainer: Train [Step   4200/100000, Epoch 0.160/0] loss=1.3596 | lr_proj=4.997475e-04, lr_lora=9.994949e-05 | elapsed=00h:39m:13s
2025-12-19 08:54:17,106 [INFO] Trainer: Train [Step   4300/100000, Epoch 0.164/0] loss=1.6844 | lr_proj=4.996212e-04, lr_lora=9.992424e-05 | elapsed=00h:39m:59s
2025-12-19 08:55:02,406 [INFO] Trainer: Train [Step   4400/100000, Epoch 0.168/0] loss=1.7018 | lr_proj=4.994949e-04, lr_lora=9.989899e-05 | elapsed=00h:40m:44s
2025-12-19 08:55:50,487 [INFO] Trainer: Train [Step   4500/100000, Epoch 0.172/0] loss=1.7147 | lr_proj=4.993687e-04, lr_lora=9.987374e-05 | elapsed=00h:41m:32s
2025-12-19 08:56:37,071 [INFO] Trainer: Train [Step   4600/100000, Epoch 0.176/0] loss=1.4522 | lr_proj=4.992424e-04, lr_lora=9.984848e-05 | elapsed=00h:42m:19s
2025-12-19 08:57:23,385 [INFO] Trainer: Train [Step   4700/100000, Epoch 0.179/0] loss=2.0800 | lr_proj=4.991162e-04, lr_lora=9.982323e-05 | elapsed=00h:43m:05s
2025-12-19 08:58:10,156 [INFO] Trainer: Train [Step   4800/100000, Epoch 0.183/0] loss=1.5517 | lr_proj=4.989899e-04, lr_lora=9.979798e-05 | elapsed=00h:43m:52s
2025-12-19 08:58:54,476 [INFO] Trainer: Train [Step   4900/100000, Epoch 0.187/0] loss=1.6563 | lr_proj=4.988636e-04, lr_lora=9.977273e-05 | elapsed=00h:44m:36s
2025-12-19 08:59:43,369 [INFO] Trainer: Train [Step   5000/100000, Epoch 0.191/0] loss=1.8630 | lr_proj=4.987374e-04, lr_lora=9.974747e-05 | elapsed=00h:45m:25s
2025-12-19 09:01:11,416 [INFO] Trainer: Eval  [Step   5000/100000, Epoch 0.191/0] loss=1.8273 | lr_proj=4.987374e-04, lr_lora=9.974747e-05 | elapsed=00h:46m:53s
2025-12-19 09:01:11,486 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step5000.proj.pt
2025-12-19 09:01:11,728 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step5000.lora
2025-12-19 09:01:11,920 [INFO] Trainer: Saved config to sft5b/checkpoint_step5000.config.json
2025-12-19 09:01:11,922 [INFO] Trainer: Removing sft5b/checkpoint_step2000.config.json.*
2025-12-19 09:01:11,923 [INFO] Trainer: Removing sft5b/checkpoint_step2000.optim.pt.*
2025-12-19 09:01:11,939 [INFO] Trainer: Removing sft5b/checkpoint_step2000.proj.pt.*
2025-12-19 09:01:11,942 [INFO] Trainer: Removing sft5b/checkpoint_step2000.lora.*
2025-12-19 09:02:01,036 [INFO] Trainer: Train [Step   5100/100000, Epoch 0.195/0] loss=1.4374 | lr_proj=4.986111e-04, lr_lora=9.972222e-05 | elapsed=00h:47m:43s
2025-12-19 09:02:48,016 [INFO] Trainer: Train [Step   5200/100000, Epoch 0.199/0] loss=1.9273 | lr_proj=4.984848e-04, lr_lora=9.969697e-05 | elapsed=00h:48m:30s
2025-12-19 09:03:36,499 [INFO] Trainer: Train [Step   5300/100000, Epoch 0.202/0] loss=1.3865 | lr_proj=4.983586e-04, lr_lora=9.967172e-05 | elapsed=00h:49m:18s
2025-12-19 09:04:24,023 [INFO] Trainer: Train [Step   5400/100000, Epoch 0.206/0] loss=1.4612 | lr_proj=4.982323e-04, lr_lora=9.964646e-05 | elapsed=00h:50m:06s
2025-12-19 09:05:13,152 [INFO] Trainer: Train [Step   5500/100000, Epoch 0.210/0] loss=1.9037 | lr_proj=4.981061e-04, lr_lora=9.962121e-05 | elapsed=00h:50m:55s
2025-12-19 09:05:59,438 [INFO] Trainer: Train [Step   5600/100000, Epoch 0.214/0] loss=1.8174 | lr_proj=4.979798e-04, lr_lora=9.959596e-05 | elapsed=00h:51m:41s
2025-12-19 09:06:45,574 [INFO] Trainer: Train [Step   5700/100000, Epoch 0.218/0] loss=1.5720 | lr_proj=4.978535e-04, lr_lora=9.957071e-05 | elapsed=00h:52m:27s
2025-12-19 09:07:34,061 [INFO] Trainer: Train [Step   5800/100000, Epoch 0.221/0] loss=1.8956 | lr_proj=4.977273e-04, lr_lora=9.954545e-05 | elapsed=00h:53m:16s
2025-12-19 09:08:22,388 [INFO] Trainer: Train [Step   5900/100000, Epoch 0.225/0] loss=1.6920 | lr_proj=4.976010e-04, lr_lora=9.952020e-05 | elapsed=00h:54m:04s
2025-12-19 09:09:11,292 [INFO] Trainer: Train [Step   6000/100000, Epoch 0.229/0] loss=1.4799 | lr_proj=4.974747e-04, lr_lora=9.949495e-05 | elapsed=00h:54m:53s
2025-12-19 09:10:39,816 [INFO] Trainer: Eval  [Step   6000/100000, Epoch 0.229/0] loss=1.7634 | lr_proj=4.974747e-04, lr_lora=9.949495e-05 | elapsed=00h:56m:21s
2025-12-19 09:10:39,857 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step6000.proj.pt
2025-12-19 09:10:40,008 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step6000.lora
2025-12-19 09:10:40,242 [INFO] Trainer: Saved config to sft5b/checkpoint_step6000.config.json
2025-12-19 09:10:40,247 [INFO] Trainer: Removing sft5b/checkpoint_step3000.lora.*
2025-12-19 09:10:40,262 [INFO] Trainer: Removing sft5b/checkpoint_step3000.proj.pt.*
2025-12-19 09:10:40,265 [INFO] Trainer: Removing sft5b/checkpoint_step3000.optim.pt.*
2025-12-19 09:10:40,282 [INFO] Trainer: Removing sft5b/checkpoint_step3000.config.json.*
2025-12-19 09:11:27,932 [INFO] Trainer: Train [Step   6100/100000, Epoch 0.233/0] loss=1.7171 | lr_proj=4.973485e-04, lr_lora=9.946970e-05 | elapsed=00h:57m:10s
2025-12-19 09:12:14,250 [INFO] Trainer: Train [Step   6200/100000, Epoch 0.237/0] loss=1.6635 | lr_proj=4.972222e-04, lr_lora=9.944444e-05 | elapsed=00h:57m:56s
2025-12-19 09:13:03,125 [INFO] Trainer: Train [Step   6300/100000, Epoch 0.241/0] loss=1.4529 | lr_proj=4.970960e-04, lr_lora=9.941919e-05 | elapsed=00h:58m:45s
2025-12-19 09:13:51,001 [INFO] Trainer: Train [Step   6400/100000, Epoch 0.244/0] loss=1.6932 | lr_proj=4.969697e-04, lr_lora=9.939394e-05 | elapsed=00h:59m:33s
2025-12-19 09:14:40,307 [INFO] Trainer: Train [Step   6500/100000, Epoch 0.248/0] loss=1.7366 | lr_proj=4.968434e-04, lr_lora=9.936869e-05 | elapsed=01h:00m:22s
2025-12-19 09:15:30,135 [INFO] Trainer: Train [Step   6600/100000, Epoch 0.252/0] loss=1.5752 | lr_proj=4.967172e-04, lr_lora=9.934343e-05 | elapsed=01h:01m:12s
2025-12-19 09:16:17,505 [INFO] Trainer: Train [Step   6700/100000, Epoch 0.256/0] loss=1.5866 | lr_proj=4.965909e-04, lr_lora=9.931818e-05 | elapsed=01h:01m:59s
2025-12-19 09:17:04,505 [INFO] Trainer: Train [Step   6800/100000, Epoch 0.260/0] loss=1.9087 | lr_proj=4.964646e-04, lr_lora=9.929293e-05 | elapsed=01h:02m:46s
2025-12-19 09:17:53,485 [INFO] Trainer: Train [Step   6900/100000, Epoch 0.263/0] loss=1.5330 | lr_proj=4.963384e-04, lr_lora=9.926768e-05 | elapsed=01h:03m:35s
2025-12-19 09:18:43,014 [INFO] Trainer: Train [Step   7000/100000, Epoch 0.267/0] loss=1.3176 | lr_proj=4.962121e-04, lr_lora=9.924242e-05 | elapsed=01h:04m:25s
2025-12-19 09:20:11,110 [INFO] Trainer: Eval  [Step   7000/100000, Epoch 0.267/0] loss=1.7941 | lr_proj=4.962121e-04, lr_lora=9.924242e-05 | elapsed=01h:05m:53s
2025-12-19 09:20:11,180 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step7000.proj.pt
2025-12-19 09:20:11,414 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step7000.lora
2025-12-19 09:20:11,679 [INFO] Trainer: Saved config to sft5b/checkpoint_step7000.config.json
2025-12-19 09:20:11,682 [INFO] Trainer: Removing sft5b/checkpoint_step4000.proj.pt.*
2025-12-19 09:20:11,687 [INFO] Trainer: Removing sft5b/checkpoint_step4000.lora.*
2025-12-19 09:20:11,699 [INFO] Trainer: Removing sft5b/checkpoint_step4000.optim.pt.*
2025-12-19 09:20:11,715 [INFO] Trainer: Removing sft5b/checkpoint_step4000.config.json.*
2025-12-19 09:20:59,590 [INFO] Trainer: Train [Step   7100/100000, Epoch 0.271/0] loss=1.5737 | lr_proj=4.960859e-04, lr_lora=9.921717e-05 | elapsed=01h:06m:41s
2025-12-19 09:21:47,756 [INFO] Trainer: Train [Step   7200/100000, Epoch 0.275/0] loss=1.6388 | lr_proj=4.959596e-04, lr_lora=9.919192e-05 | elapsed=01h:07m:29s
2025-12-19 09:22:35,182 [INFO] Trainer: Train [Step   7300/100000, Epoch 0.279/0] loss=1.6178 | lr_proj=4.958333e-04, lr_lora=9.916667e-05 | elapsed=01h:08m:17s
2025-12-19 09:23:24,302 [INFO] Trainer: Train [Step   7400/100000, Epoch 0.283/0] loss=1.7388 | lr_proj=4.957071e-04, lr_lora=9.914141e-05 | elapsed=01h:09m:06s
2025-12-19 09:24:13,964 [INFO] Trainer: Train [Step   7500/100000, Epoch 0.286/0] loss=1.5207 | lr_proj=4.955808e-04, lr_lora=9.911616e-05 | elapsed=01h:09m:56s
2025-12-19 09:25:02,532 [INFO] Trainer: Train [Step   7600/100000, Epoch 0.290/0] loss=1.5524 | lr_proj=4.954545e-04, lr_lora=9.909091e-05 | elapsed=01h:10m:44s
2025-12-19 09:25:48,864 [INFO] Trainer: Train [Step   7700/100000, Epoch 0.294/0] loss=1.7983 | lr_proj=4.953283e-04, lr_lora=9.906566e-05 | elapsed=01h:11m:31s
2025-12-19 09:26:34,639 [INFO] Trainer: Train [Step   7800/100000, Epoch 0.298/0] loss=1.4429 | lr_proj=4.952020e-04, lr_lora=9.904040e-05 | elapsed=01h:12m:16s
2025-12-19 09:27:22,263 [INFO] Trainer: Train [Step   7900/100000, Epoch 0.302/0] loss=1.9251 | lr_proj=4.950758e-04, lr_lora=9.901515e-05 | elapsed=01h:13m:04s
2025-12-19 09:28:11,219 [INFO] Trainer: Train [Step   8000/100000, Epoch 0.305/0] loss=1.7002 | lr_proj=4.949495e-04, lr_lora=9.898990e-05 | elapsed=01h:13m:53s
2025-12-19 09:29:39,668 [INFO] Trainer: Eval  [Step   8000/100000, Epoch 0.305/0] loss=1.8041 | lr_proj=4.949495e-04, lr_lora=9.898990e-05 | elapsed=01h:15m:21s
2025-12-19 09:29:39,734 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step8000.proj.pt
2025-12-19 09:29:39,955 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step8000.lora
2025-12-19 09:29:40,226 [INFO] Trainer: Saved config to sft5b/checkpoint_step8000.config.json
2025-12-19 09:29:40,228 [INFO] Trainer: Removing sft5b/checkpoint_step5000.optim.pt.*
2025-12-19 09:29:40,243 [INFO] Trainer: Removing sft5b/checkpoint_step5000.config.json.*
2025-12-19 09:29:40,246 [INFO] Trainer: Removing sft5b/checkpoint_step5000.lora.*
2025-12-19 09:29:40,256 [INFO] Trainer: Removing sft5b/checkpoint_step5000.proj.pt.*
2025-12-19 09:30:25,818 [INFO] Trainer: Train [Step   8100/100000, Epoch 0.309/0] loss=1.3988 | lr_proj=4.948232e-04, lr_lora=9.896465e-05 | elapsed=01h:16m:07s
2025-12-19 09:31:13,026 [INFO] Trainer: Train [Step   8200/100000, Epoch 0.313/0] loss=1.7840 | lr_proj=4.946970e-04, lr_lora=9.893939e-05 | elapsed=01h:16m:55s
2025-12-19 09:32:00,925 [INFO] Trainer: Train [Step   8300/100000, Epoch 0.317/0] loss=1.6019 | lr_proj=4.945707e-04, lr_lora=9.891414e-05 | elapsed=01h:17m:43s
2025-12-19 09:32:49,273 [INFO] Trainer: Train [Step   8400/100000, Epoch 0.321/0] loss=1.6622 | lr_proj=4.944444e-04, lr_lora=9.888889e-05 | elapsed=01h:18m:31s
2025-12-19 09:33:38,713 [INFO] Trainer: Train [Step   8500/100000, Epoch 0.325/0] loss=1.2606 | lr_proj=4.943182e-04, lr_lora=9.886364e-05 | elapsed=01h:19m:20s
2025-12-19 09:34:26,407 [INFO] Trainer: Train [Step   8600/100000, Epoch 0.328/0] loss=1.6471 | lr_proj=4.941919e-04, lr_lora=9.883838e-05 | elapsed=01h:20m:08s
2025-12-19 09:35:16,848 [INFO] Trainer: Train [Step   8700/100000, Epoch 0.332/0] loss=1.6646 | lr_proj=4.940657e-04, lr_lora=9.881313e-05 | elapsed=01h:20m:59s
2025-12-19 09:36:05,435 [INFO] Trainer: Train [Step   8800/100000, Epoch 0.336/0] loss=1.9160 | lr_proj=4.939394e-04, lr_lora=9.878788e-05 | elapsed=01h:21m:47s
2025-12-19 09:36:54,656 [INFO] Trainer: Train [Step   8900/100000, Epoch 0.340/0] loss=1.4545 | lr_proj=4.938131e-04, lr_lora=9.876263e-05 | elapsed=01h:22m:36s
2025-12-19 09:37:41,922 [INFO] Trainer: Train [Step   9000/100000, Epoch 0.344/0] loss=1.8412 | lr_proj=4.936869e-04, lr_lora=9.873737e-05 | elapsed=01h:23m:24s
2025-12-19 09:39:10,128 [INFO] Trainer: Eval  [Step   9000/100000, Epoch 0.344/0] loss=1.7341 | lr_proj=4.936869e-04, lr_lora=9.873737e-05 | elapsed=01h:24m:52s
2025-12-19 09:39:10,177 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step9000.proj.pt
2025-12-19 09:39:10,300 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step9000.lora
2025-12-19 09:39:10,500 [INFO] Trainer: Saved config to sft5b/checkpoint_step9000.config.json
2025-12-19 09:39:10,501 [INFO] Trainer: Removing sft5b/checkpoint_step6000.lora.*
2025-12-19 09:39:10,507 [INFO] Trainer: Removing sft5b/checkpoint_step6000.config.json.*
2025-12-19 09:39:10,509 [INFO] Trainer: Removing sft5b/checkpoint_step6000.proj.pt.*
2025-12-19 09:39:10,512 [INFO] Trainer: Removing sft5b/checkpoint_step6000.optim.pt.*
2025-12-19 09:39:59,628 [INFO] Trainer: Train [Step   9100/100000, Epoch 0.348/0] loss=1.2750 | lr_proj=4.935606e-04, lr_lora=9.871212e-05 | elapsed=01h:25m:41s
2025-12-19 09:40:48,328 [INFO] Trainer: Train [Step   9200/100000, Epoch 0.351/0] loss=1.8841 | lr_proj=4.934343e-04, lr_lora=9.868687e-05 | elapsed=01h:26m:30s
2025-12-19 09:41:36,829 [INFO] Trainer: Train [Step   9300/100000, Epoch 0.355/0] loss=1.5713 | lr_proj=4.933081e-04, lr_lora=9.866162e-05 | elapsed=01h:27m:19s
2025-12-19 09:42:25,492 [INFO] Trainer: Train [Step   9400/100000, Epoch 0.359/0] loss=1.4830 | lr_proj=4.931818e-04, lr_lora=9.863636e-05 | elapsed=01h:28m:07s
2025-12-19 09:43:15,757 [INFO] Trainer: Train [Step   9500/100000, Epoch 0.363/0] loss=1.5261 | lr_proj=4.930556e-04, lr_lora=9.861111e-05 | elapsed=01h:28m:57s
2025-12-19 09:44:01,448 [INFO] Trainer: Train [Step   9600/100000, Epoch 0.367/0] loss=1.6933 | lr_proj=4.929293e-04, lr_lora=9.858586e-05 | elapsed=01h:29m:43s
2025-12-19 09:44:49,437 [INFO] Trainer: Train [Step   9700/100000, Epoch 0.370/0] loss=1.7374 | lr_proj=4.928030e-04, lr_lora=9.856061e-05 | elapsed=01h:30m:31s
2025-12-19 09:45:36,010 [INFO] Trainer: Train [Step   9800/100000, Epoch 0.374/0] loss=1.8628 | lr_proj=4.926768e-04, lr_lora=9.853535e-05 | elapsed=01h:31m:18s
2025-12-19 09:46:22,873 [INFO] Trainer: Train [Step   9900/100000, Epoch 0.378/0] loss=1.4816 | lr_proj=4.925505e-04, lr_lora=9.851010e-05 | elapsed=01h:32m:05s
2025-12-19 09:47:11,056 [INFO] Trainer: Train [Step  10000/100000, Epoch 0.382/0] loss=1.6081 | lr_proj=4.924242e-04, lr_lora=9.848485e-05 | elapsed=01h:32m:53s
2025-12-19 09:48:39,206 [INFO] Trainer: Eval  [Step  10000/100000, Epoch 0.382/0] loss=1.7269 | lr_proj=4.924242e-04, lr_lora=9.848485e-05 | elapsed=01h:34m:21s
2025-12-19 09:48:39,244 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step10000.proj.pt
2025-12-19 09:48:39,384 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step10000.lora
2025-12-19 09:48:39,538 [INFO] Trainer: Saved config to sft5b/checkpoint_step10000.config.json
2025-12-19 09:48:39,541 [INFO] Trainer: Removing sft5b/checkpoint_step7000.optim.pt.*
2025-12-19 09:48:39,546 [INFO] Trainer: Removing sft5b/checkpoint_step7000.config.json.*
2025-12-19 09:48:39,547 [INFO] Trainer: Removing sft5b/checkpoint_step7000.proj.pt.*
2025-12-19 09:48:39,563 [INFO] Trainer: Removing sft5b/checkpoint_step7000.lora.*
2025-12-19 09:49:26,160 [INFO] Trainer: Train [Step  10100/100000, Epoch 0.386/0] loss=1.7975 | lr_proj=4.922980e-04, lr_lora=9.845960e-05 | elapsed=01h:35m:08s
2025-12-19 09:50:15,453 [INFO] Trainer: Train [Step  10200/100000, Epoch 0.390/0] loss=1.3557 | lr_proj=4.921717e-04, lr_lora=9.843434e-05 | elapsed=01h:35m:57s
2025-12-19 09:51:01,347 [INFO] Trainer: Train [Step  10300/100000, Epoch 0.393/0] loss=1.3102 | lr_proj=4.920455e-04, lr_lora=9.840909e-05 | elapsed=01h:36m:43s
2025-12-19 09:51:48,036 [INFO] Trainer: Train [Step  10400/100000, Epoch 0.397/0] loss=1.3377 | lr_proj=4.919192e-04, lr_lora=9.838384e-05 | elapsed=01h:37m:30s
2025-12-19 09:52:34,641 [INFO] Trainer: Train [Step  10500/100000, Epoch 0.401/0] loss=1.5414 | lr_proj=4.917929e-04, lr_lora=9.835859e-05 | elapsed=01h:38m:16s
2025-12-19 09:53:21,506 [INFO] Trainer: Train [Step  10600/100000, Epoch 0.405/0] loss=1.5865 | lr_proj=4.916667e-04, lr_lora=9.833333e-05 | elapsed=01h:39m:03s
2025-12-19 09:54:09,164 [INFO] Trainer: Train [Step  10700/100000, Epoch 0.409/0] loss=1.8170 | lr_proj=4.915404e-04, lr_lora=9.830808e-05 | elapsed=01h:39m:51s
2025-12-19 09:54:56,243 [INFO] Trainer: Train [Step  10800/100000, Epoch 0.412/0] loss=1.6426 | lr_proj=4.914141e-04, lr_lora=9.828283e-05 | elapsed=01h:40m:38s
2025-12-19 09:55:45,146 [INFO] Trainer: Train [Step  10900/100000, Epoch 0.416/0] loss=1.7654 | lr_proj=4.912879e-04, lr_lora=9.825758e-05 | elapsed=01h:41m:27s
2025-12-19 09:56:34,006 [INFO] Trainer: Train [Step  11000/100000, Epoch 0.420/0] loss=1.5076 | lr_proj=4.911616e-04, lr_lora=9.823232e-05 | elapsed=01h:42m:16s
2025-12-19 09:58:02,302 [INFO] Trainer: Eval  [Step  11000/100000, Epoch 0.420/0] loss=1.7145 | lr_proj=4.911616e-04, lr_lora=9.823232e-05 | elapsed=01h:43m:44s
2025-12-19 09:58:02,367 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step11000.proj.pt
2025-12-19 09:58:02,541 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step11000.lora
2025-12-19 09:58:02,719 [INFO] Trainer: Saved config to sft5b/checkpoint_step11000.config.json
2025-12-19 09:58:02,722 [INFO] Trainer: Removing sft5b/checkpoint_step8000.lora.*
2025-12-19 09:58:02,727 [INFO] Trainer: Removing sft5b/checkpoint_step8000.config.json.*
2025-12-19 09:58:02,749 [INFO] Trainer: Removing sft5b/checkpoint_step8000.proj.pt.*
2025-12-19 09:58:02,759 [INFO] Trainer: Removing sft5b/checkpoint_step8000.optim.pt.*
2025-12-19 09:58:52,824 [INFO] Trainer: Train [Step  11100/100000, Epoch 0.424/0] loss=1.5668 | lr_proj=4.910354e-04, lr_lora=9.820707e-05 | elapsed=01h:44m:35s
2025-12-19 09:59:39,786 [INFO] Trainer: Train [Step  11200/100000, Epoch 0.428/0] loss=1.5607 | lr_proj=4.909091e-04, lr_lora=9.818182e-05 | elapsed=01h:45m:21s
2025-12-19 10:00:28,129 [INFO] Trainer: Train [Step  11300/100000, Epoch 0.432/0] loss=1.9054 | lr_proj=4.907828e-04, lr_lora=9.815657e-05 | elapsed=01h:46m:10s
2025-12-19 10:01:14,763 [INFO] Trainer: Train [Step  11400/100000, Epoch 0.435/0] loss=1.9704 | lr_proj=4.906566e-04, lr_lora=9.813131e-05 | elapsed=01h:46m:56s
2025-12-19 10:02:02,533 [INFO] Trainer: Train [Step  11500/100000, Epoch 0.439/0] loss=1.8187 | lr_proj=4.905303e-04, lr_lora=9.810606e-05 | elapsed=01h:47m:44s
2025-12-19 10:02:48,158 [INFO] Trainer: Train [Step  11600/100000, Epoch 0.443/0] loss=1.7471 | lr_proj=4.904040e-04, lr_lora=9.808081e-05 | elapsed=01h:48m:30s
2025-12-19 10:03:35,298 [INFO] Trainer: Train [Step  11700/100000, Epoch 0.447/0] loss=1.7688 | lr_proj=4.902778e-04, lr_lora=9.805556e-05 | elapsed=01h:49m:17s
2025-12-19 10:04:21,882 [INFO] Trainer: Train [Step  11800/100000, Epoch 0.451/0] loss=1.5545 | lr_proj=4.901515e-04, lr_lora=9.803030e-05 | elapsed=01h:50m:04s
2025-12-19 10:05:10,799 [INFO] Trainer: Train [Step  11900/100000, Epoch 0.454/0] loss=1.8918 | lr_proj=4.900253e-04, lr_lora=9.800505e-05 | elapsed=01h:50m:52s
2025-12-19 10:05:58,131 [INFO] Trainer: Train [Step  12000/100000, Epoch 0.458/0] loss=1.1845 | lr_proj=4.898990e-04, lr_lora=9.797980e-05 | elapsed=01h:51m:40s
2025-12-19 10:07:26,001 [INFO] Trainer: Eval  [Step  12000/100000, Epoch 0.458/0] loss=1.7097 | lr_proj=4.898990e-04, lr_lora=9.797980e-05 | elapsed=01h:53m:08s
2025-12-19 10:07:26,060 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step12000.proj.pt
2025-12-19 10:07:26,260 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step12000.lora
2025-12-19 10:07:26,471 [INFO] Trainer: Saved config to sft5b/checkpoint_step12000.config.json
2025-12-19 10:07:26,473 [INFO] Trainer: Removing sft5b/checkpoint_step9000.optim.pt.*
2025-12-19 10:07:26,488 [INFO] Trainer: Removing sft5b/checkpoint_step9000.lora.*
2025-12-19 10:07:26,496 [INFO] Trainer: Removing sft5b/checkpoint_step9000.proj.pt.*
2025-12-19 10:07:26,498 [INFO] Trainer: Removing sft5b/checkpoint_step9000.config.json.*
2025-12-19 10:08:15,150 [INFO] Trainer: Train [Step  12100/100000, Epoch 0.462/0] loss=1.4753 | lr_proj=4.897727e-04, lr_lora=9.795455e-05 | elapsed=01h:53m:57s
2025-12-19 10:09:01,673 [INFO] Trainer: Train [Step  12200/100000, Epoch 0.466/0] loss=1.2002 | lr_proj=4.896465e-04, lr_lora=9.792929e-05 | elapsed=01h:54m:43s
2025-12-19 10:09:48,576 [INFO] Trainer: Train [Step  12300/100000, Epoch 0.470/0] loss=1.5029 | lr_proj=4.895202e-04, lr_lora=9.790404e-05 | elapsed=01h:55m:30s
2025-12-19 10:10:35,610 [INFO] Trainer: Train [Step  12400/100000, Epoch 0.474/0] loss=1.4000 | lr_proj=4.893939e-04, lr_lora=9.787879e-05 | elapsed=01h:56m:17s
2025-12-19 10:11:24,135 [INFO] Trainer: Train [Step  12500/100000, Epoch 0.477/0] loss=1.7556 | lr_proj=4.892677e-04, lr_lora=9.785354e-05 | elapsed=01h:57m:06s
2025-12-19 10:12:11,835 [INFO] Trainer: Train [Step  12600/100000, Epoch 0.481/0] loss=1.4537 | lr_proj=4.891414e-04, lr_lora=9.782828e-05 | elapsed=01h:57m:54s
2025-12-19 10:13:01,347 [INFO] Trainer: Train [Step  12700/100000, Epoch 0.485/0] loss=1.4987 | lr_proj=4.890152e-04, lr_lora=9.780303e-05 | elapsed=01h:58m:43s
2025-12-19 10:13:47,316 [INFO] Trainer: Train [Step  12800/100000, Epoch 0.489/0] loss=1.2941 | lr_proj=4.888889e-04, lr_lora=9.777778e-05 | elapsed=01h:59m:29s
2025-12-19 10:14:33,740 [INFO] Trainer: Train [Step  12900/100000, Epoch 0.493/0] loss=1.6780 | lr_proj=4.887626e-04, lr_lora=9.775253e-05 | elapsed=02h:00m:15s
2025-12-19 10:15:21,283 [INFO] Trainer: Train [Step  13000/100000, Epoch 0.496/0] loss=1.5193 | lr_proj=4.886364e-04, lr_lora=9.772727e-05 | elapsed=02h:01m:03s
2025-12-19 10:16:49,217 [INFO] Trainer: Eval  [Step  13000/100000, Epoch 0.496/0] loss=1.6997 | lr_proj=4.886364e-04, lr_lora=9.772727e-05 | elapsed=02h:02m:31s
2025-12-19 10:16:49,257 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step13000.proj.pt
2025-12-19 10:16:49,417 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step13000.lora
2025-12-19 10:16:49,608 [INFO] Trainer: Saved config to sft5b/checkpoint_step13000.config.json
2025-12-19 10:16:49,611 [INFO] Trainer: Removing sft5b/checkpoint_step10000.optim.pt.*
2025-12-19 10:16:49,627 [INFO] Trainer: Removing sft5b/checkpoint_step10000.proj.pt.*
2025-12-19 10:16:49,630 [INFO] Trainer: Removing sft5b/checkpoint_step10000.lora.*
2025-12-19 10:16:49,641 [INFO] Trainer: Removing sft5b/checkpoint_step10000.config.json.*
2025-12-19 10:17:39,549 [INFO] Trainer: Train [Step  13100/100000, Epoch 0.500/0] loss=1.8802 | lr_proj=4.885101e-04, lr_lora=9.770202e-05 | elapsed=02h:03m:21s
2025-12-19 10:18:26,508 [INFO] Trainer: Train [Step  13200/100000, Epoch 0.504/0] loss=1.3048 | lr_proj=4.883838e-04, lr_lora=9.767677e-05 | elapsed=02h:04m:08s
2025-12-19 10:19:13,026 [INFO] Trainer: Train [Step  13300/100000, Epoch 0.508/0] loss=1.7487 | lr_proj=4.882576e-04, lr_lora=9.765152e-05 | elapsed=02h:04m:55s
2025-12-19 10:20:00,285 [INFO] Trainer: Train [Step  13400/100000, Epoch 0.512/0] loss=1.4100 | lr_proj=4.881313e-04, lr_lora=9.762626e-05 | elapsed=02h:05m:42s
2025-12-19 10:20:47,179 [INFO] Trainer: Train [Step  13500/100000, Epoch 0.516/0] loss=1.7966 | lr_proj=4.880051e-04, lr_lora=9.760101e-05 | elapsed=02h:06m:29s
2025-12-19 10:21:33,885 [INFO] Trainer: Train [Step  13600/100000, Epoch 0.519/0] loss=1.5809 | lr_proj=4.878788e-04, lr_lora=9.757576e-05 | elapsed=02h:07m:16s
2025-12-19 10:22:19,029 [INFO] Trainer: Train [Step  13700/100000, Epoch 0.523/0] loss=1.7471 | lr_proj=4.877525e-04, lr_lora=9.755051e-05 | elapsed=02h:08m:01s
2025-12-19 10:23:04,814 [INFO] Trainer: Train [Step  13800/100000, Epoch 0.527/0] loss=1.9138 | lr_proj=4.876263e-04, lr_lora=9.752525e-05 | elapsed=02h:08m:46s
2025-12-19 10:23:54,658 [INFO] Trainer: Train [Step  13900/100000, Epoch 0.531/0] loss=1.9383 | lr_proj=4.875000e-04, lr_lora=9.750000e-05 | elapsed=02h:09m:36s
2025-12-19 10:24:43,313 [INFO] Trainer: Train [Step  14000/100000, Epoch 0.535/0] loss=1.6856 | lr_proj=4.873737e-04, lr_lora=9.747475e-05 | elapsed=02h:10m:25s
2025-12-19 10:26:11,847 [INFO] Trainer: Eval  [Step  14000/100000, Epoch 0.535/0] loss=1.6982 | lr_proj=4.873737e-04, lr_lora=9.747475e-05 | elapsed=02h:11m:54s
2025-12-19 10:26:11,925 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step14000.proj.pt
2025-12-19 10:26:12,064 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step14000.lora
2025-12-19 10:26:12,266 [INFO] Trainer: Saved config to sft5b/checkpoint_step14000.config.json
2025-12-19 10:26:12,268 [INFO] Trainer: Removing sft5b/checkpoint_step11000.optim.pt.*
2025-12-19 10:26:12,282 [INFO] Trainer: Removing sft5b/checkpoint_step11000.proj.pt.*
2025-12-19 10:26:12,285 [INFO] Trainer: Removing sft5b/checkpoint_step11000.config.json.*
2025-12-19 10:26:12,286 [INFO] Trainer: Removing sft5b/checkpoint_step11000.lora.*
2025-12-19 10:27:00,320 [INFO] Trainer: Train [Step  14100/100000, Epoch 0.538/0] loss=1.7041 | lr_proj=4.872475e-04, lr_lora=9.744949e-05 | elapsed=02h:12m:42s
2025-12-19 10:27:48,962 [INFO] Trainer: Train [Step  14200/100000, Epoch 0.542/0] loss=0.9951 | lr_proj=4.871212e-04, lr_lora=9.742424e-05 | elapsed=02h:13m:31s
2025-12-19 10:28:36,970 [INFO] Trainer: Train [Step  14300/100000, Epoch 0.546/0] loss=1.8584 | lr_proj=4.869949e-04, lr_lora=9.739899e-05 | elapsed=02h:14m:19s
2025-12-19 10:29:26,339 [INFO] Trainer: Train [Step  14400/100000, Epoch 0.550/0] loss=1.6978 | lr_proj=4.868687e-04, lr_lora=9.737374e-05 | elapsed=02h:15m:08s
2025-12-19 10:30:13,110 [INFO] Trainer: Train [Step  14500/100000, Epoch 0.554/0] loss=1.7043 | lr_proj=4.867424e-04, lr_lora=9.734848e-05 | elapsed=02h:15m:55s
2025-12-19 10:31:01,882 [INFO] Trainer: Train [Step  14600/100000, Epoch 0.558/0] loss=1.5815 | lr_proj=4.866162e-04, lr_lora=9.732323e-05 | elapsed=02h:16m:44s
2025-12-19 10:31:48,825 [INFO] Trainer: Train [Step  14700/100000, Epoch 0.561/0] loss=1.6159 | lr_proj=4.864899e-04, lr_lora=9.729798e-05 | elapsed=02h:17m:31s
2025-12-19 10:32:37,279 [INFO] Trainer: Train [Step  14800/100000, Epoch 0.565/0] loss=1.3975 | lr_proj=4.863636e-04, lr_lora=9.727273e-05 | elapsed=02h:18m:19s
2025-12-19 10:33:22,616 [INFO] Trainer: Train [Step  14900/100000, Epoch 0.569/0] loss=2.0172 | lr_proj=4.862374e-04, lr_lora=9.724747e-05 | elapsed=02h:19m:04s
2025-12-19 10:34:10,212 [INFO] Trainer: Train [Step  15000/100000, Epoch 0.573/0] loss=1.7003 | lr_proj=4.861111e-04, lr_lora=9.722222e-05 | elapsed=02h:19m:52s
2025-12-19 10:35:38,339 [INFO] Trainer: Eval  [Step  15000/100000, Epoch 0.573/0] loss=1.7237 | lr_proj=4.861111e-04, lr_lora=9.722222e-05 | elapsed=02h:21m:20s
2025-12-19 10:35:38,381 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step15000.proj.pt
2025-12-19 10:35:38,507 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step15000.lora
2025-12-19 10:35:38,697 [INFO] Trainer: Saved config to sft5b/checkpoint_step15000.config.json
2025-12-19 10:35:38,699 [INFO] Trainer: Removing sft5b/checkpoint_step12000.config.json.*
2025-12-19 10:35:38,701 [INFO] Trainer: Removing sft5b/checkpoint_step12000.lora.*
2025-12-19 10:35:38,708 [INFO] Trainer: Removing sft5b/checkpoint_step12000.proj.pt.*
2025-12-19 10:35:38,710 [INFO] Trainer: Removing sft5b/checkpoint_step12000.optim.pt.*
2025-12-19 10:36:26,886 [INFO] Trainer: Train [Step  15100/100000, Epoch 0.577/0] loss=1.5158 | lr_proj=4.859848e-04, lr_lora=9.719697e-05 | elapsed=02h:22m:09s
2025-12-19 10:37:12,221 [INFO] Trainer: Train [Step  15200/100000, Epoch 0.580/0] loss=1.8195 | lr_proj=4.858586e-04, lr_lora=9.717172e-05 | elapsed=02h:22m:54s
2025-12-19 10:38:02,499 [INFO] Trainer: Train [Step  15300/100000, Epoch 0.584/0] loss=1.4514 | lr_proj=4.857323e-04, lr_lora=9.714646e-05 | elapsed=02h:23m:44s
2025-12-19 10:38:51,033 [INFO] Trainer: Train [Step  15400/100000, Epoch 0.588/0] loss=2.0107 | lr_proj=4.856061e-04, lr_lora=9.712121e-05 | elapsed=02h:24m:33s
2025-12-19 10:39:38,836 [INFO] Trainer: Train [Step  15500/100000, Epoch 0.592/0] loss=1.3139 | lr_proj=4.854798e-04, lr_lora=9.709596e-05 | elapsed=02h:25m:21s
2025-12-19 10:40:26,346 [INFO] Trainer: Train [Step  15600/100000, Epoch 0.596/0] loss=1.5542 | lr_proj=4.853535e-04, lr_lora=9.707071e-05 | elapsed=02h:26m:08s
2025-12-19 10:41:14,712 [INFO] Trainer: Train [Step  15700/100000, Epoch 0.600/0] loss=1.8936 | lr_proj=4.852273e-04, lr_lora=9.704545e-05 | elapsed=02h:26m:56s
2025-12-19 10:42:02,487 [INFO] Trainer: Train [Step  15800/100000, Epoch 0.603/0] loss=2.0594 | lr_proj=4.851010e-04, lr_lora=9.702020e-05 | elapsed=02h:27m:44s
2025-12-19 10:42:52,555 [INFO] Trainer: Train [Step  15900/100000, Epoch 0.607/0] loss=1.7178 | lr_proj=4.849747e-04, lr_lora=9.699495e-05 | elapsed=02h:28m:34s
2025-12-19 10:43:38,850 [INFO] Trainer: Train [Step  16000/100000, Epoch 0.611/0] loss=1.3185 | lr_proj=4.848485e-04, lr_lora=9.696970e-05 | elapsed=02h:29m:21s
2025-12-19 10:45:08,122 [INFO] Trainer: Eval  [Step  16000/100000, Epoch 0.611/0] loss=1.6774 | lr_proj=4.848485e-04, lr_lora=9.696970e-05 | elapsed=02h:30m:50s
2025-12-19 10:45:08,169 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step16000.proj.pt
2025-12-19 10:45:08,277 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step16000.lora
2025-12-19 10:45:08,431 [INFO] Trainer: Saved config to sft5b/checkpoint_step16000.config.json
2025-12-19 10:45:08,434 [INFO] Trainer: Removing sft5b/checkpoint_step13000.lora.*
2025-12-19 10:45:08,441 [INFO] Trainer: Removing sft5b/checkpoint_step13000.config.json.*
2025-12-19 10:45:08,442 [INFO] Trainer: Removing sft5b/checkpoint_step13000.optim.pt.*
2025-12-19 10:45:08,448 [INFO] Trainer: Removing sft5b/checkpoint_step13000.proj.pt.*
2025-12-19 10:45:55,622 [INFO] Trainer: Train [Step  16100/100000, Epoch 0.615/0] loss=1.6142 | lr_proj=4.847222e-04, lr_lora=9.694444e-05 | elapsed=02h:31m:37s
2025-12-19 10:46:42,395 [INFO] Trainer: Train [Step  16200/100000, Epoch 0.619/0] loss=1.7484 | lr_proj=4.845960e-04, lr_lora=9.691919e-05 | elapsed=02h:32m:24s
2025-12-19 10:47:31,509 [INFO] Trainer: Train [Step  16300/100000, Epoch 0.622/0] loss=1.3943 | lr_proj=4.844697e-04, lr_lora=9.689394e-05 | elapsed=02h:33m:13s
2025-12-19 10:48:19,141 [INFO] Trainer: Train [Step  16400/100000, Epoch 0.626/0] loss=1.7979 | lr_proj=4.843434e-04, lr_lora=9.686869e-05 | elapsed=02h:34m:01s
2025-12-19 10:49:07,946 [INFO] Trainer: Train [Step  16500/100000, Epoch 0.630/0] loss=1.3344 | lr_proj=4.842172e-04, lr_lora=9.684343e-05 | elapsed=02h:34m:50s
2025-12-19 10:49:57,314 [INFO] Trainer: Train [Step  16600/100000, Epoch 0.634/0] loss=1.5882 | lr_proj=4.840909e-04, lr_lora=9.681818e-05 | elapsed=02h:35m:39s
2025-12-19 10:50:44,580 [INFO] Trainer: Train [Step  16700/100000, Epoch 0.638/0] loss=1.1655 | lr_proj=4.839646e-04, lr_lora=9.679293e-05 | elapsed=02h:36m:26s
2025-12-19 10:51:32,956 [INFO] Trainer: Train [Step  16800/100000, Epoch 0.642/0] loss=1.7049 | lr_proj=4.838384e-04, lr_lora=9.676768e-05 | elapsed=02h:37m:15s
2025-12-19 10:52:20,713 [INFO] Trainer: Train [Step  16900/100000, Epoch 0.645/0] loss=1.4262 | lr_proj=4.837121e-04, lr_lora=9.674242e-05 | elapsed=02h:38m:02s
2025-12-19 10:53:09,192 [INFO] Trainer: Train [Step  17000/100000, Epoch 0.649/0] loss=1.5992 | lr_proj=4.835859e-04, lr_lora=9.671717e-05 | elapsed=02h:38m:51s
2025-12-19 10:54:38,251 [INFO] Trainer: Eval  [Step  17000/100000, Epoch 0.649/0] loss=1.7177 | lr_proj=4.835859e-04, lr_lora=9.671717e-05 | elapsed=02h:40m:20s
2025-12-19 10:54:38,310 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step17000.proj.pt
2025-12-19 10:54:38,455 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step17000.lora
2025-12-19 10:54:38,626 [INFO] Trainer: Saved config to sft5b/checkpoint_step17000.config.json
2025-12-19 10:54:38,628 [INFO] Trainer: Removing sft5b/checkpoint_step14000.config.json.*
2025-12-19 10:54:38,666 [INFO] Trainer: Removing sft5b/checkpoint_step14000.optim.pt.*
2025-12-19 10:54:38,680 [INFO] Trainer: Removing sft5b/checkpoint_step14000.proj.pt.*
2025-12-19 10:54:38,697 [INFO] Trainer: Removing sft5b/checkpoint_step14000.lora.*
2025-12-19 10:55:28,303 [INFO] Trainer: Train [Step  17100/100000, Epoch 0.653/0] loss=1.3190 | lr_proj=4.834596e-04, lr_lora=9.669192e-05 | elapsed=02h:41m:10s
2025-12-19 10:56:17,024 [INFO] Trainer: Train [Step  17200/100000, Epoch 0.657/0] loss=1.6090 | lr_proj=4.833333e-04, lr_lora=9.666667e-05 | elapsed=02h:41m:59s
2025-12-19 10:57:05,470 [INFO] Trainer: Train [Step  17300/100000, Epoch 0.661/0] loss=1.5389 | lr_proj=4.832071e-04, lr_lora=9.664141e-05 | elapsed=02h:42m:47s
2025-12-19 10:57:55,107 [INFO] Trainer: Train [Step  17400/100000, Epoch 0.664/0] loss=1.6244 | lr_proj=4.830808e-04, lr_lora=9.661616e-05 | elapsed=02h:43m:37s
2025-12-19 10:58:44,802 [INFO] Trainer: Train [Step  17500/100000, Epoch 0.668/0] loss=1.5863 | lr_proj=4.829545e-04, lr_lora=9.659091e-05 | elapsed=02h:44m:26s
2025-12-19 10:59:33,047 [INFO] Trainer: Train [Step  17600/100000, Epoch 0.672/0] loss=1.2916 | lr_proj=4.828283e-04, lr_lora=9.656566e-05 | elapsed=02h:45m:15s
2025-12-19 11:00:22,009 [INFO] Trainer: Train [Step  17700/100000, Epoch 0.676/0] loss=1.3445 | lr_proj=4.827020e-04, lr_lora=9.654040e-05 | elapsed=02h:46m:04s
2025-12-19 11:01:11,125 [INFO] Trainer: Train [Step  17800/100000, Epoch 0.680/0] loss=1.3725 | lr_proj=4.825758e-04, lr_lora=9.651515e-05 | elapsed=02h:46m:53s
2025-12-19 11:02:00,130 [INFO] Trainer: Train [Step  17900/100000, Epoch 0.684/0] loss=1.5549 | lr_proj=4.824495e-04, lr_lora=9.648990e-05 | elapsed=02h:47m:42s
2025-12-19 11:02:49,017 [INFO] Trainer: Train [Step  18000/100000, Epoch 0.687/0] loss=1.4478 | lr_proj=4.823232e-04, lr_lora=9.646465e-05 | elapsed=02h:48m:31s
2025-12-19 11:04:17,792 [INFO] Trainer: Eval  [Step  18000/100000, Epoch 0.687/0] loss=1.6925 | lr_proj=4.823232e-04, lr_lora=9.646465e-05 | elapsed=02h:49m:59s
2025-12-19 11:04:17,859 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step18000.proj.pt
2025-12-19 11:04:18,010 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step18000.lora
2025-12-19 11:04:18,195 [INFO] Trainer: Saved config to sft5b/checkpoint_step18000.config.json
2025-12-19 11:04:18,197 [INFO] Trainer: Removing sft5b/checkpoint_step15000.lora.*
2025-12-19 11:04:18,203 [INFO] Trainer: Removing sft5b/checkpoint_step15000.config.json.*
2025-12-19 11:04:18,218 [INFO] Trainer: Removing sft5b/checkpoint_step15000.proj.pt.*
2025-12-19 11:04:18,231 [INFO] Trainer: Removing sft5b/checkpoint_step15000.optim.pt.*
2025-12-19 11:05:05,472 [INFO] Trainer: Train [Step  18100/100000, Epoch 0.691/0] loss=1.3007 | lr_proj=4.821970e-04, lr_lora=9.643939e-05 | elapsed=02h:50m:47s
2025-12-19 11:05:53,391 [INFO] Trainer: Train [Step  18200/100000, Epoch 0.695/0] loss=1.6090 | lr_proj=4.820707e-04, lr_lora=9.641414e-05 | elapsed=02h:51m:35s
2025-12-19 11:06:39,992 [INFO] Trainer: Train [Step  18300/100000, Epoch 0.699/0] loss=2.1339 | lr_proj=4.819444e-04, lr_lora=9.638889e-05 | elapsed=02h:52m:22s
2025-12-19 11:07:28,752 [INFO] Trainer: Train [Step  18400/100000, Epoch 0.703/0] loss=1.7361 | lr_proj=4.818182e-04, lr_lora=9.636364e-05 | elapsed=02h:53m:10s
2025-12-19 11:08:15,965 [INFO] Trainer: Train [Step  18500/100000, Epoch 0.706/0] loss=1.7845 | lr_proj=4.816919e-04, lr_lora=9.633838e-05 | elapsed=02h:53m:58s
2025-12-19 11:09:04,627 [INFO] Trainer: Train [Step  18600/100000, Epoch 0.710/0] loss=1.4055 | lr_proj=4.815657e-04, lr_lora=9.631313e-05 | elapsed=02h:54m:46s
2025-12-19 11:09:55,441 [INFO] Trainer: Train [Step  18700/100000, Epoch 0.714/0] loss=1.8268 | lr_proj=4.814394e-04, lr_lora=9.628788e-05 | elapsed=02h:55m:37s
2025-12-19 11:10:41,244 [INFO] Trainer: Train [Step  18800/100000, Epoch 0.718/0] loss=1.6366 | lr_proj=4.813131e-04, lr_lora=9.626263e-05 | elapsed=02h:56m:23s
2025-12-19 11:11:30,788 [INFO] Trainer: Train [Step  18900/100000, Epoch 0.722/0] loss=1.2624 | lr_proj=4.811869e-04, lr_lora=9.623737e-05 | elapsed=02h:57m:12s
2025-12-19 11:12:17,400 [INFO] Trainer: Train [Step  19000/100000, Epoch 0.726/0] loss=1.9006 | lr_proj=4.810606e-04, lr_lora=9.621212e-05 | elapsed=02h:57m:59s
2025-12-19 11:13:46,091 [INFO] Trainer: Eval  [Step  19000/100000, Epoch 0.726/0] loss=1.6774 | lr_proj=4.810606e-04, lr_lora=9.621212e-05 | elapsed=02h:59m:28s
2025-12-19 11:13:46,161 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step19000.proj.pt
2025-12-19 11:13:46,330 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step19000.lora
2025-12-19 11:13:46,569 [INFO] Trainer: Saved config to sft5b/checkpoint_step19000.config.json
2025-12-19 11:13:46,572 [INFO] Trainer: Removing sft5b/checkpoint_step16000.lora.*
2025-12-19 11:13:46,583 [INFO] Trainer: Removing sft5b/checkpoint_step16000.proj.pt.*
2025-12-19 11:13:46,619 [INFO] Trainer: Removing sft5b/checkpoint_step16000.optim.pt.*
2025-12-19 11:13:46,646 [INFO] Trainer: Removing sft5b/checkpoint_step16000.config.json.*
2025-12-19 11:14:34,770 [INFO] Trainer: Train [Step  19100/100000, Epoch 0.729/0] loss=1.5829 | lr_proj=4.809343e-04, lr_lora=9.618687e-05 | elapsed=03h:00m:16s
2025-12-19 11:15:25,157 [INFO] Trainer: Train [Step  19200/100000, Epoch 0.733/0] loss=1.5886 | lr_proj=4.808081e-04, lr_lora=9.616162e-05 | elapsed=03h:01m:07s
2025-12-19 11:16:10,077 [INFO] Trainer: Train [Step  19300/100000, Epoch 0.737/0] loss=1.8624 | lr_proj=4.806818e-04, lr_lora=9.613636e-05 | elapsed=03h:01m:52s
2025-12-19 11:16:59,146 [INFO] Trainer: Train [Step  19400/100000, Epoch 0.741/0] loss=1.5036 | lr_proj=4.805556e-04, lr_lora=9.611111e-05 | elapsed=03h:02m:41s
2025-12-19 11:17:47,205 [INFO] Trainer: Train [Step  19500/100000, Epoch 0.745/0] loss=1.7440 | lr_proj=4.804293e-04, lr_lora=9.608586e-05 | elapsed=03h:03m:29s
2025-12-19 11:18:33,610 [INFO] Trainer: Train [Step  19600/100000, Epoch 0.748/0] loss=1.7075 | lr_proj=4.803030e-04, lr_lora=9.606061e-05 | elapsed=03h:04m:15s
2025-12-19 11:19:21,881 [INFO] Trainer: Train [Step  19700/100000, Epoch 0.752/0] loss=1.7141 | lr_proj=4.801768e-04, lr_lora=9.603535e-05 | elapsed=03h:05m:04s
2025-12-19 11:20:08,973 [INFO] Trainer: Train [Step  19800/100000, Epoch 0.756/0] loss=1.5382 | lr_proj=4.800505e-04, lr_lora=9.601010e-05 | elapsed=03h:05m:51s
2025-12-19 11:20:56,947 [INFO] Trainer: Train [Step  19900/100000, Epoch 0.760/0] loss=1.6977 | lr_proj=4.799242e-04, lr_lora=9.598485e-05 | elapsed=03h:06m:39s
2025-12-19 11:21:44,866 [INFO] Trainer: Train [Step  20000/100000, Epoch 0.764/0] loss=1.2024 | lr_proj=4.797980e-04, lr_lora=9.595960e-05 | elapsed=03h:07m:27s
2025-12-19 11:23:14,103 [INFO] Trainer: Eval  [Step  20000/100000, Epoch 0.764/0] loss=1.6764 | lr_proj=4.797980e-04, lr_lora=9.595960e-05 | elapsed=03h:08m:56s
2025-12-19 11:23:14,131 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step20000.proj.pt
2025-12-19 11:23:14,272 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step20000.lora
2025-12-19 11:23:14,510 [INFO] Trainer: Saved config to sft5b/checkpoint_step20000.config.json
2025-12-19 11:23:14,512 [INFO] Trainer: Removing sft5b/checkpoint_step17000.proj.pt.*
2025-12-19 11:23:14,516 [INFO] Trainer: Removing sft5b/checkpoint_step17000.lora.*
2025-12-19 11:23:14,527 [INFO] Trainer: Removing sft5b/checkpoint_step17000.optim.pt.*
2025-12-19 11:23:14,541 [INFO] Trainer: Removing sft5b/checkpoint_step17000.config.json.*
2025-12-19 11:24:03,736 [INFO] Trainer: Train [Step  20100/100000, Epoch 0.768/0] loss=1.7598 | lr_proj=4.796717e-04, lr_lora=9.593434e-05 | elapsed=03h:09m:45s
2025-12-19 11:24:50,468 [INFO] Trainer: Train [Step  20200/100000, Epoch 0.771/0] loss=1.5610 | lr_proj=4.795455e-04, lr_lora=9.590909e-05 | elapsed=03h:10m:32s
2025-12-19 11:25:39,754 [INFO] Trainer: Train [Step  20300/100000, Epoch 0.775/0] loss=1.6950 | lr_proj=4.794192e-04, lr_lora=9.588384e-05 | elapsed=03h:11m:21s
2025-12-19 11:26:28,826 [INFO] Trainer: Train [Step  20400/100000, Epoch 0.779/0] loss=1.4554 | lr_proj=4.792929e-04, lr_lora=9.585859e-05 | elapsed=03h:12m:11s
2025-12-19 11:27:17,157 [INFO] Trainer: Train [Step  20500/100000, Epoch 0.783/0] loss=1.2505 | lr_proj=4.791667e-04, lr_lora=9.583333e-05 | elapsed=03h:12m:59s
2025-12-19 11:28:04,997 [INFO] Trainer: Train [Step  20600/100000, Epoch 0.787/0] loss=1.4801 | lr_proj=4.790404e-04, lr_lora=9.580808e-05 | elapsed=03h:13m:47s
2025-12-19 11:28:53,955 [INFO] Trainer: Train [Step  20700/100000, Epoch 0.790/0] loss=1.6258 | lr_proj=4.789141e-04, lr_lora=9.578283e-05 | elapsed=03h:14m:36s
2025-12-19 11:29:43,710 [INFO] Trainer: Train [Step  20800/100000, Epoch 0.794/0] loss=1.7002 | lr_proj=4.787879e-04, lr_lora=9.575758e-05 | elapsed=03h:15m:25s
2025-12-19 11:30:31,567 [INFO] Trainer: Train [Step  20900/100000, Epoch 0.798/0] loss=1.1786 | lr_proj=4.786616e-04, lr_lora=9.573232e-05 | elapsed=03h:16m:13s
2025-12-19 11:31:19,729 [INFO] Trainer: Train [Step  21000/100000, Epoch 0.802/0] loss=1.6041 | lr_proj=4.785354e-04, lr_lora=9.570707e-05 | elapsed=03h:17m:01s
2025-12-19 11:32:47,905 [INFO] Trainer: Eval  [Step  21000/100000, Epoch 0.802/0] loss=1.6415 | lr_proj=4.785354e-04, lr_lora=9.570707e-05 | elapsed=03h:18m:30s
2025-12-19 11:32:47,970 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step21000.proj.pt
2025-12-19 11:32:48,138 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step21000.lora
2025-12-19 11:32:48,306 [INFO] Trainer: Saved config to sft5b/checkpoint_step21000.config.json
2025-12-19 11:32:48,308 [INFO] Trainer: Removing sft5b/checkpoint_step18000.optim.pt.*
2025-12-19 11:32:48,323 [INFO] Trainer: Removing sft5b/checkpoint_step18000.config.json.*
2025-12-19 11:32:48,324 [INFO] Trainer: Removing sft5b/checkpoint_step18000.proj.pt.*
2025-12-19 11:32:48,326 [INFO] Trainer: Removing sft5b/checkpoint_step18000.lora.*
2025-12-19 11:33:36,905 [INFO] Trainer: Train [Step  21100/100000, Epoch 0.806/0] loss=1.1459 | lr_proj=4.784091e-04, lr_lora=9.568182e-05 | elapsed=03h:19m:19s
2025-12-19 11:34:25,684 [INFO] Trainer: Train [Step  21200/100000, Epoch 0.810/0] loss=1.4154 | lr_proj=4.782828e-04, lr_lora=9.565657e-05 | elapsed=03h:20m:07s
2025-12-19 11:35:12,545 [INFO] Trainer: Train [Step  21300/100000, Epoch 0.813/0] loss=1.3632 | lr_proj=4.781566e-04, lr_lora=9.563131e-05 | elapsed=03h:20m:54s
2025-12-19 11:36:00,162 [INFO] Trainer: Train [Step  21400/100000, Epoch 0.817/0] loss=1.4610 | lr_proj=4.780303e-04, lr_lora=9.560606e-05 | elapsed=03h:21m:42s
2025-12-19 11:36:46,985 [INFO] Trainer: Train [Step  21500/100000, Epoch 0.821/0] loss=1.5050 | lr_proj=4.779040e-04, lr_lora=9.558081e-05 | elapsed=03h:22m:29s
2025-12-19 11:37:35,222 [INFO] Trainer: Train [Step  21600/100000, Epoch 0.825/0] loss=1.3739 | lr_proj=4.777778e-04, lr_lora=9.555556e-05 | elapsed=03h:23m:17s
2025-12-19 11:38:20,624 [INFO] Trainer: Train [Step  21700/100000, Epoch 0.829/0] loss=1.3884 | lr_proj=4.776515e-04, lr_lora=9.553030e-05 | elapsed=03h:24m:02s
2025-12-19 11:39:08,775 [INFO] Trainer: Train [Step  21800/100000, Epoch 0.832/0] loss=1.6163 | lr_proj=4.775253e-04, lr_lora=9.550505e-05 | elapsed=03h:24m:50s
2025-12-19 11:39:55,347 [INFO] Trainer: Train [Step  21900/100000, Epoch 0.836/0] loss=1.3561 | lr_proj=4.773990e-04, lr_lora=9.547980e-05 | elapsed=03h:25m:37s
2025-12-19 11:40:45,316 [INFO] Trainer: Train [Step  22000/100000, Epoch 0.840/0] loss=1.1998 | lr_proj=4.772727e-04, lr_lora=9.545455e-05 | elapsed=03h:26m:27s
2025-12-19 11:42:13,594 [INFO] Trainer: Eval  [Step  22000/100000, Epoch 0.840/0] loss=1.6571 | lr_proj=4.772727e-04, lr_lora=9.545455e-05 | elapsed=03h:27m:55s
2025-12-19 11:42:13,653 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step22000.proj.pt
2025-12-19 11:42:13,785 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step22000.lora
2025-12-19 11:42:13,979 [INFO] Trainer: Saved config to sft5b/checkpoint_step22000.config.json
2025-12-19 11:42:13,982 [INFO] Trainer: Removing sft5b/checkpoint_step19000.proj.pt.*
2025-12-19 11:42:13,984 [INFO] Trainer: Removing sft5b/checkpoint_step19000.config.json.*
2025-12-19 11:42:13,985 [INFO] Trainer: Removing sft5b/checkpoint_step19000.lora.*
2025-12-19 11:42:13,993 [INFO] Trainer: Removing sft5b/checkpoint_step19000.optim.pt.*
2025-12-19 11:43:03,982 [INFO] Trainer: Train [Step  22100/100000, Epoch 0.844/0] loss=1.1156 | lr_proj=4.771465e-04, lr_lora=9.542929e-05 | elapsed=03h:28m:46s
2025-12-19 11:43:52,835 [INFO] Trainer: Train [Step  22200/100000, Epoch 0.848/0] loss=1.2544 | lr_proj=4.770202e-04, lr_lora=9.540404e-05 | elapsed=03h:29m:35s
2025-12-19 11:44:42,086 [INFO] Trainer: Train [Step  22300/100000, Epoch 0.852/0] loss=1.2777 | lr_proj=4.768939e-04, lr_lora=9.537879e-05 | elapsed=03h:30m:24s
2025-12-19 11:45:29,136 [INFO] Trainer: Train [Step  22400/100000, Epoch 0.855/0] loss=1.6733 | lr_proj=4.767677e-04, lr_lora=9.535354e-05 | elapsed=03h:31m:11s
2025-12-19 11:46:17,863 [INFO] Trainer: Train [Step  22500/100000, Epoch 0.859/0] loss=1.3807 | lr_proj=4.766414e-04, lr_lora=9.532828e-05 | elapsed=03h:32m:00s
2025-12-19 11:47:03,560 [INFO] Trainer: Train [Step  22600/100000, Epoch 0.863/0] loss=1.6163 | lr_proj=4.765152e-04, lr_lora=9.530303e-05 | elapsed=03h:32m:45s
2025-12-19 11:47:52,816 [INFO] Trainer: Train [Step  22700/100000, Epoch 0.867/0] loss=1.2535 | lr_proj=4.763889e-04, lr_lora=9.527778e-05 | elapsed=03h:33m:34s
2025-12-19 11:48:40,813 [INFO] Trainer: Train [Step  22800/100000, Epoch 0.871/0] loss=1.5664 | lr_proj=4.762626e-04, lr_lora=9.525253e-05 | elapsed=03h:34m:22s
2025-12-19 11:49:28,742 [INFO] Trainer: Train [Step  22900/100000, Epoch 0.874/0] loss=1.1365 | lr_proj=4.761364e-04, lr_lora=9.522727e-05 | elapsed=03h:35m:10s
2025-12-19 11:50:17,406 [INFO] Trainer: Train [Step  23000/100000, Epoch 0.878/0] loss=1.1472 | lr_proj=4.760101e-04, lr_lora=9.520202e-05 | elapsed=03h:35m:59s
2025-12-19 11:51:45,457 [INFO] Trainer: Eval  [Step  23000/100000, Epoch 0.878/0] loss=1.6392 | lr_proj=4.760101e-04, lr_lora=9.520202e-05 | elapsed=03h:37m:27s
2025-12-19 11:51:45,521 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step23000.proj.pt
2025-12-19 11:51:45,705 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step23000.lora
2025-12-19 11:51:45,871 [INFO] Trainer: Saved config to sft5b/checkpoint_step23000.config.json
2025-12-19 11:51:45,873 [INFO] Trainer: Removing sft5b/checkpoint_step20000.optim.pt.*
2025-12-19 11:51:45,890 [INFO] Trainer: Removing sft5b/checkpoint_step20000.lora.*
2025-12-19 11:51:45,895 [INFO] Trainer: Removing sft5b/checkpoint_step20000.proj.pt.*
2025-12-19 11:51:45,897 [INFO] Trainer: Removing sft5b/checkpoint_step20000.config.json.*
2025-12-19 11:52:33,933 [INFO] Trainer: Train [Step  23100/100000, Epoch 0.882/0] loss=1.1824 | lr_proj=4.758838e-04, lr_lora=9.517677e-05 | elapsed=03h:38m:16s
2025-12-19 11:53:24,639 [INFO] Trainer: Train [Step  23200/100000, Epoch 0.886/0] loss=1.1181 | lr_proj=4.757576e-04, lr_lora=9.515152e-05 | elapsed=03h:39m:06s
2025-12-19 11:54:10,534 [INFO] Trainer: Train [Step  23300/100000, Epoch 0.890/0] loss=1.3433 | lr_proj=4.756313e-04, lr_lora=9.512626e-05 | elapsed=03h:39m:52s
2025-12-19 11:54:58,587 [INFO] Trainer: Train [Step  23400/100000, Epoch 0.894/0] loss=1.2150 | lr_proj=4.755051e-04, lr_lora=9.510101e-05 | elapsed=03h:40m:40s
2025-12-19 11:55:48,291 [INFO] Trainer: Train [Step  23500/100000, Epoch 0.897/0] loss=1.1174 | lr_proj=4.753788e-04, lr_lora=9.507576e-05 | elapsed=03h:41m:30s
2025-12-19 11:56:36,217 [INFO] Trainer: Train [Step  23600/100000, Epoch 0.901/0] loss=1.3315 | lr_proj=4.752525e-04, lr_lora=9.505051e-05 | elapsed=03h:42m:18s
2025-12-19 11:57:26,806 [INFO] Trainer: Train [Step  23700/100000, Epoch 0.905/0] loss=1.1215 | lr_proj=4.751263e-04, lr_lora=9.502525e-05 | elapsed=03h:43m:08s
2025-12-19 11:58:16,365 [INFO] Trainer: Train [Step  23800/100000, Epoch 0.909/0] loss=1.5883 | lr_proj=4.750000e-04, lr_lora=9.500000e-05 | elapsed=03h:43m:58s
2025-12-19 11:59:03,957 [INFO] Trainer: Train [Step  23900/100000, Epoch 0.913/0] loss=1.5615 | lr_proj=4.748737e-04, lr_lora=9.497475e-05 | elapsed=03h:44m:46s
2025-12-19 11:59:51,401 [INFO] Trainer: Train [Step  24000/100000, Epoch 0.916/0] loss=1.8728 | lr_proj=4.747475e-04, lr_lora=9.494949e-05 | elapsed=03h:45m:33s
2025-12-19 12:01:19,303 [INFO] Trainer: Eval  [Step  24000/100000, Epoch 0.916/0] loss=1.6315 | lr_proj=4.747475e-04, lr_lora=9.494949e-05 | elapsed=03h:47m:01s
2025-12-19 12:01:19,390 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step24000.proj.pt
2025-12-19 12:01:19,577 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step24000.lora
2025-12-19 12:01:19,795 [INFO] Trainer: Saved config to sft5b/checkpoint_step24000.config.json
2025-12-19 12:01:19,797 [INFO] Trainer: Removing sft5b/checkpoint_step21000.proj.pt.*
2025-12-19 12:01:19,829 [INFO] Trainer: Removing sft5b/checkpoint_step21000.lora.*
2025-12-19 12:01:19,836 [INFO] Trainer: Removing sft5b/checkpoint_step21000.optim.pt.*
2025-12-19 12:01:19,863 [INFO] Trainer: Removing sft5b/checkpoint_step21000.config.json.*
2025-12-19 12:02:05,913 [INFO] Trainer: Train [Step  24100/100000, Epoch 0.920/0] loss=1.2493 | lr_proj=4.746212e-04, lr_lora=9.492424e-05 | elapsed=03h:47m:48s
2025-12-19 12:02:51,808 [INFO] Trainer: Train [Step  24200/100000, Epoch 0.924/0] loss=1.1236 | lr_proj=4.744949e-04, lr_lora=9.489899e-05 | elapsed=03h:48m:33s
2025-12-19 12:03:41,826 [INFO] Trainer: Train [Step  24300/100000, Epoch 0.928/0] loss=1.7658 | lr_proj=4.743687e-04, lr_lora=9.487374e-05 | elapsed=03h:49m:24s
2025-12-19 12:04:29,152 [INFO] Trainer: Train [Step  24400/100000, Epoch 0.932/0] loss=1.4408 | lr_proj=4.742424e-04, lr_lora=9.484848e-05 | elapsed=03h:50m:11s
2025-12-19 12:05:16,072 [INFO] Trainer: Train [Step  24500/100000, Epoch 0.936/0] loss=1.1765 | lr_proj=4.741162e-04, lr_lora=9.482323e-05 | elapsed=03h:50m:58s
2025-12-19 12:06:04,739 [INFO] Trainer: Train [Step  24600/100000, Epoch 0.939/0] loss=1.5105 | lr_proj=4.739899e-04, lr_lora=9.479798e-05 | elapsed=03h:51m:46s
2025-12-19 12:06:54,076 [INFO] Trainer: Train [Step  24700/100000, Epoch 0.943/0] loss=1.3276 | lr_proj=4.738636e-04, lr_lora=9.477273e-05 | elapsed=03h:52m:36s
2025-12-19 12:07:43,324 [INFO] Trainer: Train [Step  24800/100000, Epoch 0.947/0] loss=1.7028 | lr_proj=4.737374e-04, lr_lora=9.474747e-05 | elapsed=03h:53m:25s
2025-12-19 12:08:31,589 [INFO] Trainer: Train [Step  24900/100000, Epoch 0.951/0] loss=1.4608 | lr_proj=4.736111e-04, lr_lora=9.472222e-05 | elapsed=03h:54m:13s
2025-12-19 12:09:18,404 [INFO] Trainer: Train [Step  25000/100000, Epoch 0.955/0] loss=1.4599 | lr_proj=4.734848e-04, lr_lora=9.469697e-05 | elapsed=03h:55m:00s
2025-12-19 12:10:46,239 [INFO] Trainer: Eval  [Step  25000/100000, Epoch 0.955/0] loss=1.6248 | lr_proj=4.734848e-04, lr_lora=9.469697e-05 | elapsed=03h:56m:28s
2025-12-19 12:10:46,387 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step25000.proj.pt
2025-12-19 12:10:46,488 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step25000.lora
2025-12-19 12:10:46,669 [INFO] Trainer: Saved config to sft5b/checkpoint_step25000.config.json
2025-12-19 12:10:46,671 [INFO] Trainer: Removing sft5b/checkpoint_step22000.config.json.*
2025-12-19 12:10:46,687 [INFO] Trainer: Removing sft5b/checkpoint_step22000.lora.*
2025-12-19 12:10:46,689 [INFO] Trainer: Removing sft5b/checkpoint_step22000.optim.pt.*
2025-12-19 12:10:46,709 [INFO] Trainer: Removing sft5b/checkpoint_step22000.proj.pt.*
2025-12-19 12:11:32,265 [INFO] Trainer: Train [Step  25100/100000, Epoch 0.958/0] loss=1.5894 | lr_proj=4.733586e-04, lr_lora=9.467172e-05 | elapsed=03h:57m:14s
2025-12-19 12:12:19,058 [INFO] Trainer: Train [Step  25200/100000, Epoch 0.962/0] loss=1.4199 | lr_proj=4.732323e-04, lr_lora=9.464646e-05 | elapsed=03h:58m:01s
2025-12-19 12:13:08,035 [INFO] Trainer: Train [Step  25300/100000, Epoch 0.966/0] loss=1.7704 | lr_proj=4.731061e-04, lr_lora=9.462121e-05 | elapsed=03h:58m:50s
2025-12-19 12:13:55,946 [INFO] Trainer: Train [Step  25400/100000, Epoch 0.970/0] loss=1.9083 | lr_proj=4.729798e-04, lr_lora=9.459596e-05 | elapsed=03h:59m:38s
2025-12-19 12:14:44,367 [INFO] Trainer: Train [Step  25500/100000, Epoch 0.974/0] loss=1.7082 | lr_proj=4.728535e-04, lr_lora=9.457071e-05 | elapsed=04h:00m:26s
2025-12-19 12:15:32,635 [INFO] Trainer: Train [Step  25600/100000, Epoch 0.978/0] loss=1.3542 | lr_proj=4.727273e-04, lr_lora=9.454545e-05 | elapsed=04h:01m:14s
2025-12-19 12:16:19,994 [INFO] Trainer: Train [Step  25700/100000, Epoch 0.981/0] loss=1.5870 | lr_proj=4.726010e-04, lr_lora=9.452020e-05 | elapsed=04h:02m:02s
2025-12-19 12:17:08,031 [INFO] Trainer: Train [Step  25800/100000, Epoch 0.985/0] loss=1.5909 | lr_proj=4.724747e-04, lr_lora=9.449495e-05 | elapsed=04h:02m:50s
2025-12-19 12:17:55,344 [INFO] Trainer: Train [Step  25900/100000, Epoch 0.989/0] loss=1.5282 | lr_proj=4.723485e-04, lr_lora=9.446970e-05 | elapsed=04h:03m:37s
2025-12-19 12:18:42,814 [INFO] Trainer: Train [Step  26000/100000, Epoch 0.993/0] loss=1.5198 | lr_proj=4.722222e-04, lr_lora=9.444444e-05 | elapsed=04h:04m:24s
2025-12-19 12:20:10,758 [INFO] Trainer: Eval  [Step  26000/100000, Epoch 0.993/0] loss=1.6279 | lr_proj=4.722222e-04, lr_lora=9.444444e-05 | elapsed=04h:05m:52s
2025-12-19 12:20:10,911 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step26000.proj.pt
2025-12-19 12:20:11,072 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step26000.lora
2025-12-19 12:20:11,303 [INFO] Trainer: Saved config to sft5b/checkpoint_step26000.config.json
2025-12-19 12:20:11,305 [INFO] Trainer: Removing sft5b/checkpoint_step23000.optim.pt.*
2025-12-19 12:20:11,323 [INFO] Trainer: Removing sft5b/checkpoint_step23000.lora.*
2025-12-19 12:20:11,334 [INFO] Trainer: Removing sft5b/checkpoint_step23000.config.json.*
2025-12-19 12:20:11,336 [INFO] Trainer: Removing sft5b/checkpoint_step23000.proj.pt.*
2025-12-19 12:21:01,274 [INFO] Trainer: Train [Step  26100/100000, Epoch 0.997/0] loss=1.3481 | lr_proj=4.720960e-04, lr_lora=9.441919e-05 | elapsed=04h:06m:43s
2025-12-19 12:21:47,969 [INFO] Trainer: Train [Step  26200/100000, Epoch 1.000/0] loss=1.4047 | lr_proj=4.719697e-04, lr_lora=9.439394e-05 | elapsed=04h:07m:30s
2025-12-19 12:22:36,421 [INFO] Trainer: Train [Step  26300/100000, Epoch 1.004/0] loss=1.3863 | lr_proj=4.718434e-04, lr_lora=9.436869e-05 | elapsed=04h:08m:18s
2025-12-19 12:23:23,849 [INFO] Trainer: Train [Step  26400/100000, Epoch 1.008/0] loss=1.8315 | lr_proj=4.717172e-04, lr_lora=9.434343e-05 | elapsed=04h:09m:06s
2025-12-19 12:24:10,733 [INFO] Trainer: Train [Step  26500/100000, Epoch 1.012/0] loss=1.6721 | lr_proj=4.715909e-04, lr_lora=9.431818e-05 | elapsed=04h:09m:52s
2025-12-19 12:24:59,886 [INFO] Trainer: Train [Step  26600/100000, Epoch 1.016/0] loss=1.5558 | lr_proj=4.714646e-04, lr_lora=9.429293e-05 | elapsed=04h:10m:42s
2025-12-19 12:25:47,032 [INFO] Trainer: Train [Step  26700/100000, Epoch 1.020/0] loss=1.4430 | lr_proj=4.713384e-04, lr_lora=9.426768e-05 | elapsed=04h:11m:29s
2025-12-19 12:26:36,017 [INFO] Trainer: Train [Step  26800/100000, Epoch 1.023/0] loss=1.6943 | lr_proj=4.712121e-04, lr_lora=9.424242e-05 | elapsed=04h:12m:18s
2025-12-19 12:27:23,374 [INFO] Trainer: Train [Step  26900/100000, Epoch 1.027/0] loss=1.3843 | lr_proj=4.710859e-04, lr_lora=9.421717e-05 | elapsed=04h:13m:05s
2025-12-19 12:28:12,752 [INFO] Trainer: Train [Step  27000/100000, Epoch 1.031/0] loss=1.7626 | lr_proj=4.709596e-04, lr_lora=9.419192e-05 | elapsed=04h:13m:54s
2025-12-19 12:29:40,764 [INFO] Trainer: Eval  [Step  27000/100000, Epoch 1.031/0] loss=1.6293 | lr_proj=4.709596e-04, lr_lora=9.419192e-05 | elapsed=04h:15m:22s
2025-12-19 12:29:40,835 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step27000.proj.pt
2025-12-19 12:29:40,966 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step27000.lora
2025-12-19 12:29:41,221 [INFO] Trainer: Saved config to sft5b/checkpoint_step27000.config.json
2025-12-19 12:29:41,223 [INFO] Trainer: Removing sft5b/checkpoint_step24000.proj.pt.*
2025-12-19 12:29:41,228 [INFO] Trainer: Removing sft5b/checkpoint_step24000.lora.*
2025-12-19 12:29:41,240 [INFO] Trainer: Removing sft5b/checkpoint_step24000.optim.pt.*
2025-12-19 12:29:41,279 [INFO] Trainer: Removing sft5b/checkpoint_step24000.config.json.*
2025-12-19 12:30:29,746 [INFO] Trainer: Train [Step  27100/100000, Epoch 1.035/0] loss=1.3801 | lr_proj=4.708333e-04, lr_lora=9.416667e-05 | elapsed=04h:16m:11s
2025-12-19 12:31:18,573 [INFO] Trainer: Train [Step  27200/100000, Epoch 1.039/0] loss=1.3213 | lr_proj=4.707071e-04, lr_lora=9.414141e-05 | elapsed=04h:17m:00s
2025-12-19 12:32:05,766 [INFO] Trainer: Train [Step  27300/100000, Epoch 1.043/0] loss=1.8517 | lr_proj=4.705808e-04, lr_lora=9.411616e-05 | elapsed=04h:17m:47s
2025-12-19 12:32:54,552 [INFO] Trainer: Train [Step  27400/100000, Epoch 1.046/0] loss=1.6026 | lr_proj=4.704545e-04, lr_lora=9.409091e-05 | elapsed=04h:18m:36s
2025-12-19 12:33:41,084 [INFO] Trainer: Train [Step  27500/100000, Epoch 1.050/0] loss=1.5556 | lr_proj=4.703283e-04, lr_lora=9.406566e-05 | elapsed=04h:19m:23s
2025-12-19 12:34:28,101 [INFO] Trainer: Train [Step  27600/100000, Epoch 1.054/0] loss=1.5128 | lr_proj=4.702020e-04, lr_lora=9.404040e-05 | elapsed=04h:20m:10s
2025-12-19 12:35:16,432 [INFO] Trainer: Train [Step  27700/100000, Epoch 1.058/0] loss=1.4506 | lr_proj=4.700758e-04, lr_lora=9.401515e-05 | elapsed=04h:20m:58s
2025-12-19 12:36:05,089 [INFO] Trainer: Train [Step  27800/100000, Epoch 1.062/0] loss=1.3052 | lr_proj=4.699495e-04, lr_lora=9.398990e-05 | elapsed=04h:21m:47s
2025-12-19 12:36:51,358 [INFO] Trainer: Train [Step  27900/100000, Epoch 1.065/0] loss=1.1677 | lr_proj=4.698232e-04, lr_lora=9.396465e-05 | elapsed=04h:22m:33s
2025-12-19 12:37:40,401 [INFO] Trainer: Train [Step  28000/100000, Epoch 1.069/0] loss=1.7199 | lr_proj=4.696970e-04, lr_lora=9.393939e-05 | elapsed=04h:23m:22s
2025-12-19 12:39:08,537 [INFO] Trainer: Eval  [Step  28000/100000, Epoch 1.069/0] loss=1.6144 | lr_proj=4.696970e-04, lr_lora=9.393939e-05 | elapsed=04h:24m:50s
2025-12-19 12:39:08,638 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step28000.proj.pt
2025-12-19 12:39:08,779 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step28000.lora
2025-12-19 12:39:08,935 [INFO] Trainer: Saved config to sft5b/checkpoint_step28000.config.json
2025-12-19 12:39:08,937 [INFO] Trainer: Removing sft5b/checkpoint_step25000.config.json.*
2025-12-19 12:39:08,937 [INFO] Trainer: Removing sft5b/checkpoint_step25000.proj.pt.*
2025-12-19 12:39:08,947 [INFO] Trainer: Removing sft5b/checkpoint_step25000.lora.*
2025-12-19 12:39:08,955 [INFO] Trainer: Removing sft5b/checkpoint_step25000.optim.pt.*
2025-12-19 12:39:56,626 [INFO] Trainer: Train [Step  28100/100000, Epoch 1.073/0] loss=1.5382 | lr_proj=4.695707e-04, lr_lora=9.391414e-05 | elapsed=04h:25m:38s
2025-12-19 12:40:43,979 [INFO] Trainer: Train [Step  28200/100000, Epoch 1.077/0] loss=1.5861 | lr_proj=4.694444e-04, lr_lora=9.388889e-05 | elapsed=04h:26m:26s
2025-12-19 12:41:32,257 [INFO] Trainer: Train [Step  28300/100000, Epoch 1.081/0] loss=1.3289 | lr_proj=4.693182e-04, lr_lora=9.386364e-05 | elapsed=04h:27m:14s
2025-12-19 12:42:17,579 [INFO] Trainer: Train [Step  28400/100000, Epoch 1.085/0] loss=1.5316 | lr_proj=4.691919e-04, lr_lora=9.383838e-05 | elapsed=04h:27m:59s
2025-12-19 12:43:07,336 [INFO] Trainer: Train [Step  28500/100000, Epoch 1.088/0] loss=1.7976 | lr_proj=4.690657e-04, lr_lora=9.381313e-05 | elapsed=04h:28m:49s
2025-12-19 12:43:54,107 [INFO] Trainer: Train [Step  28600/100000, Epoch 1.092/0] loss=1.4947 | lr_proj=4.689394e-04, lr_lora=9.378788e-05 | elapsed=04h:29m:36s
2025-12-19 12:44:41,016 [INFO] Trainer: Train [Step  28700/100000, Epoch 1.096/0] loss=1.6613 | lr_proj=4.688131e-04, lr_lora=9.376263e-05 | elapsed=04h:30m:23s
2025-12-19 12:45:27,121 [INFO] Trainer: Train [Step  28800/100000, Epoch 1.100/0] loss=1.4458 | lr_proj=4.686869e-04, lr_lora=9.373737e-05 | elapsed=04h:31m:09s
2025-12-19 12:46:14,059 [INFO] Trainer: Train [Step  28900/100000, Epoch 1.104/0] loss=1.5514 | lr_proj=4.685606e-04, lr_lora=9.371212e-05 | elapsed=04h:31m:56s
2025-12-19 12:47:02,229 [INFO] Trainer: Train [Step  29000/100000, Epoch 1.107/0] loss=1.3108 | lr_proj=4.684343e-04, lr_lora=9.368687e-05 | elapsed=04h:32m:44s
2025-12-19 12:48:30,203 [INFO] Trainer: Eval  [Step  29000/100000, Epoch 1.107/0] loss=1.6462 | lr_proj=4.684343e-04, lr_lora=9.368687e-05 | elapsed=04h:34m:12s
2025-12-19 12:48:30,292 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step29000.proj.pt
2025-12-19 12:48:30,438 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step29000.lora
2025-12-19 12:48:30,664 [INFO] Trainer: Saved config to sft5b/checkpoint_step29000.config.json
2025-12-19 12:48:30,667 [INFO] Trainer: Removing sft5b/checkpoint_step26000.proj.pt.*
2025-12-19 12:48:30,669 [INFO] Trainer: Removing sft5b/checkpoint_step26000.optim.pt.*
2025-12-19 12:48:30,686 [INFO] Trainer: Removing sft5b/checkpoint_step26000.lora.*
2025-12-19 12:48:30,693 [INFO] Trainer: Removing sft5b/checkpoint_step26000.config.json.*
2025-12-19 12:49:19,831 [INFO] Trainer: Train [Step  29100/100000, Epoch 1.111/0] loss=1.5753 | lr_proj=4.683081e-04, lr_lora=9.366162e-05 | elapsed=04h:35m:02s
2025-12-19 12:50:06,098 [INFO] Trainer: Train [Step  29200/100000, Epoch 1.115/0] loss=1.2971 | lr_proj=4.681818e-04, lr_lora=9.363636e-05 | elapsed=04h:35m:48s
2025-12-19 12:50:54,312 [INFO] Trainer: Train [Step  29300/100000, Epoch 1.119/0] loss=1.5060 | lr_proj=4.680556e-04, lr_lora=9.361111e-05 | elapsed=04h:36m:36s
2025-12-19 12:51:44,093 [INFO] Trainer: Train [Step  29400/100000, Epoch 1.123/0] loss=1.4289 | lr_proj=4.679293e-04, lr_lora=9.358586e-05 | elapsed=04h:37m:26s
2025-12-19 12:52:31,609 [INFO] Trainer: Train [Step  29500/100000, Epoch 1.127/0] loss=1.3009 | lr_proj=4.678030e-04, lr_lora=9.356061e-05 | elapsed=04h:38m:13s
2025-12-19 12:53:18,267 [INFO] Trainer: Train [Step  29600/100000, Epoch 1.130/0] loss=1.3274 | lr_proj=4.676768e-04, lr_lora=9.353535e-05 | elapsed=04h:39m:00s
2025-12-19 12:54:07,406 [INFO] Trainer: Train [Step  29700/100000, Epoch 1.134/0] loss=1.5059 | lr_proj=4.675505e-04, lr_lora=9.351010e-05 | elapsed=04h:39m:49s
2025-12-19 12:54:54,877 [INFO] Trainer: Train [Step  29800/100000, Epoch 1.138/0] loss=1.7316 | lr_proj=4.674242e-04, lr_lora=9.348485e-05 | elapsed=04h:40m:37s
2025-12-19 12:55:42,855 [INFO] Trainer: Train [Step  29900/100000, Epoch 1.142/0] loss=1.1743 | lr_proj=4.672980e-04, lr_lora=9.345960e-05 | elapsed=04h:41m:25s
2025-12-19 12:56:31,330 [INFO] Trainer: Train [Step  30000/100000, Epoch 1.146/0] loss=1.6264 | lr_proj=4.671717e-04, lr_lora=9.343434e-05 | elapsed=04h:42m:13s
2025-12-19 12:57:59,125 [INFO] Trainer: Eval  [Step  30000/100000, Epoch 1.146/0] loss=1.6011 | lr_proj=4.671717e-04, lr_lora=9.343434e-05 | elapsed=04h:43m:41s
2025-12-19 12:57:59,169 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step30000.proj.pt
2025-12-19 12:57:59,327 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step30000.lora
2025-12-19 12:57:59,504 [INFO] Trainer: Saved config to sft5b/checkpoint_step30000.config.json
2025-12-19 12:57:59,506 [INFO] Trainer: Removing sft5b/checkpoint_step27000.proj.pt.*
2025-12-19 12:57:59,509 [INFO] Trainer: Removing sft5b/checkpoint_step27000.optim.pt.*
2025-12-19 12:57:59,527 [INFO] Trainer: Removing sft5b/checkpoint_step27000.config.json.*
2025-12-19 12:57:59,528 [INFO] Trainer: Removing sft5b/checkpoint_step27000.lora.*
2025-12-19 12:58:45,377 [INFO] Trainer: Train [Step  30100/100000, Epoch 1.149/0] loss=1.3300 | lr_proj=4.670455e-04, lr_lora=9.340909e-05 | elapsed=04h:44m:27s
2025-12-19 12:59:32,175 [INFO] Trainer: Train [Step  30200/100000, Epoch 1.153/0] loss=1.9932 | lr_proj=4.669192e-04, lr_lora=9.338384e-05 | elapsed=04h:45m:14s
2025-12-19 13:00:20,848 [INFO] Trainer: Train [Step  30300/100000, Epoch 1.157/0] loss=1.2710 | lr_proj=4.667929e-04, lr_lora=9.335859e-05 | elapsed=04h:46m:03s
2025-12-19 13:01:08,853 [INFO] Trainer: Train [Step  30400/100000, Epoch 1.161/0] loss=1.7206 | lr_proj=4.666667e-04, lr_lora=9.333333e-05 | elapsed=04h:46m:51s
2025-12-19 13:01:54,521 [INFO] Trainer: Train [Step  30500/100000, Epoch 1.165/0] loss=1.6629 | lr_proj=4.665404e-04, lr_lora=9.330808e-05 | elapsed=04h:47m:36s
2025-12-19 13:02:40,321 [INFO] Trainer: Train [Step  30600/100000, Epoch 1.169/0] loss=1.3283 | lr_proj=4.664141e-04, lr_lora=9.328283e-05 | elapsed=04h:48m:22s
2025-12-19 13:03:28,916 [INFO] Trainer: Train [Step  30700/100000, Epoch 1.172/0] loss=1.0684 | lr_proj=4.662879e-04, lr_lora=9.325758e-05 | elapsed=04h:49m:11s
2025-12-19 13:04:15,641 [INFO] Trainer: Train [Step  30800/100000, Epoch 1.176/0] loss=1.2301 | lr_proj=4.661616e-04, lr_lora=9.323232e-05 | elapsed=04h:49m:57s
2025-12-19 13:05:02,403 [INFO] Trainer: Train [Step  30900/100000, Epoch 1.180/0] loss=1.4000 | lr_proj=4.660354e-04, lr_lora=9.320707e-05 | elapsed=04h:50m:44s
2025-12-19 13:05:50,219 [INFO] Trainer: Train [Step  31000/100000, Epoch 1.184/0] loss=1.6244 | lr_proj=4.659091e-04, lr_lora=9.318182e-05 | elapsed=04h:51m:32s
2025-12-19 13:07:18,080 [INFO] Trainer: Eval  [Step  31000/100000, Epoch 1.184/0] loss=1.6090 | lr_proj=4.659091e-04, lr_lora=9.318182e-05 | elapsed=04h:53m:00s
2025-12-19 13:07:18,128 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step31000.proj.pt
2025-12-19 13:07:18,314 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step31000.lora
2025-12-19 13:07:18,514 [INFO] Trainer: Saved config to sft5b/checkpoint_step31000.config.json
2025-12-19 13:07:18,516 [INFO] Trainer: Removing sft5b/checkpoint_step28000.optim.pt.*
2025-12-19 13:07:18,559 [INFO] Trainer: Removing sft5b/checkpoint_step28000.lora.*
2025-12-19 13:07:18,562 [INFO] Trainer: Removing sft5b/checkpoint_step28000.config.json.*
2025-12-19 13:07:18,563 [INFO] Trainer: Removing sft5b/checkpoint_step28000.proj.pt.*
2025-12-19 13:08:02,798 [INFO] Trainer: Train [Step  31100/100000, Epoch 1.188/0] loss=1.2537 | lr_proj=4.657828e-04, lr_lora=9.315657e-05 | elapsed=04h:53m:44s
2025-12-19 13:08:52,208 [INFO] Trainer: Train [Step  31200/100000, Epoch 1.191/0] loss=1.1847 | lr_proj=4.656566e-04, lr_lora=9.313131e-05 | elapsed=04h:54m:34s
2025-12-19 13:09:41,354 [INFO] Trainer: Train [Step  31300/100000, Epoch 1.195/0] loss=1.2839 | lr_proj=4.655303e-04, lr_lora=9.310606e-05 | elapsed=04h:55m:23s
2025-12-19 13:10:28,183 [INFO] Trainer: Train [Step  31400/100000, Epoch 1.199/0] loss=1.8175 | lr_proj=4.654040e-04, lr_lora=9.308081e-05 | elapsed=04h:56m:10s
2025-12-19 13:11:17,258 [INFO] Trainer: Train [Step  31500/100000, Epoch 1.203/0] loss=1.5010 | lr_proj=4.652778e-04, lr_lora=9.305556e-05 | elapsed=04h:56m:59s
2025-12-19 13:12:04,878 [INFO] Trainer: Train [Step  31600/100000, Epoch 1.207/0] loss=1.4148 | lr_proj=4.651515e-04, lr_lora=9.303030e-05 | elapsed=04h:57m:47s
2025-12-19 13:12:55,076 [INFO] Trainer: Train [Step  31700/100000, Epoch 1.211/0] loss=1.1589 | lr_proj=4.650253e-04, lr_lora=9.300505e-05 | elapsed=04h:58m:37s
2025-12-19 13:13:41,122 [INFO] Trainer: Train [Step  31800/100000, Epoch 1.214/0] loss=1.4591 | lr_proj=4.648990e-04, lr_lora=9.297980e-05 | elapsed=04h:59m:23s
2025-12-19 13:14:27,833 [INFO] Trainer: Train [Step  31900/100000, Epoch 1.218/0] loss=1.3315 | lr_proj=4.647727e-04, lr_lora=9.295455e-05 | elapsed=05h:00m:10s
2025-12-19 13:15:16,784 [INFO] Trainer: Train [Step  32000/100000, Epoch 1.222/0] loss=1.3341 | lr_proj=4.646465e-04, lr_lora=9.292929e-05 | elapsed=05h:00m:58s
2025-12-19 13:16:44,830 [INFO] Trainer: Eval  [Step  32000/100000, Epoch 1.222/0] loss=1.6055 | lr_proj=4.646465e-04, lr_lora=9.292929e-05 | elapsed=05h:02m:27s
2025-12-19 13:16:44,947 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step32000.proj.pt
2025-12-19 13:16:45,121 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step32000.lora
2025-12-19 13:16:45,308 [INFO] Trainer: Saved config to sft5b/checkpoint_step32000.config.json
2025-12-19 13:16:45,310 [INFO] Trainer: Removing sft5b/checkpoint_step29000.lora.*
2025-12-19 13:16:45,314 [INFO] Trainer: Removing sft5b/checkpoint_step29000.proj.pt.*
2025-12-19 13:16:45,317 [INFO] Trainer: Removing sft5b/checkpoint_step29000.config.json.*
2025-12-19 13:16:45,330 [INFO] Trainer: Removing sft5b/checkpoint_step29000.optim.pt.*
2025-12-19 13:17:33,915 [INFO] Trainer: Train [Step  32100/100000, Epoch 1.226/0] loss=1.4962 | lr_proj=4.645202e-04, lr_lora=9.290404e-05 | elapsed=05h:03m:16s
2025-12-19 13:18:23,942 [INFO] Trainer: Train [Step  32200/100000, Epoch 1.230/0] loss=1.0392 | lr_proj=4.643939e-04, lr_lora=9.287879e-05 | elapsed=05h:04m:06s
2025-12-19 13:19:12,306 [INFO] Trainer: Train [Step  32300/100000, Epoch 1.233/0] loss=1.3753 | lr_proj=4.642677e-04, lr_lora=9.285354e-05 | elapsed=05h:04m:54s
2025-12-19 13:19:59,889 [INFO] Trainer: Train [Step  32400/100000, Epoch 1.237/0] loss=1.0637 | lr_proj=4.641414e-04, lr_lora=9.282828e-05 | elapsed=05h:05m:42s
2025-12-19 13:20:49,965 [INFO] Trainer: Train [Step  32500/100000, Epoch 1.241/0] loss=1.6899 | lr_proj=4.640152e-04, lr_lora=9.280303e-05 | elapsed=05h:06m:32s
2025-12-19 13:21:39,721 [INFO] Trainer: Train [Step  32600/100000, Epoch 1.245/0] loss=1.2816 | lr_proj=4.638889e-04, lr_lora=9.277778e-05 | elapsed=05h:07m:21s
2025-12-19 13:22:29,939 [INFO] Trainer: Train [Step  32700/100000, Epoch 1.249/0] loss=1.5069 | lr_proj=4.637626e-04, lr_lora=9.275253e-05 | elapsed=05h:08m:12s
2025-12-19 13:23:20,190 [INFO] Trainer: Train [Step  32800/100000, Epoch 1.253/0] loss=1.2876 | lr_proj=4.636364e-04, lr_lora=9.272727e-05 | elapsed=05h:09m:02s
2025-12-19 13:24:08,079 [INFO] Trainer: Train [Step  32900/100000, Epoch 1.256/0] loss=1.3542 | lr_proj=4.635101e-04, lr_lora=9.270202e-05 | elapsed=05h:09m:50s
2025-12-19 13:24:55,814 [INFO] Trainer: Train [Step  33000/100000, Epoch 1.260/0] loss=1.5197 | lr_proj=4.633838e-04, lr_lora=9.267677e-05 | elapsed=05h:10m:37s
2025-12-19 13:26:24,680 [INFO] Trainer: Eval  [Step  33000/100000, Epoch 1.260/0] loss=1.5998 | lr_proj=4.633838e-04, lr_lora=9.267677e-05 | elapsed=05h:12m:06s
2025-12-19 13:26:24,762 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step33000.proj.pt
2025-12-19 13:26:24,895 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step33000.lora
2025-12-19 13:26:25,052 [INFO] Trainer: Saved config to sft5b/checkpoint_step33000.config.json
2025-12-19 13:26:25,054 [INFO] Trainer: Removing sft5b/checkpoint_step30000.config.json.*
2025-12-19 13:26:25,087 [INFO] Trainer: Removing sft5b/checkpoint_step30000.optim.pt.*
2025-12-19 13:26:25,104 [INFO] Trainer: Removing sft5b/checkpoint_step30000.proj.pt.*
2025-12-19 13:26:25,121 [INFO] Trainer: Removing sft5b/checkpoint_step30000.lora.*
2025-12-19 13:27:14,214 [INFO] Trainer: Train [Step  33100/100000, Epoch 1.264/0] loss=1.3371 | lr_proj=4.632576e-04, lr_lora=9.265152e-05 | elapsed=05h:12m:56s
2025-12-19 13:28:03,183 [INFO] Trainer: Train [Step  33200/100000, Epoch 1.268/0] loss=1.4179 | lr_proj=4.631313e-04, lr_lora=9.262626e-05 | elapsed=05h:13m:45s
2025-12-19 13:28:51,468 [INFO] Trainer: Train [Step  33300/100000, Epoch 1.272/0] loss=1.3279 | lr_proj=4.630051e-04, lr_lora=9.260101e-05 | elapsed=05h:14m:33s
2025-12-19 13:29:39,945 [INFO] Trainer: Train [Step  33400/100000, Epoch 1.275/0] loss=1.3785 | lr_proj=4.628788e-04, lr_lora=9.257576e-05 | elapsed=05h:15m:22s
2025-12-19 13:30:28,443 [INFO] Trainer: Train [Step  33500/100000, Epoch 1.279/0] loss=1.2627 | lr_proj=4.627525e-04, lr_lora=9.255051e-05 | elapsed=05h:16m:10s
2025-12-19 13:31:17,489 [INFO] Trainer: Train [Step  33600/100000, Epoch 1.283/0] loss=1.4396 | lr_proj=4.626263e-04, lr_lora=9.252525e-05 | elapsed=05h:16m:59s
2025-12-19 13:32:06,345 [INFO] Trainer: Train [Step  33700/100000, Epoch 1.287/0] loss=1.0685 | lr_proj=4.625000e-04, lr_lora=9.250000e-05 | elapsed=05h:17m:48s
2025-12-19 13:32:55,542 [INFO] Trainer: Train [Step  33800/100000, Epoch 1.291/0] loss=1.5282 | lr_proj=4.623737e-04, lr_lora=9.247475e-05 | elapsed=05h:18m:37s
2025-12-19 13:33:41,811 [INFO] Trainer: Train [Step  33900/100000, Epoch 1.295/0] loss=1.5610 | lr_proj=4.622475e-04, lr_lora=9.244949e-05 | elapsed=05h:19m:23s
2025-12-19 13:34:28,371 [INFO] Trainer: Train [Step  34000/100000, Epoch 1.298/0] loss=1.3021 | lr_proj=4.621212e-04, lr_lora=9.242424e-05 | elapsed=05h:20m:10s
2025-12-19 13:35:58,032 [INFO] Trainer: Eval  [Step  34000/100000, Epoch 1.298/0] loss=1.6333 | lr_proj=4.621212e-04, lr_lora=9.242424e-05 | elapsed=05h:21m:40s
2025-12-19 13:35:58,122 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step34000.proj.pt
2025-12-19 13:35:58,293 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step34000.lora
2025-12-19 13:35:58,502 [INFO] Trainer: Saved config to sft5b/checkpoint_step34000.config.json
2025-12-19 13:35:58,504 [INFO] Trainer: Removing sft5b/checkpoint_step31000.lora.*
2025-12-19 13:35:58,512 [INFO] Trainer: Removing sft5b/checkpoint_step31000.optim.pt.*
2025-12-19 13:35:58,524 [INFO] Trainer: Removing sft5b/checkpoint_step31000.proj.pt.*
2025-12-19 13:35:58,527 [INFO] Trainer: Removing sft5b/checkpoint_step31000.config.json.*
2025-12-19 13:36:46,779 [INFO] Trainer: Train [Step  34100/100000, Epoch 1.302/0] loss=1.3344 | lr_proj=4.619949e-04, lr_lora=9.239899e-05 | elapsed=05h:22m:28s
2025-12-19 13:37:35,106 [INFO] Trainer: Train [Step  34200/100000, Epoch 1.306/0] loss=1.1340 | lr_proj=4.618687e-04, lr_lora=9.237374e-05 | elapsed=05h:23m:17s
2025-12-19 13:38:22,012 [INFO] Trainer: Train [Step  34300/100000, Epoch 1.310/0] loss=1.2439 | lr_proj=4.617424e-04, lr_lora=9.234848e-05 | elapsed=05h:24m:04s
2025-12-19 13:39:08,242 [INFO] Trainer: Train [Step  34400/100000, Epoch 1.314/0] loss=1.3587 | lr_proj=4.616162e-04, lr_lora=9.232323e-05 | elapsed=05h:24m:50s
2025-12-19 13:39:57,402 [INFO] Trainer: Train [Step  34500/100000, Epoch 1.317/0] loss=1.4950 | lr_proj=4.614899e-04, lr_lora=9.229798e-05 | elapsed=05h:25m:39s
2025-12-19 13:40:46,701 [INFO] Trainer: Train [Step  34600/100000, Epoch 1.321/0] loss=1.3705 | lr_proj=4.613636e-04, lr_lora=9.227273e-05 | elapsed=05h:26m:28s
2025-12-19 13:41:36,023 [INFO] Trainer: Train [Step  34700/100000, Epoch 1.325/0] loss=1.3890 | lr_proj=4.612374e-04, lr_lora=9.224747e-05 | elapsed=05h:27m:18s
2025-12-19 13:42:24,603 [INFO] Trainer: Train [Step  34800/100000, Epoch 1.329/0] loss=1.2172 | lr_proj=4.611111e-04, lr_lora=9.222222e-05 | elapsed=05h:28m:06s
2025-12-19 13:43:14,911 [INFO] Trainer: Train [Step  34900/100000, Epoch 1.333/0] loss=1.2150 | lr_proj=4.609848e-04, lr_lora=9.219697e-05 | elapsed=05h:28m:57s
2025-12-19 13:44:03,912 [INFO] Trainer: Train [Step  35000/100000, Epoch 1.337/0] loss=1.2801 | lr_proj=4.608586e-04, lr_lora=9.217172e-05 | elapsed=05h:29m:46s
2025-12-19 13:45:32,515 [INFO] Trainer: Eval  [Step  35000/100000, Epoch 1.337/0] loss=1.6087 | lr_proj=4.608586e-04, lr_lora=9.217172e-05 | elapsed=05h:31m:14s
2025-12-19 13:45:32,625 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step35000.proj.pt
2025-12-19 13:45:32,771 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step35000.lora
2025-12-19 13:45:32,989 [INFO] Trainer: Saved config to sft5b/checkpoint_step35000.config.json
2025-12-19 13:45:32,991 [INFO] Trainer: Removing sft5b/checkpoint_step32000.lora.*
2025-12-19 13:45:32,997 [INFO] Trainer: Removing sft5b/checkpoint_step32000.config.json.*
2025-12-19 13:45:33,056 [INFO] Trainer: Removing sft5b/checkpoint_step32000.proj.pt.*
2025-12-19 13:45:33,128 [INFO] Trainer: Removing sft5b/checkpoint_step32000.optim.pt.*
2025-12-19 13:46:22,281 [INFO] Trainer: Train [Step  35100/100000, Epoch 1.340/0] loss=1.3710 | lr_proj=4.607323e-04, lr_lora=9.214646e-05 | elapsed=05h:32m:04s
2025-12-19 13:47:10,099 [INFO] Trainer: Train [Step  35200/100000, Epoch 1.344/0] loss=1.4100 | lr_proj=4.606061e-04, lr_lora=9.212121e-05 | elapsed=05h:32m:52s
2025-12-19 13:47:59,882 [INFO] Trainer: Train [Step  35300/100000, Epoch 1.348/0] loss=1.5166 | lr_proj=4.604798e-04, lr_lora=9.209596e-05 | elapsed=05h:33m:42s
2025-12-19 13:48:48,871 [INFO] Trainer: Train [Step  35400/100000, Epoch 1.352/0] loss=1.7157 | lr_proj=4.603535e-04, lr_lora=9.207071e-05 | elapsed=05h:34m:31s
2025-12-19 13:49:37,622 [INFO] Trainer: Train [Step  35500/100000, Epoch 1.356/0] loss=1.3069 | lr_proj=4.602273e-04, lr_lora=9.204545e-05 | elapsed=05h:35m:19s
2025-12-19 13:50:26,160 [INFO] Trainer: Train [Step  35600/100000, Epoch 1.359/0] loss=1.0913 | lr_proj=4.601010e-04, lr_lora=9.202020e-05 | elapsed=05h:36m:08s
2025-12-19 13:51:17,549 [INFO] Trainer: Train [Step  35700/100000, Epoch 1.363/0] loss=1.1843 | lr_proj=4.599747e-04, lr_lora=9.199495e-05 | elapsed=05h:36m:59s
2025-12-19 13:52:01,868 [INFO] Trainer: Train [Step  35800/100000, Epoch 1.367/0] loss=1.4756 | lr_proj=4.598485e-04, lr_lora=9.196970e-05 | elapsed=05h:37m:44s
2025-12-19 13:52:51,001 [INFO] Trainer: Train [Step  35900/100000, Epoch 1.371/0] loss=1.3563 | lr_proj=4.597222e-04, lr_lora=9.194444e-05 | elapsed=05h:38m:33s
2025-12-19 13:53:38,989 [INFO] Trainer: Train [Step  36000/100000, Epoch 1.375/0] loss=1.0674 | lr_proj=4.595960e-04, lr_lora=9.191919e-05 | elapsed=05h:39m:21s
2025-12-19 13:55:07,558 [INFO] Trainer: Eval  [Step  36000/100000, Epoch 1.375/0] loss=1.6035 | lr_proj=4.595960e-04, lr_lora=9.191919e-05 | elapsed=05h:40m:49s
2025-12-19 13:55:07,633 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step36000.proj.pt
2025-12-19 13:55:07,802 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step36000.lora
2025-12-19 13:55:08,013 [INFO] Trainer: Saved config to sft5b/checkpoint_step36000.config.json
2025-12-19 13:55:08,015 [INFO] Trainer: Removing sft5b/checkpoint_step33000.optim.pt.*
2025-12-19 13:55:08,017 [INFO] Trainer: Removing sft5b/checkpoint_step33000.proj.pt.*
2025-12-19 13:55:08,018 [INFO] Trainer: Removing sft5b/checkpoint_step33000.config.json.*
2025-12-19 13:55:08,019 [INFO] Trainer: Removing sft5b/checkpoint_step33000.lora.*
2025-12-19 13:55:54,153 [INFO] Trainer: Train [Step  36100/100000, Epoch 1.379/0] loss=1.2526 | lr_proj=4.594697e-04, lr_lora=9.189394e-05 | elapsed=05h:41m:36s
2025-12-19 13:56:43,030 [INFO] Trainer: Train [Step  36200/100000, Epoch 1.382/0] loss=1.1273 | lr_proj=4.593434e-04, lr_lora=9.186869e-05 | elapsed=05h:42m:25s
2025-12-19 13:57:30,356 [INFO] Trainer: Train [Step  36300/100000, Epoch 1.386/0] loss=1.5074 | lr_proj=4.592172e-04, lr_lora=9.184343e-05 | elapsed=05h:43m:12s
2025-12-19 13:58:19,175 [INFO] Trainer: Train [Step  36400/100000, Epoch 1.390/0] loss=1.3399 | lr_proj=4.590909e-04, lr_lora=9.181818e-05 | elapsed=05h:44m:01s
2025-12-19 13:59:05,879 [INFO] Trainer: Train [Step  36500/100000, Epoch 1.394/0] loss=1.3292 | lr_proj=4.589646e-04, lr_lora=9.179293e-05 | elapsed=05h:44m:48s
2025-12-19 13:59:52,958 [INFO] Trainer: Train [Step  36600/100000, Epoch 1.398/0] loss=1.4227 | lr_proj=4.588384e-04, lr_lora=9.176768e-05 | elapsed=05h:45m:35s
2025-12-19 14:00:39,231 [INFO] Trainer: Train [Step  36700/100000, Epoch 1.401/0] loss=1.4195 | lr_proj=4.587121e-04, lr_lora=9.174242e-05 | elapsed=05h:46m:21s
2025-12-19 14:01:27,153 [INFO] Trainer: Train [Step  36800/100000, Epoch 1.405/0] loss=1.6383 | lr_proj=4.585859e-04, lr_lora=9.171717e-05 | elapsed=05h:47m:09s
2025-12-19 14:02:15,403 [INFO] Trainer: Train [Step  36900/100000, Epoch 1.409/0] loss=1.0691 | lr_proj=4.584596e-04, lr_lora=9.169192e-05 | elapsed=05h:47m:57s
2025-12-19 14:03:03,706 [INFO] Trainer: Train [Step  37000/100000, Epoch 1.413/0] loss=1.0055 | lr_proj=4.583333e-04, lr_lora=9.166667e-05 | elapsed=05h:48m:45s
2025-12-19 14:04:32,568 [INFO] Trainer: Eval  [Step  37000/100000, Epoch 1.413/0] loss=1.6181 | lr_proj=4.583333e-04, lr_lora=9.166667e-05 | elapsed=05h:50m:14s
2025-12-19 14:04:32,634 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step37000.proj.pt
2025-12-19 14:04:32,778 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step37000.lora
2025-12-19 14:04:32,952 [INFO] Trainer: Saved config to sft5b/checkpoint_step37000.config.json
2025-12-19 14:04:32,954 [INFO] Trainer: Removing sft5b/checkpoint_step34000.optim.pt.*
2025-12-19 14:04:32,964 [INFO] Trainer: Removing sft5b/checkpoint_step34000.lora.*
2025-12-19 14:04:32,968 [INFO] Trainer: Removing sft5b/checkpoint_step34000.config.json.*
2025-12-19 14:04:32,969 [INFO] Trainer: Removing sft5b/checkpoint_step34000.proj.pt.*
2025-12-19 14:05:21,940 [INFO] Trainer: Train [Step  37100/100000, Epoch 1.417/0] loss=1.7838 | lr_proj=4.582071e-04, lr_lora=9.164141e-05 | elapsed=05h:51m:04s
2025-12-19 14:06:11,943 [INFO] Trainer: Train [Step  37200/100000, Epoch 1.421/0] loss=1.1443 | lr_proj=4.580808e-04, lr_lora=9.161616e-05 | elapsed=05h:51m:54s
2025-12-19 14:07:01,464 [INFO] Trainer: Train [Step  37300/100000, Epoch 1.424/0] loss=1.4995 | lr_proj=4.579545e-04, lr_lora=9.159091e-05 | elapsed=05h:52m:43s
2025-12-19 14:07:49,142 [INFO] Trainer: Train [Step  37400/100000, Epoch 1.428/0] loss=1.4732 | lr_proj=4.578283e-04, lr_lora=9.156566e-05 | elapsed=05h:53m:31s
2025-12-19 14:08:38,502 [INFO] Trainer: Train [Step  37500/100000, Epoch 1.432/0] loss=0.9926 | lr_proj=4.577020e-04, lr_lora=9.154040e-05 | elapsed=05h:54m:20s
2025-12-19 14:09:25,292 [INFO] Trainer: Train [Step  37600/100000, Epoch 1.436/0] loss=1.2376 | lr_proj=4.575758e-04, lr_lora=9.151515e-05 | elapsed=05h:55m:07s
2025-12-19 14:10:14,106 [INFO] Trainer: Train [Step  37700/100000, Epoch 1.440/0] loss=1.0882 | lr_proj=4.574495e-04, lr_lora=9.148990e-05 | elapsed=05h:55m:56s
2025-12-19 14:11:00,237 [INFO] Trainer: Train [Step  37800/100000, Epoch 1.443/0] loss=1.5210 | lr_proj=4.573232e-04, lr_lora=9.146465e-05 | elapsed=05h:56m:42s
2025-12-19 14:11:49,043 [INFO] Trainer: Train [Step  37900/100000, Epoch 1.447/0] loss=1.2675 | lr_proj=4.571970e-04, lr_lora=9.143939e-05 | elapsed=05h:57m:31s
2025-12-19 14:12:35,841 [INFO] Trainer: Train [Step  38000/100000, Epoch 1.451/0] loss=1.3779 | lr_proj=4.570707e-04, lr_lora=9.141414e-05 | elapsed=05h:58m:18s
2025-12-19 14:14:04,850 [INFO] Trainer: Eval  [Step  38000/100000, Epoch 1.451/0] loss=1.6048 | lr_proj=4.570707e-04, lr_lora=9.141414e-05 | elapsed=05h:59m:47s
2025-12-19 14:14:04,983 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step38000.proj.pt
2025-12-19 14:14:05,140 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step38000.lora
2025-12-19 14:14:05,343 [INFO] Trainer: Saved config to sft5b/checkpoint_step38000.config.json
2025-12-19 14:14:05,346 [INFO] Trainer: Removing sft5b/checkpoint_step35000.config.json.*
2025-12-19 14:14:05,349 [INFO] Trainer: Removing sft5b/checkpoint_step35000.proj.pt.*
2025-12-19 14:14:05,351 [INFO] Trainer: Removing sft5b/checkpoint_step35000.lora.*
2025-12-19 14:14:05,356 [INFO] Trainer: Removing sft5b/checkpoint_step35000.optim.pt.*
2025-12-19 14:14:55,330 [INFO] Trainer: Train [Step  38100/100000, Epoch 1.455/0] loss=1.6797 | lr_proj=4.569444e-04, lr_lora=9.138889e-05 | elapsed=06h:00m:37s
2025-12-19 14:15:43,968 [INFO] Trainer: Train [Step  38200/100000, Epoch 1.459/0] loss=1.1865 | lr_proj=4.568182e-04, lr_lora=9.136364e-05 | elapsed=06h:01m:26s
2025-12-19 14:16:31,501 [INFO] Trainer: Train [Step  38300/100000, Epoch 1.463/0] loss=1.4732 | lr_proj=4.566919e-04, lr_lora=9.133838e-05 | elapsed=06h:02m:13s
2025-12-19 14:17:18,471 [INFO] Trainer: Train [Step  38400/100000, Epoch 1.466/0] loss=1.2893 | lr_proj=4.565657e-04, lr_lora=9.131313e-05 | elapsed=06h:03m:00s
2025-12-19 14:18:04,906 [INFO] Trainer: Train [Step  38500/100000, Epoch 1.470/0] loss=1.2086 | lr_proj=4.564394e-04, lr_lora=9.128788e-05 | elapsed=06h:03m:47s
2025-12-19 14:18:52,374 [INFO] Trainer: Train [Step  38600/100000, Epoch 1.474/0] loss=1.1508 | lr_proj=4.563131e-04, lr_lora=9.126263e-05 | elapsed=06h:04m:34s
2025-12-19 14:19:40,766 [INFO] Trainer: Train [Step  38700/100000, Epoch 1.478/0] loss=1.4788 | lr_proj=4.561869e-04, lr_lora=9.123737e-05 | elapsed=06h:05m:22s
2025-12-19 14:20:28,501 [INFO] Trainer: Train [Step  38800/100000, Epoch 1.482/0] loss=1.2328 | lr_proj=4.560606e-04, lr_lora=9.121212e-05 | elapsed=06h:06m:10s
2025-12-19 14:21:18,336 [INFO] Trainer: Train [Step  38900/100000, Epoch 1.485/0] loss=1.6408 | lr_proj=4.559343e-04, lr_lora=9.118687e-05 | elapsed=06h:07m:00s
2025-12-19 14:22:03,880 [INFO] Trainer: Train [Step  39000/100000, Epoch 1.489/0] loss=1.4450 | lr_proj=4.558081e-04, lr_lora=9.116162e-05 | elapsed=06h:07m:46s
2025-12-19 14:23:31,795 [INFO] Trainer: Eval  [Step  39000/100000, Epoch 1.489/0] loss=1.5932 | lr_proj=4.558081e-04, lr_lora=9.116162e-05 | elapsed=06h:09m:13s
2025-12-19 14:23:31,884 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step39000.proj.pt
2025-12-19 14:23:32,032 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step39000.lora
2025-12-19 14:23:32,216 [INFO] Trainer: Saved config to sft5b/checkpoint_step39000.config.json
2025-12-19 14:23:32,218 [INFO] Trainer: Removing sft5b/checkpoint_step36000.optim.pt.*
2025-12-19 14:23:32,224 [INFO] Trainer: Removing sft5b/checkpoint_step36000.lora.*
2025-12-19 14:23:32,227 [INFO] Trainer: Removing sft5b/checkpoint_step36000.config.json.*
2025-12-19 14:23:32,242 [INFO] Trainer: Removing sft5b/checkpoint_step36000.proj.pt.*
2025-12-19 14:24:18,844 [INFO] Trainer: Train [Step  39100/100000, Epoch 1.493/0] loss=1.2977 | lr_proj=4.556818e-04, lr_lora=9.113636e-05 | elapsed=06h:10m:01s
2025-12-19 14:25:06,978 [INFO] Trainer: Train [Step  39200/100000, Epoch 1.497/0] loss=1.4363 | lr_proj=4.555556e-04, lr_lora=9.111111e-05 | elapsed=06h:10m:49s
2025-12-19 14:25:56,515 [INFO] Trainer: Train [Step  39300/100000, Epoch 1.501/0] loss=1.3179 | lr_proj=4.554293e-04, lr_lora=9.108586e-05 | elapsed=06h:11m:38s
2025-12-19 14:26:43,612 [INFO] Trainer: Train [Step  39400/100000, Epoch 1.505/0] loss=1.0443 | lr_proj=4.553030e-04, lr_lora=9.106061e-05 | elapsed=06h:12m:25s
2025-12-19 14:27:29,827 [INFO] Trainer: Train [Step  39500/100000, Epoch 1.508/0] loss=1.4316 | lr_proj=4.551768e-04, lr_lora=9.103535e-05 | elapsed=06h:13m:12s
2025-12-19 14:28:16,995 [INFO] Trainer: Train [Step  39600/100000, Epoch 1.512/0] loss=1.5719 | lr_proj=4.550505e-04, lr_lora=9.101010e-05 | elapsed=06h:13m:59s
2025-12-19 14:29:04,221 [INFO] Trainer: Train [Step  39700/100000, Epoch 1.516/0] loss=1.4951 | lr_proj=4.549242e-04, lr_lora=9.098485e-05 | elapsed=06h:14m:46s
2025-12-19 14:29:50,195 [INFO] Trainer: Train [Step  39800/100000, Epoch 1.520/0] loss=1.4330 | lr_proj=4.547980e-04, lr_lora=9.095960e-05 | elapsed=06h:15m:32s
2025-12-19 14:30:35,530 [INFO] Trainer: Train [Step  39900/100000, Epoch 1.524/0] loss=1.3581 | lr_proj=4.546717e-04, lr_lora=9.093434e-05 | elapsed=06h:16m:17s
2025-12-19 14:31:21,802 [INFO] Trainer: Train [Step  40000/100000, Epoch 1.527/0] loss=1.6979 | lr_proj=4.545455e-04, lr_lora=9.090909e-05 | elapsed=06h:17m:03s
2025-12-19 14:32:50,118 [INFO] Trainer: Eval  [Step  40000/100000, Epoch 1.527/0] loss=1.5850 | lr_proj=4.545455e-04, lr_lora=9.090909e-05 | elapsed=06h:18m:32s
2025-12-19 14:32:50,193 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step40000.proj.pt
2025-12-19 14:32:50,310 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step40000.lora
2025-12-19 14:32:50,513 [INFO] Trainer: Saved config to sft5b/checkpoint_step40000.config.json
2025-12-19 14:32:50,515 [INFO] Trainer: Removing sft5b/checkpoint_step37000.optim.pt.*
2025-12-19 14:32:50,527 [INFO] Trainer: Removing sft5b/checkpoint_step37000.proj.pt.*
2025-12-19 14:32:50,530 [INFO] Trainer: Removing sft5b/checkpoint_step37000.lora.*
2025-12-19 14:32:50,540 [INFO] Trainer: Removing sft5b/checkpoint_step37000.config.json.*
2025-12-19 14:33:41,192 [INFO] Trainer: Train [Step  40100/100000, Epoch 1.531/0] loss=1.3772 | lr_proj=4.544192e-04, lr_lora=9.088384e-05 | elapsed=06h:19m:23s
2025-12-19 14:34:30,329 [INFO] Trainer: Train [Step  40200/100000, Epoch 1.535/0] loss=1.3534 | lr_proj=4.542929e-04, lr_lora=9.085859e-05 | elapsed=06h:20m:12s
2025-12-19 14:35:16,988 [INFO] Trainer: Train [Step  40300/100000, Epoch 1.539/0] loss=1.3121 | lr_proj=4.541667e-04, lr_lora=9.083333e-05 | elapsed=06h:20m:59s
2025-12-19 14:36:06,512 [INFO] Trainer: Train [Step  40400/100000, Epoch 1.543/0] loss=1.6177 | lr_proj=4.540404e-04, lr_lora=9.080808e-05 | elapsed=06h:21m:48s
2025-12-19 14:36:54,247 [INFO] Trainer: Train [Step  40500/100000, Epoch 1.547/0] loss=2.0527 | lr_proj=4.539141e-04, lr_lora=9.078283e-05 | elapsed=06h:22m:36s
2025-12-19 14:37:43,586 [INFO] Trainer: Train [Step  40600/100000, Epoch 1.550/0] loss=1.2353 | lr_proj=4.537879e-04, lr_lora=9.075758e-05 | elapsed=06h:23m:25s
2025-12-19 14:38:30,616 [INFO] Trainer: Train [Step  40700/100000, Epoch 1.554/0] loss=1.4017 | lr_proj=4.536616e-04, lr_lora=9.073232e-05 | elapsed=06h:24m:12s
2025-12-19 14:39:19,372 [INFO] Trainer: Train [Step  40800/100000, Epoch 1.558/0] loss=1.4707 | lr_proj=4.535354e-04, lr_lora=9.070707e-05 | elapsed=06h:25m:01s
2025-12-19 14:40:05,873 [INFO] Trainer: Train [Step  40900/100000, Epoch 1.562/0] loss=1.4331 | lr_proj=4.534091e-04, lr_lora=9.068182e-05 | elapsed=06h:25m:48s
2025-12-19 14:40:54,467 [INFO] Trainer: Train [Step  41000/100000, Epoch 1.566/0] loss=1.3140 | lr_proj=4.532828e-04, lr_lora=9.065657e-05 | elapsed=06h:26m:36s
2025-12-19 14:42:22,700 [INFO] Trainer: Eval  [Step  41000/100000, Epoch 1.566/0] loss=1.5907 | lr_proj=4.532828e-04, lr_lora=9.065657e-05 | elapsed=06h:28m:04s
2025-12-19 14:42:22,749 [INFO] Trainer: Saved Projector to sft5b/checkpoint_step41000.proj.pt
2025-12-19 14:42:22,934 [INFO] Trainer: Saved LoRa adapters to sft5b/checkpoint_step41000.lora
2025-12-19 14:42:23,128 [INFO] Trainer: Saved config to sft5b/checkpoint_step41000.config.json
2025-12-19 14:42:23,130 [INFO] Trainer: Removing sft5b/checkpoint_step38000.config.json.*
2025-12-19 14:42:23,131 [INFO] Trainer: Removing sft5b/checkpoint_step38000.lora.*
2025-12-19 14:42:23,138 [INFO] Trainer: Removing sft5b/checkpoint_step38000.proj.pt.*
2025-12-19 14:42:23,141 [INFO] Trainer: Removing sft5b/checkpoint_step38000.optim.pt.*
2025-12-19 14:43:08,984 [INFO] Trainer: Train [Step  41100/100000, Epoch 1.569/0] loss=1.2787 | lr_proj=4.531566e-04, lr_lora=9.063131e-05 | elapsed=06h:28m:51s
2025-12-19 14:43:56,778 [INFO] Trainer: Train [Step  41200/100000, Epoch 1.573/0] loss=0.9283 | lr_proj=4.530303e-04, lr_lora=9.060606e-05 | elapsed=06h:29m:38s
2025-12-19 14:44:44,527 [INFO] Trainer: Train [Step  41300/100000, Epoch 1.577/0] loss=1.2395 | lr_proj=4.529040e-04, lr_lora=9.058081e-05 | elapsed=06h:30m:26s
2025-12-19 14:45:30,045 [INFO] Trainer: Train [Step  41400/100000, Epoch 1.581/0] loss=1.5438 | lr_proj=4.527778e-04, lr_lora=9.055556e-05 | elapsed=06h:31m:12s
2025-12-19 14:46:19,929 [INFO] Trainer: Train [Step  41500/100000, Epoch 1.585/0] loss=1.5234 | lr_proj=4.526515e-04, lr_lora=9.053030e-05 | elapsed=06h:32m:02s
2025-12-19 14:47:08,309 [INFO] Trainer: Train [Step  41600/100000, Epoch 1.589/0] loss=1.2930 | lr_proj=4.525253e-04, lr_lora=9.050505e-05 | elapsed=06h:32m:50s
2025-12-19 14:47:54,995 [INFO] Trainer: Train [Step  41700/100000, Epoch 1.592/0] loss=1.3269 | lr_proj=4.523990e-04, lr_lora=9.047980e-05 | elapsed=06h:33m:37s
